[
  {
    "objectID": "VB_Bayes_activity2.html",
    "href": "VB_Bayes_activity2.html",
    "title": "Introduction to Bayesian Methods",
    "section": "",
    "text": "Main materials"
  },
  {
    "objectID": "VB_Bayes_activity2.html#packages-and-tools",
    "href": "VB_Bayes_activity2.html#packages-and-tools",
    "title": "Introduction to Bayesian Methods",
    "section": "Packages and tools",
    "text": "Packages and tools\nFor this practical you will need to first install nimble, then be sure to install the following packages:\n\n# Load libraries\nrequire(nimble)\nrequire(HDInterval)\nlibrary(MCMCvis)\nrequire(coda) # makes diagnostic plots\nrequire(IDPmisc) # makes nice colored pairs plots to look at joint posteriors\n\n##require(mcmcplots) # another option for diagnostic plots, currently unused"
  },
  {
    "objectID": "VB_Bayes_activity2.html#example-midge-wing-length",
    "href": "VB_Bayes_activity2.html#example-midge-wing-length",
    "title": "Introduction to Bayesian Methods",
    "section": "Example: Midge Wing Length",
    "text": "Example: Midge Wing Length\nWe will use this simple example to go through the steps of assessing a Bayesian model and we’ll see that MCMC can allow us to approximate the posterior distribution.\nGrogan and Wirth (1981) provide data on the wing length (in millimeters) of nine members of a species of midge (small, two-winged flies).\nFrom these measurements we wish to make inference about the population mean \\mu.\n\n# Load data\nWL.data <- read.csv(\"MidgeWingLength.csv\")\nY <- WL.data$WingLength\nn <- length(Y)\n\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\") \n\n\n\n\n\n\n\n\nWe’ll also need summary statistics for the data that we calculated last time:\n\nm<-sum(Y)/n\ns2<-sum((Y-m)^2)/(n-1)"
  },
  {
    "objectID": "VB_Bayes_activity2.html#recall-setting-up-the-bayesian-model",
    "href": "VB_Bayes_activity2.html#recall-setting-up-the-bayesian-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Recall: Setting up the Bayesian Model",
    "text": "Recall: Setting up the Bayesian Model\nWe need to define the likelihood and the priors for our Bayesian analysis. Given the analysis that we’ve just done, let’s assume that our data come from a normal distribution with unknown mean, \\mu but that we know the variance is \\sigma^2 = 0.025. That is: \n\\mathbf{Y} \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)\n\nIn the last activity we our prior for \\mu to be be: \n\\mu \\sim \\mathcal{N}(1.9, 0.8^2)\n Together, then, our full model is: \n\\begin{align*}\n\\mathbf{Y} & \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)\\\\\n\\mu &\\sim \\mathcal{N}(1.9, 0.8^2)\n\\end{align*}\n\nIn the previous activity we wrote a function to calculate \\mu_p and \\tau_p and then plugged in our numbers:\n\ntau.post<-function(tau, tau0, n){n*tau + tau0}\nmu.post<-function(Ybar, mu0, sig20, sig2, n){\n  weight<-sig2+n*sig20\n  \n  return(n*sig20*Ybar/weight + sig2*mu0/weight)\n}\n\nFinally we plotted 3 things together – the data histogram, the prior, and the posterior\n\nmu0 <- 1.9\ns20 <- 0.8\ns2<- 0.025 ## \"true\" variance\n\nmp<-mu.post(Ybar=m, mu0=mu0, sig20=s20, sig2=s2, n=n)\ntp<-tau.post(tau=1/s2, tau0=1/s20, n=n)\n\n\nx<-seq(1.3,2.3, length=1000)\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\", xlim=c(1.3, 2.3),\n     freq=FALSE, ylim=c(0,8)) \nlines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior\nlines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior\nlegend(\"topleft\", legend=c(\"prior\", \"posterior\"), col=c(2,4), lty=c(2,1), lwd=2)"
  },
  {
    "objectID": "VB_Bayes_activity2.html#specifying-the-model",
    "href": "VB_Bayes_activity2.html#specifying-the-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Specifying the model",
    "text": "Specifying the model\nFirst we must encode our choices for our data model and priors to pass them to the fitting routines in nimble. This involves setting up a {\\tt model} that includes the likelihood for each data point and a prior for every parameter we want to estimate. Here is an example of how we would do this for the simple model we fit for the midge data (note that nimble uses the precision instead of the variance or sd for the normal distribution):\n\nmodelCode <-  nimbleCode({\n\n  ## Likelihood\n  for(i in 1:n){\n    Y[i] ~ dnorm(mu,tau)\n  }\n\n  ## Prior for mu\n  mu  ~ dnorm(mu0,tau0)\n\n} ## close model\n)\n\nThis model is formally in the BUGS language (also used by JAGS, WinBugs, etc). Now we will create the nimble model\n\nmodel1 <- nimbleModel(code = modelCode, name = \"model1\", \n                    constants = list(tau=1/s2, mu0=mu0,\n                                    tau0=1/s20, n=n),\n                    data  = list(Y=Y), \n                    inits = list(mu=5))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\nmodel1$getNodeNames()\n\n [1] \"mu\"   \"Y[1]\" \"Y[2]\" \"Y[3]\" \"Y[4]\" \"Y[5]\" \"Y[6]\" \"Y[7]\" \"Y[8]\" \"Y[9]\"\n\n## just checking it can compile\nCmodel1<- compileNimble(model1)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nThen we run the MCMC and, see how the output looks for a short chain:\n\nmcmc.out <- nimbleMCMC(code = modelCode, \n                       constants = list(tau=1/s2, mu0=mu0,\n                                    tau0=1/s20,n=n),\n                       data  = list(Y=Y),\n                       inits = list(mu=5),\n                       nchains = 1, niter = 100,\n                       #summary = TRUE, WAIC = TRUE,\n                       monitors = c('mu'))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\ndim(mcmc.out)\n\n[1] 100   1\n\nhead(mcmc.out)\n\n           mu\n[1,] 1.830954\n[2,] 1.912009\n[3,] 1.794305\n[4,] 1.844535\n[5,] 1.803714\n[6,] 1.735574\n\n\n\nsamps<-as.mcmc(mcmc.out)\nplot(samps)\n\n\n\n\n\n\n\n\nMCMC is a rejection algorithm that often needs to converge or “burn-in” – that is we need to potentially move until we’re taking draws from the correct distribution. Unlike for optimization problems, this does not mean that the algorithm heads toward a single value. Instead we’re looking for a pattern where the draws are seemingly unrelated and random. To assess convergence we look at trace plots, the goal is to get traces that look like “fuzzy caterpillars”.\nSometimes at the beginning of a run, if we start far from the area near the posterior mean of the parameter, we will instead get something that looks like a trending time series. If this is the case we have to drop the samples that were taken during the burn-in phase. Here’s an example of how to do that, also now running 2 chains simultaneously.\n\nmcmc.out <- nimbleMCMC(code = modelCode, \n                       constants = list(tau=1/s2, mu0=mu0,\n                                    tau0=1/s20,n=n),\n                       data  = list(Y=Y),\n                       inits = list(mu=5),\n                       nchains = 2, niter = 11000,\n                       nburnin = 1000,\n                       #summary = TRUE, WAIC = TRUE,\n                       monitors = c('mu'))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\nrunning chain 2...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\ndim(mcmc.out$chain1)\n\n[1] 10000     1\n\nhead(mcmc.out$chain1)\n\n           mu\n[1,] 1.849637\n[2,] 1.759040\n[3,] 1.774108\n[4,] 1.802210\n[5,] 1.721513\n[6,] 1.849575\n\n\n\nsamp<-as.mcmc(mcmc.out$chain1)\nplot(samp)\n\n\n\n\n\n\n\n\nThis is a very fuzzy caterpillar!\nWe also often want to check the autocorrelation in the chain.\n\nacfplot(samp, lag=20, aspect=\"fill\", ylim=c(-1,1))\n\n\n\n\nThis is really good! It means that the samples are almost entirely uncorrelated.\nFinally we can also use the summary function to examine the samples generated:\n\nsummary(samp)\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n     1.8047942      0.0523170      0.0005232      0.0005232 \n\n2. Quantiles for each variable:\n\n 2.5%   25%   50%   75% 97.5% \n1.702 1.770 1.805 1.840 1.908 \n\n\nLet’s compare these draws to what we got with our analytic solution:\n\nx<-seq(1.3,2.3, length=1000)\nhist(samp, xlab=\"mu\", xlim=c(1.3, 2.3),\n     freq=FALSE, ylim=c(0,8), main =\"posterior samples\") \nlines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior\nlines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior\nlegend(\"topleft\", legend=c(\"prior\", \"analytic posterior\"), col=c(2,4), lty=c(2,1), lwd=2)\n\n\n\n\n\n\n\n\nIt worked!\nAs with the analytic approach, it’s always a good idea when you run your analyses to see how sensitive is your result to the priors you choose. Unless you are purposefully choosing an informative prior, we usually want the prior and posterior to look different, as we see here. You can experiment yourself and try changing the prior to see how this effects the posterior."
  },
  {
    "objectID": "VB_Bayes_activity2.html#practice-applying-to-a-new-dataset",
    "href": "VB_Bayes_activity2.html#practice-applying-to-a-new-dataset",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice: Applying to a new dataset",
    "text": "Practice: Applying to a new dataset\nDownload VecTraits dataset 562 (Kutcherov et al. 2018. Effects of temperature and photoperiod on the immature development in Cassida rubiginosa Mull. and C. stigmatica Sffr. (Coleoptera: Chrysomelidae). Sci. Rep. 9: 10047). This dataset explores the effects of temperature and photoperiod on body size (weight in mg) for Cassida stigmatica, a type of small beetle. Subset the data so you focus on one temperature/photoperiod combination (you could also choose to subset by sex). Using the same model as above, potentially with a different prior distribution, and with the value of \\tau set to 1/s^2 (where s is the empirical standard deviation of your dataset), redo the analysis above."
  },
  {
    "objectID": "VB_Bayes_activity2.html#practice-updating-the-model",
    "href": "VB_Bayes_activity2.html#practice-updating-the-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice: Updating the model",
    "text": "Practice: Updating the model\nRedo the previous analysis placing a gamma prior on \\mu as well. Set the prior so that the mean and variance are the same as in the normal example from above (use moment matching). Do you get something similar?"
  },
  {
    "objectID": "VB_Bayes_activity1.html",
    "href": "VB_Bayes_activity1.html",
    "title": "Introduction to Bayesian Methods",
    "section": "",
    "text": "Main Materials"
  },
  {
    "objectID": "VB_Bayes_activity1.html#example-midge-wing-length",
    "href": "VB_Bayes_activity1.html#example-midge-wing-length",
    "title": "Introduction to Bayesian Methods",
    "section": "Example: Midge Wing Length",
    "text": "Example: Midge Wing Length\nWe will use this simple example to go through the steps of assessing a Bayesian model and we’ll see that MCMC can allow us to approximate the posterior distribution.\nGrogan and Wirth (1981) provide data on the wing length (in millimeters) of nine members of a species of midge (small, two-winged flies).\nFrom these measurements we wish to make inference about the population mean \\mu.\n\n# Load data\nWL.data <- read.csv(\"MidgeWingLength.csv\")\nY <- WL.data$WingLength\nn <- length(Y)\n\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\")"
  },
  {
    "objectID": "VB_Bayes_activity1.html#non-bayesian-analysis",
    "href": "VB_Bayes_activity1.html#non-bayesian-analysis",
    "title": "Introduction to Bayesian Methods",
    "section": "Non-Bayesian analysis",
    "text": "Non-Bayesian analysis\nWe might expect that these midge data could be draws from a Normal distribution \\mathcal{N}(\\mu, \\sigma^2). Recall that the MLEs for \\mu and \\sigma^2 here are simply the sample mean and sample variance respectively:\n\nm<-sum(Y)/n\ns2<-sum((Y-m)^2)/(n-1)\nround(c(m, s2), 3)\n\n[1] 1.804 0.017\n\n\n\nx<-seq(1.4,2.2, length=50)\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\", xlim=c(1.4, 2.2), freq=FALSE) \nlines(x, dnorm(x, mean=m, sd=sqrt(s2)), col=2)\n\n\n\n\n\n\n\n\nNOTE: I’ve plotted the estimate of the population distribution here, but this is not the predictive distribution (which would be a Student T because we’re estimating both the mean and variance…).\n\nThe non-Bayesian version here has the advantage of being quick and familiar. However, from our point of view it has two weaknesses:\n\nBecause we have so few data points estimates of the accuracy of our predictions aren’t available. 9 points is only barely enough to estimate a mean, so we don’t trust any of the variance calculations.\nWe can’t easily incorporate things that we might already know about midges into our analysis.\n\nLet’s see how we can do a similar analysis using a Bayesian approach, here analytically."
  },
  {
    "objectID": "VB_Bayes_activity1.html#setting-up-the-bayesian-model",
    "href": "VB_Bayes_activity1.html#setting-up-the-bayesian-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Setting up the Bayesian Model",
    "text": "Setting up the Bayesian Model\nWe need to define the likelihood and the priors for our Bayesian analysis. Given the analysis that we’ve just done, let’s assume that our data come from a normal distribution with unknown mean, \\mu but that we know the variance is \\sigma^2 = 0.025. That is: \n\\mathbf{Y} \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)"
  },
  {
    "objectID": "VB_Bayes_activity1.html#prior-information",
    "href": "VB_Bayes_activity1.html#prior-information",
    "title": "Introduction to Bayesian Methods",
    "section": "Prior Information",
    "text": "Prior Information\nStudies from other populations suggest that wing lengths are usually around 1.9 mm, so we set \\mu_0 = 1.9\nWe also know that lengths must be positive (\\mu >0)\nWe can approximate this restriction with a normal prior distribution for \\mu as follows:\nSince most of the normal density is within two standard deviations of the mean we choose \\tau^2_0 so that\n \\mu_0 - 2\\sigma_0 >0 \\Rightarrow \\sigma_0 <1.9/2 = 0.95  I will choose \\sigma_0=0.8 here. Thus our prior for mu will be: \n\\mu \\sim \\mathcal{N}(1.9, 0.8^2)\n\n\nTogether, then, our full model is: \n\\begin{align*}\n\\mathbf{Y} & \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)\\\\\n\\mu &\\sim \\mathcal{N}(1.9, 0.8^2)\n\\end{align*}"
  },
  {
    "objectID": "VB_Bayes_activity1.html#analytic-posterior",
    "href": "VB_Bayes_activity1.html#analytic-posterior",
    "title": "Introduction to Bayesian Methods",
    "section": "Analytic Posterior",
    "text": "Analytic Posterior\nFor this very simple case it is easy to write down the posterior distribution (up to some constant). First, note that the likelihood for the data can be written as\n\n\\begin{align*}\n\\mathcal{L} &\\propto \\prod_{i=1}^n \\frac{1}{\\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2}(Y_i-\\mu)^2 \\right) \\\\\n& =  \\frac{1}{\\sigma^n} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i-\\mu)^2 \\right)\\\\\n& \\propto \\exp\\left(-\\frac{n}{2\\sigma^2} (\\bar{Y}-\\mu)^2 \\right)\n\\end{align*}\n\nMultiplying the prior through we get the following for the posterior:\n\n\\mathrm{P}(\\mu|\\mathbf{Y}) \\propto \\exp \\left(-\\frac{n}{2\\sigma^2} (\\bar{Y}-\\mu)^2 \\right) \\exp\\left(-\\frac{1}{2\\sigma_0^2}(\\mu-\\mu_0)^2 \\right)\n\nYou can re-arrange, complete the square, etc, to get a new expression that is like\n\n\\mathrm{P}(\\mu|\\mathbf{Y}) \\propto \\exp \\left(-\\frac{1}{2\\sigma_p^2} (\\mu_p-\\mu)^2 \\right)\n\nwhere\n\n\\begin{align*}\n\\mu_p & = \\frac{n\\sigma_0^2}{\\sigma^2 + n\\sigma_0^2} \\bar{Y} +  \\frac{\\sigma^2}{\\frac{\\sigma^2}{n} + \\sigma_0^2} \\mu_0\\\\\n& \\\\\n\\sigma_p^2 & = \\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1}\n\\end{align*}\n\nInstead of writing this last in terms of the variances, we could instead use precision (the inverse variance) which gives a simpler expression: \n\\tau_p = n\\tau + \\tau_0\n\nJust like in our earlier example, our estimate of the mean is a weighted average of the data and the prior, with the variance being determined by the data and prior variances.\nSo lets write a little function to calculate \\mu_p and \\tau_p and the plug in our numbers\n\ntau.post<-function(tau, tau0, n){n*tau + tau0}\nmu.post<-function(Ybar, mu0, sig20, sig2, n){\n  weight<-sig2+n*sig20\n  \n  return(n*sig20*Ybar/weight + sig2*mu0/weight)\n}\n\nLet’s plot 3 things together – the data histogram, the prior, and the posterior\n\nmu0 <- 1.9\ns20 <- 0.8\ns2<- 0.025 ## \"true\" variance\n\nmp<-mu.post(Ybar=m, mu0=mu0, sig20=s20, sig2=s2, n=n)\ntp<-tau.post(tau=1/s2, tau0=1/s20, n=n)\n\n\nx<-seq(1.3,2.3, length=1000)\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\", xlim=c(1.3, 2.3),\n     freq=FALSE, ylim=c(0,8)) \nlines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior\nlines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior\nlegend(\"topleft\", legend=c(\"prior\", \"posterior\"), col=c(2,4), lty=c(2,1), lwd=2)"
  },
  {
    "objectID": "VB_Bayes_activity1.html#practice-prior-sensitivity",
    "href": "VB_Bayes_activity1.html#practice-prior-sensitivity",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice: Prior sensitivity",
    "text": "Practice: Prior sensitivity\nChange the values of the mean and the variance that you choose for the prior (“hyperparameters”). What does this do to the posterior distribution. E.g., what happens if the variance you choose is small, and \\mu_0 =2.5 or so. Is this what you expect?"
  },
  {
    "objectID": "data_wrangling.html",
    "href": "data_wrangling.html",
    "title": "",
    "section": "",
    "text": "Main materials"
  },
  {
    "objectID": "data_wrangling.html#welcome-to-the-vectorbyte-2023-section-on-data-wrangling-and-visualiszation",
    "href": "data_wrangling.html#welcome-to-the-vectorbyte-2023-section-on-data-wrangling-and-visualiszation",
    "title": "",
    "section": "Welcome to the VectorByte 2023 section on Data Wrangling and Visualiszation",
    "text": "Welcome to the VectorByte 2023 section on Data Wrangling and Visualiszation\nThis section will cover: * Data Management and Visualization * Experimental Design"
  },
  {
    "objectID": "data_wrangling.html#data-management-and-visualization",
    "href": "data_wrangling.html#data-management-and-visualization",
    "title": "",
    "section": "Data Management and Visualization",
    "text": "Data Management and Visualization\n\nClutter and confusion are failures of design, not attributes of information. – Edward Tuftey"
  },
  {
    "objectID": "data_wrangling.html#introduction",
    "href": "data_wrangling.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nThis section aims at introducing you to key principles and methods for data processing, storage, exploration and visualization.\nIn this modern world, massive amounts of data are being generated in biology due to rapid advances in technologies for collecting new, as well as for digitizing old data. Some prominent examples are Genomic, Ecosystem respiration, Climatic, and Animal tracking data. Ultimately, the goal of quantitative biology is to both, discover patterns in these data, and fit mathematical models to them. Reproducible data manipulation, analyses and visualization are particularly necessary when data are so large and complex, and few are more so than biological data, which are extremely heterogeneous in their structure and quality. Furthermore, when these data are “big” (more below on what makes a dataset “big”), computationally-efficient data handling, manipulation and analysis techniques are needed.\n\nR vs. Python\nWe will use R in VectorBiTE 2023 Training because it a great one stop solution for both data manipulation, analysis and visualization. In general, R will do the job for most of your purposes. There is not much between difference these two languages for data science. One advantage that Python has is its greater computational efficiency. On the other hand, R was developed for convenient statistical analyses, with efficiency not being the main concern.\nRead more about R vs Python for data science here and here."
  },
  {
    "objectID": "data_wrangling.html#data-wrangling",
    "href": "data_wrangling.html#data-wrangling",
    "title": "",
    "section": "Data wrangling",
    "text": "Data wrangling\nYou are likely to spend far more time than you think dredging through data files manually – checking them, editing them, and reformatting them to make them useful for data exploration and analysis. It is often the case that you’ll have to deal with messy or incomplete data, either because of sampling challenges (e.g., “field” data), or because you got given data that was poorly recorded and maintained. The data we obtain from different data sources is often unusable at the beginning; for example you may need to: * Identify the variables vs observations within the data—somebody else might have recorded the data, or you youself might have collected the data some time back! * Fill in zeros (true measured or observed absences) * Identify and add a value (e.g., -999999) to denote missing observations * Derive or calculate new variables from the raw observations (e.g., convert measurements to SI units; kilograms, meters, seconds, etc.) * Reshape/reformat your data into a layout that works best for analysis (e.g., for R itself);e.g., from wide to long data format for replicated (across plates, chambers, plots, sites, etc.) data * Merge multiple datasets together into a single data sheet\nThis is not an exhaustive list. Doing so many different things to your raw data is both time-consuming and risky. Why risky? Because to err is very human, and every new, tired mouse-click and/or keyboard-stab has a high probability of inducing an erroneous data point!"
  },
  {
    "objectID": "data_wrangling.html#some-data-wrangling-principles",
    "href": "data_wrangling.html#some-data-wrangling-principles",
    "title": "",
    "section": "Some data wrangling principles",
    "text": "Some data wrangling principles\nSo you would like a record of the data wrangling process (so that it is repeatable and even reversible), and automate it to the extent possible. To this end, here are some guidelines:\n\nStore data in universally (machine)-readable, non-proprietary formats; basically, use plain ASCII text for your file names, variable/field/column names, and data values. And make sure the data file’s “text encoding” is correct and standard (e.g., UTF-8).\nKeep a metadata file for each unique dataset (again, in non-proprietary format).\nMinimize modifying raw data by hand—use scripts instead—keep a copy of the data as they were recorded.\nUse meaningful names for your data and files and field (column) names\nWhen you add data, try not to add columns (widening the format); rather, design your tables/data-sheets so that you add only rows (lengthening the format)—and convert “wide format data” to “long format data” using scripts, not by hand,\nAll cells within a data column should contain only one type of information (i.e., either text (character), numeric, etc.).\nUltimately, consider creating a relational database for your data (More on this below).\n\nThis is not an exhaustive list either— see the Readings & Resources Section.\n\nAn example\nWe will use the Pound Hill dataset collected by students in an Imperial College Field Course for understanding some of these principles. This is not vectorbite data but it is an excellent example of a dataset that needs wrangling. VectorBiTE is too well structured for this sort of wrangling.\nTo start with, we need to import the raw data file, for which, follow these steps:\n★ Copy the file PoundHillData.csv and PoundHillMetaData.csv files from the VectorBiTE data directory into your own R data directory. Then load the data in R:\n\nMyData <- as.matrix(read.csv(\"activities/data/PoundHillData.csv\",header = FALSE))\nclass(MyData)\n\n[1] \"matrix\" \"array\" \n\n\n\nLoading the data as.matrix(), and setting header=FALSE guarantees that the data are imported “as is” so that you can wrangle them. Otherwise read.csv will convert the first row to column headers.\nAll the data will be converted to the character class in the resulting matrix called MyData because at least one of the entries is already character class.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nAs of R version 4.0.0 of R released in April 2020, the default for stringsAsFactors was changed to false If you are using R version 3.x.x, you will need to add stringsAsFactors = FALSE to the above command to prevent R from converting all columns of character type (strings) to the factor data type (this will create problems with subsequent data wrangling).\n\n\n\n★ Now load the Metadata:\n\nMyMetaData <- read.csv(\"activities/data/PoundHillMetaData.csv\", header = TRUE,  sep=\";\")\nclass(MyMetaData)\n\n[1] \"data.frame\"\n\n\n\nHere, header =TRUE because we do have metadata headers (FieldName andDescription), and\nWe have used semicolon (;) as delimiter because there are commas in one of the field descriptions.\nWe have avoided spaces in the columns headers (so “FieldName” instead of “Field Name”) — please avoid spaces in field or column names because R will replace each space in a column header with a dot, which may be confusing.\n\n\n\n\n\n\n\nTip! The text encoding of your data file\n\n\n\n\n\nIf you have a string (character data type) stored as a variable in your R workspace, or a file containing strings, the computer has to know what character encoding it is in or it cannot interpret or display it to you correctly. Usually, the encoding will be UTF-8 or ASCII, which is easily handled by most computer languages. Sometimes you may run into (unexpected) bugs when importing and running scripts in R because your file has a non-standard text encoding. You will need to specify the encoding in that case, using the encoding argument of read.csv() and read.table(). You can check the encoding of a file by using find in Linux/Mac. Try in your UNIX terminal:\nfile -i data/PoundHillData.csv\nor, check encoding of all files in the Data directory:\nfile -i data/*.csv\nUse file -I instead of file -i in a Mac terminal.\n\n\n\nNow check out what the data look like:\n\nhead(MyData)\n\n     V1                     V2        V3        V4        V5        V6       \n[1,] \"Cultivation\"          \"october\" \"october\" \"october\" \"october\" \"october\"\n[2,] \"Block\"                \"a\"       \"a\"       \"a\"       \"a\"       \"a\"      \n[3,] \"Plot\"                 \"1\"       \"1\"       \"1\"       \"1\"       \"1\"      \n[4,] \"Quadrat\"              \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q5\"     \n[5,] \"Achillea millefolium\" \"4\"       \"8\"       \"3\"       \"20\"      \"6\"      \n[6,] \"Agrostis gigantea\"    \"\"        \"\"        \"\"        \"\"        \"15\"     \n     V7    V8    V9    V10   V11   V12     V13     V14     V15     V16    \n[1,] \"may\" \"may\" \"may\" \"may\" \"may\" \"march\" \"march\" \"march\" \"march\" \"march\"\n[2,] \"a\"   \"a\"   \"a\"   \"a\"   \"a\"   \"a\"     \"a\"     \"a\"     \"a\"     \"a\"    \n[3,] \"2\"   \"2\"   \"2\"   \"2\"   \"2\"   \"3\"     \"3\"     \"3\"     \"3\"     \"3\"    \n[4,] \"Q1\"  \"Q2\"  \"Q3\"  \"Q4\"  \"Q5\"  \"Q1\"    \"Q2\"    \"Q3\"    \"Q4\"    \"Q5\"   \n[5,] \"4\"   \"\"    \"\"    \"\"    \"\"    \"1\"     \"2\"     \"5\"     \"12\"    \"2\"    \n[6,] \"\"    \"\"    \"\"    \"\"    \"2\"   \"8\"     \"\"      \"53\"    \"16\"    \"47\"   \n     V17     V18   V19   V20   V21   V22   V23     V24     V25     V26    \n[1,] \"march\" \"may\" \"may\" \"may\" \"may\" \"may\" \"march\" \"march\" \"march\" \"march\"\n[2,] \"a\"     \"b\"   \"b\"   \"b\"   \"b\"   \"b\"   \"b\"     \"b\"     \"b\"     \"b\"    \n[3,] \"3\"     \"4\"   \"4\"   \"4\"   \"4\"   \"4\"   \"5\"     \"5\"     \"5\"     \"5\"    \n[4,] \"Q6\"    \"Q1\"  \"Q2\"  \"Q3\"  \"Q4\"  \"Q5\"  \"Q1\"    \"Q2\"    \"Q3\"    \"Q4\"   \n[5,] \"\"      \"\"    \"\"    \"\"    \"\"    \"\"    \"3\"     \"8\"     \"3\"     \"7\"    \n[6,] \"12\"    \"68\"  \"57\"  \"83\"  \"88\"  \"89\"  \"10\"    \"29\"    \"24\"    \"24\"   \n     V27     V28       V29       V30       V31       V32     V33     V34    \n[1,] \"march\" \"october\" \"october\" \"october\" \"october\" \"march\" \"march\" \"march\"\n[2,] \"b\"     \"b\"       \"b\"       \"b\"       \"b\"       \"c\"     \"c\"     \"c\"    \n[3,] \"5\"     \"6\"       \"6\"       \"6\"       \"6\"       \"7\"     \"7\"     \"7\"    \n[4,] \"Q5\"    \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q1\"    \"Q2\"    \"Q3\"   \n[5,] \"20\"    \"1\"       \"\"        \"3\"       \"1\"       \"24\"    \"26\"    \"37\"   \n[6,] \"43\"    \"143\"     \"39\"      \"10\"      \"67\"      \"62\"    \"22\"    \"37\"   \n     V35     V36     V37       V38       V39       V40       V41       V42  \n[1,] \"march\" \"march\" \"october\" \"october\" \"october\" \"october\" \"october\" \"may\"\n[2,] \"c\"     \"c\"     \"c\"       \"c\"       \"c\"       \"c\"       \"c\"       \"c\"  \n[3,] \"7\"     \"7\"     \"8\"       \"8\"       \"8\"       \"8\"       \"8\"       \"9\"  \n[4,] \"Q4\"    \"Q5\"    \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q5\"      \"Q1\" \n[5,] \"37\"    \"100\"   \"2\"       \"\"        \"\"        \"1\"       \"\"        \"\"   \n[6,] \"4\"     \"27\"    \"198\"     \"96\"      \"99\"      \"48\"      \"351\"     \"57\" \n     V43   V44   V45   V46   V47     V48     V49     V50     V51     V52    \n[1,] \"may\" \"may\" \"may\" \"may\" \"march\" \"march\" \"march\" \"march\" \"march\" \"march\"\n[2,] \"c\"   \"c\"   \"c\"   \"c\"   \"d\"     \"d\"     \"d\"     \"d\"     \"d\"     \"d\"    \n[3,] \"9\"   \"9\"   \"9\"   \"9\"   \"10\"    \"10\"    \"10\"    \"10\"    \"10\"    \"10\"   \n[4,] \"Q2\"  \"Q3\"  \"Q4\"  \"Q5\"  \"Q1\"    \"Q2\"    \"Q3\"    \"Q4\"    \"Q5\"    \"Q6\"   \n[5,] \"\"    \"7\"   \"\"    \"5\"   \"7\"     \"2\"     \"2\"     \"9\"     \"4\"     \"10\"   \n[6,] \"41\"  \"65\"  \"38\"  \"45\"  \"259\"   \"33\"    \"96\"    \"291\"   \"19\"    \"80\"   \n     V53   V54   V55   V56       V57       V58       V59       V60      \n[1,] \"may\" \"may\" \"may\" \"october\" \"october\" \"october\" \"october\" \"october\"\n[2,] \"d\"   \"d\"   \"d\"   \"d\"       \"d\"       \"d\"       \"d\"       \"d\"      \n[3,] \"12\"  \"12\"  \"12\"  \"11\"      \"11\"      \"11\"      \"11\"      \"11\"     \n[4,] \"Q1\"  \"Q2\"  \"Q4\"  \"Q1\"      \"Q2\"      \"Q3\"      \"Q4\"      \"Q5\"     \n[5,] \"\"    \"\"    \"\"    \"\"        \"\"        \"12\"      \"6\"       \"5\"      \n[6,] \"33\"  \"145\" \"45\"  \"62\"      \"25\"      \"57\"      \"113\"     \"12\"     \n\n\nNote that column names V1-V60 were generated automatically by R when you imported the data\nIn RStudio you can also do view(MyData) at the R prompt or any other code editor, fix(MyData). We won’t do anything with the metadata file in this session except inspect the information it contains.\n\n\nKeep a metadata file for each unique dataset\nData wrangling really begins immediately after data collection. You may collect data of different kinds (e.g., diversity, biomass, tree girth), etc. Keep the original spreadsheet well documented using a “metadata” file that describes the data (you would hopefully have written the first version of this even before you started collecting the data!). The minimum information needed to make a metadata file useful is a description of each of the fields — the column or row headers under which the information is stored in your data/spreadsheet.\nHave a look at the metadata file for the Pound Hill dataset:\nBoolean arguments in R: In R, you can use F and T for boolean FALSE and TRUE respectively. To see this, type a <- T\nin the R commandline, and then see what R returns when you type a. Using F and T for boolean FALSE and TRUE respectively is not necessarily good practice, but be aware that this option exists.\n\nMyMetaData\n\n     FieldName\n1  Cultivation\n2        Block\n3         Plot\n4      Quadrat\n5 SpeciesCount\n                                                           Description\n1  Cultivation treatments applied in three months: october, may, march\n2                                            Treatment blocks ids: a-d\n3                                 Plot ids under each treatment : 1-12\n4                  Sampling quadrats (25x50 cm each) per plot: Q1--Q6 \n5                 Number of individuals of species (count) per quadrat\n\n\nIdeally, you would also like to add more information about the data, such as the measurement units of each type of observation. These data include just one type of observation: Number of individuals of species per sample (plot), which is a count (integer, or int data class).\n\n\nMinimize modifying raw data by hand\nWhen the dataset is large (e.g., 1000’s of rows), cleaning and exploring it can get tricky, and you are very likely to make many mistakes. You should record all the steps you used to process it with an R script rather than risking a manual and basically irreproducible processing. Most importantly, avoid or minimize editing your raw data file—make a copy (with a meaningful tag in the file name to indicate the date and author) before making hand edits.\nAll blank cells in the data are true absences, in the sense that species was actually not present in that quadrat. So we can replace those blanks with zeros:\n\nMyData[MyData == \"\"] = 0\n\n\n\nConvert wide format data to long format using scripts\nOne typically records data in the field or experiments using a “wide” format, where a subject’s (e.g., habitat, plot, treatment, species etc) repeated responses or observations (e.g., species count, biomass, etc) will be in a single row, and each response in a separate column. The raw Pound Hill data were recorded in this way. However, the wide format is not ideal for data analysis — instead you need the data in a “long” format, where each row is one observation point per subject. So each subject will have data in multiple rows. Any measures/variables/observations that don’t change across the subjects will have the same value in all the rows. For humans, the wide format is generally more intuitive for recording (e.g., in field data sheets) data. However, for data inspection and analysis, the long format is preferable for two main reasons:\n\nIf you have many response and/or treatment variables, it is hard to inspect the data values in a wide-form version of the dataset. In the case of the pound hill dataset, the response variable is species, and treatment variable is cultivation month (with sequentially nested replicates—block, plot, quadrat— within it), and the data values are the number (count) of individuals of each species per quadrat. As you can see, there are a a large number of columns (60 to be exact), with columns V2-V60 containing different treatment combinations. This makes it hard to visually see problems with the data values. You would have to look across all these columns to see any issues, or if you wanted to run a single command on all the data values (e.g., is.integer() to check if they are all integers, as you would expect), it would be harder to do so or interpret the output if all the species counts were in a single column.\nLong-form datasets are typically required for statistical analysis and visualization packages or commands in R (or Python, for that matter). For example, if you wanted to fit a linear model using R’s lm() command, with treatment (cultivation month) as the independent variable, you would need to structure your data in long form. Similarly, and if you wanted to plot histograms of species numbers by treatment using ggplot (coming up), you would also need these data in long format.\n\nOK, so let’s go from wide to long format already!\nYou can switch between wide and long formats using melt() and dcast() from the reshape2 package, as illustrated in the script DataWrang.R available at TheMulQuaBio repository. But first, let’s transpose the data, because for a long format, the (nested) treatments variables should be in rows:\n\nMyData <- t(MyData) \nhead(MyData)\n\n   [,1]          [,2]    [,3]   [,4]      [,5]                  \nV1 \"Cultivation\" \"Block\" \"Plot\" \"Quadrat\" \"Achillea millefolium\"\nV2 \"october\"     \"a\"     \"1\"    \"Q1\"      \"4\"                   \nV3 \"october\"     \"a\"     \"1\"    \"Q2\"      \"8\"                   \nV4 \"october\"     \"a\"     \"1\"    \"Q3\"      \"3\"                   \nV5 \"october\"     \"a\"     \"1\"    \"Q4\"      \"20\"                  \nV6 \"october\"     \"a\"     \"1\"    \"Q5\"      \"6\"                   \n   [,6]                [,7]                 [,8]              \nV1 \"Agrostis gigantea\" \"Anagallis arvensis\" \"Anchusa arvensis\"\nV2 \"0\"                 \"0\"                  \"0\"               \nV3 \"0\"                 \"0\"                  \"3\"               \nV4 \"0\"                 \"0\"                  \"1\"               \nV5 \"0\"                 \"0\"                  \"1\"               \nV6 \"15\"                \"0\"                  \"0\"               \n   [,9]                 [,10]               [,11]               \nV1 \"Anisantha sterilis\" \"Aphanes australis\" \"Artemesia vulgaris\"\nV2 \"0\"                  \"0\"                 \"0\"                 \nV3 \"0\"                  \"0\"                 \"0\"                 \nV4 \"0\"                  \"0\"                 \"2\"                 \nV5 \"0\"                  \"0\"                 \"0\"                 \nV6 \"5\"                  \"0\"                 \"0\"                 \n   [,12]              [,13]                  [,14]              \nV1 \"Bromus hordaceus\" \"Cerastium glomeratun\" \"Chenopodium album\"\nV2 \"0\"                \"0\"                    \"0\"                \nV3 \"0\"                \"2\"                    \"0\"                \nV4 \"0\"                \"1\"                    \"0\"                \nV5 \"0\"                \"0\"                    \"0\"                \nV6 \"0\"                \"0\"                    \"0\"                \n   [,15]             [,16]               [,17]               [,18]           \nV1 \"Cirsium arvense\" \"Conyza canadensis\" \"Crepis capillaris\" \"Crepis paludos\"\nV2 \"0\"               \"0\"                 \"0\"                 \"0\"             \nV3 \"5\"               \"0\"                 \"0\"                 \"0\"             \nV4 \"0\"               \"0\"                 \"0\"                 \"0\"             \nV5 \"0\"               \"0\"                 \"0\"                 \"0\"             \nV6 \"0\"               \"0\"                 \"0\"                 \"0\"             \n   [,19]              [,20]                [,21]               \nV1 \"Elytrigia repens\" \"Epilobium ciliatum\" \"Erodium cicutarium\"\nV2 \"0\"                \"1\"                  \"0\"                 \nV3 \"0\"                \"0\"                  \"0\"                 \nV4 \"0\"                \"0\"                  \"0\"                 \nV5 \"0\"                \"0\"                  \"0\"                 \nV6 \"0\"                \"0\"                  \"0\"                 \n   [,22]                  [,23]           [,24]            [,25]          \nV1 \"Fallopia convolvulus\" \"Festuca rubra\" \"Geranium molle\" \"Holcus mollis\"\nV2 \"0\"                    \"0\"             \"0\"              \"159\"          \nV3 \"0\"                    \"0\"             \"0\"              \"81\"           \nV4 \"0\"                    \"0\"             \"0\"              \"144\"          \nV5 \"0\"                    \"0\"             \"0\"              \"124\"          \nV6 \"0\"                    \"0\"             \"0\"              \"115\"          \n   [,26]                  [,27]              [,28]               \nV1 \"Hypochaeris radicata\" \"Lactuca serriola\" \"Medicago lupulina \"\nV2 \"0\"                    \"0\"                \"0\"                 \nV3 \"0\"                    \"0\"                \"0\"                 \nV4 \"0\"                    \"0\"                \"0\"                 \nV5 \"0\"                    \"0\"                \"0\"                 \nV6 \"0\"                    \"0\"                \"0\"                 \n   [,29]               [,30]                  [,31]            \nV1 \"Myosotis arvensis\" \"Plantago lanceolata \" \"Polpaver dubium\"\nV2 \"0\"                 \"0\"                    \"0\"              \nV3 \"0\"                 \"0\"                    \"0\"              \nV4 \"0\"                 \"0\"                    \"0\"              \nV5 \"0\"                 \"0\"                    \"0\"              \nV6 \"0\"                 \"0\"                    \"0\"              \n   [,32]                    [,33]              [,34]          \nV1 \"Raphanus raphanistrum \" \"Rumex acetosella\" \"Rumex crispus\"\nV2 \"0\"                      \"25\"               \"0\"            \nV3 \"0\"                      \"0\"                \"0\"            \nV4 \"0\"                      \"0\"                \"0\"            \nV5 \"0\"                      \"6\"                \"0\"            \nV6 \"0\"                      \"1\"                \"0\"            \n   [,35]               [,36]              [,37]           [,38]              \nV1 \"Rumex obtusifolia\" \"Semecio jacobaea\" \"Sonchus asper\" \"Spergula arvensis\"\nV2 \"0\"                 \"0\"                \"0\"             \"0\"                \nV3 \"0\"                 \"0\"                \"0\"             \"0\"                \nV4 \"0\"                 \"0\"                \"0\"             \"0\"                \nV5 \"0\"                 \"0\"                \"0\"             \"0\"                \nV6 \"0\"                 \"0\"                \"0\"             \"0\"                \n   [,39]                [,40]                  [,41]                       \nV1 \"Stellaria graminea\" \"Taraxacum officinale\" \"Tripleurospermum inodorum \"\nV2 \"8\"                  \"0\"                    \"11\"                        \nV3 \"4\"                  \"0\"                    \"1\"                         \nV4 \"1\"                  \"0\"                    \"0\"                         \nV5 \"0\"                  \"0\"                    \"0\"                         \nV6 \"1\"                  \"10\"                   \"0\"                         \n   [,42]               [,43]              [,44]            [,45]           \nV1 \"Veronica arvensis\" \"Veronica persica\" \"Viola arvensis\" \"Vulpia myuros \"\nV2 \"0\"                 \"1\"                \"0\"              \"0\"             \nV3 \"0\"                 \"0\"                \"3\"              \"0\"             \nV4 \"0\"                 \"0\"                \"1\"              \"0\"             \nV5 \"0\"                 \"0\"                \"1\"              \"0\"             \nV6 \"0\"                 \"0\"                \"0\"              \"7\"             \n\n\nAt this point, you should note that the first row in the data matrix actually contains the column headers, but these are not encoded by R as being column headers. As far as R is concerned, the first row is just another data row. You can check this:\n\ncolnames(MyData)\n\nNULL\n\n\nWe need to fix this. Also, we would like to create a dataframe now, as this is needed by the R commands we will be using, and ultimately, any visualization and analysis we will be doing. To this end, let’s first create a temporary dataframe with just the data, without the column names:\n\nTempData <- as.data.frame(MyData[-1,],stringsAsFactors = F)\nhead(TempData)\n\n        V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20\nV2 october  a  1 Q1  4  0  0  0  0   0   0   0   0   0   0   0   0   0   0   1\nV3 october  a  1 Q2  8  0  0  3  0   0   0   0   2   0   5   0   0   0   0   0\nV4 october  a  1 Q3  3  0  0  1  0   0   2   0   1   0   0   0   0   0   0   0\nV5 october  a  1 Q4 20  0  0  1  0   0   0   0   0   0   0   0   0   0   0   0\nV6 october  a  1 Q5  6 15  0  0  5   0   0   0   0   0   0   0   0   0   0   0\nV7     may  a  2 Q1  4  0  0  1  0   0   0   0   0   7   1   0   0   0   0   0\n   V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39\nV2   0   0   0   0 159   0   0   0   0   0   0   0  25   0   0   0   0   0   8\nV3   0   0   0   0  81   0   0   0   0   0   0   0   0   0   0   0   0   0   4\nV4   0   0   0   0 144   0   0   0   0   0   0   0   0   0   0   0   0   0   1\nV5   0   0   0   0 124   0   0   0   0   0   0   0   6   0   0   0   0   0   0\nV6   0   0   0   0 115   0   0   0   0   0   0   0   1   0   0   0   0   0   1\nV7  16  12   0   0 120   0   0   0   0   0   0   0   1   0   0   0   0   2   0\n   V40 V41 V42 V43 V44 V45\nV2   0  11   0   1   0   0\nV3   0   1   0   0   3   0\nV4   0   0   0   0   1   0\nV5   0   0   0   0   1   0\nV6  10   0   0   0   0   7\nV7   0   0   0   2   0   0\n\n\nNote that stringsAsFactors = F is important here because we don’t want R to convert columns to the factor class without supervision. This might be a good idea in may cases, but let’s just do it manually later. This will also force you to think carefully about what data each of you columns contains.\nNow let’s assign the original column names to the temporary dataset:\n\ncolnames(TempData) <- MyData[1,] # assign column names from original data\nhead(TempData)\n\n   Cultivation Block Plot Quadrat Achillea millefolium Agrostis gigantea\nV2     october     a    1      Q1                    4                 0\nV3     october     a    1      Q2                    8                 0\nV4     october     a    1      Q3                    3                 0\nV5     october     a    1      Q4                   20                 0\nV6     october     a    1      Q5                    6                15\nV7         may     a    2      Q1                    4                 0\n   Anagallis arvensis Anchusa arvensis Anisantha sterilis Aphanes australis\nV2                  0                0                  0                 0\nV3                  0                3                  0                 0\nV4                  0                1                  0                 0\nV5                  0                1                  0                 0\nV6                  0                0                  5                 0\nV7                  0                1                  0                 0\n   Artemesia vulgaris Bromus hordaceus Cerastium glomeratun Chenopodium album\nV2                  0                0                    0                 0\nV3                  0                0                    2                 0\nV4                  2                0                    1                 0\nV5                  0                0                    0                 0\nV6                  0                0                    0                 0\nV7                  0                0                    0                 7\n   Cirsium arvense Conyza canadensis Crepis capillaris Crepis paludos\nV2               0                 0                 0              0\nV3               5                 0                 0              0\nV4               0                 0                 0              0\nV5               0                 0                 0              0\nV6               0                 0                 0              0\nV7               1                 0                 0              0\n   Elytrigia repens Epilobium ciliatum Erodium cicutarium Fallopia convolvulus\nV2                0                  1                  0                    0\nV3                0                  0                  0                    0\nV4                0                  0                  0                    0\nV5                0                  0                  0                    0\nV6                0                  0                  0                    0\nV7                0                  0                 16                   12\n   Festuca rubra Geranium molle Holcus mollis Hypochaeris radicata\nV2             0              0           159                    0\nV3             0              0            81                    0\nV4             0              0           144                    0\nV5             0              0           124                    0\nV6             0              0           115                    0\nV7             0              0           120                    0\n   Lactuca serriola Medicago lupulina  Myosotis arvensis Plantago lanceolata \nV2                0                  0                 0                    0\nV3                0                  0                 0                    0\nV4                0                  0                 0                    0\nV5                0                  0                 0                    0\nV6                0                  0                 0                    0\nV7                0                  0                 0                    0\n   Polpaver dubium Raphanus raphanistrum  Rumex acetosella Rumex crispus\nV2               0                      0               25             0\nV3               0                      0                0             0\nV4               0                      0                0             0\nV5               0                      0                6             0\nV6               0                      0                1             0\nV7               0                      0                1             0\n   Rumex obtusifolia Semecio jacobaea Sonchus asper Spergula arvensis\nV2                 0                0             0                 0\nV3                 0                0             0                 0\nV4                 0                0             0                 0\nV5                 0                0             0                 0\nV6                 0                0             0                 0\nV7                 0                0             0                 2\n   Stellaria graminea Taraxacum officinale Tripleurospermum inodorum \nV2                  8                    0                         11\nV3                  4                    0                          1\nV4                  1                    0                          0\nV5                  0                    0                          0\nV6                  1                   10                          0\nV7                  0                    0                          0\n   Veronica arvensis Veronica persica Viola arvensis Vulpia myuros \nV2                 0                1              0              0\nV3                 0                0              3              0\nV4                 0                0              1              0\nV5                 0                0              1              0\nV6                 0                0              0              7\nV7                 0                2              0              0\n\n\nThe row names still remain, but they are not really bothering us, so we can ignore them. But if you do want to get rid of them, you can:\n\nrownames(TempData) <- NULL\nhead(TempData)\n\n  Cultivation Block Plot Quadrat Achillea millefolium Agrostis gigantea\n1     october     a    1      Q1                    4                 0\n2     october     a    1      Q2                    8                 0\n3     october     a    1      Q3                    3                 0\n4     october     a    1      Q4                   20                 0\n5     october     a    1      Q5                    6                15\n6         may     a    2      Q1                    4                 0\n  Anagallis arvensis Anchusa arvensis Anisantha sterilis Aphanes australis\n1                  0                0                  0                 0\n2                  0                3                  0                 0\n3                  0                1                  0                 0\n4                  0                1                  0                 0\n5                  0                0                  5                 0\n6                  0                1                  0                 0\n  Artemesia vulgaris Bromus hordaceus Cerastium glomeratun Chenopodium album\n1                  0                0                    0                 0\n2                  0                0                    2                 0\n3                  2                0                    1                 0\n4                  0                0                    0                 0\n5                  0                0                    0                 0\n6                  0                0                    0                 7\n  Cirsium arvense Conyza canadensis Crepis capillaris Crepis paludos\n1               0                 0                 0              0\n2               5                 0                 0              0\n3               0                 0                 0              0\n4               0                 0                 0              0\n5               0                 0                 0              0\n6               1                 0                 0              0\n  Elytrigia repens Epilobium ciliatum Erodium cicutarium Fallopia convolvulus\n1                0                  1                  0                    0\n2                0                  0                  0                    0\n3                0                  0                  0                    0\n4                0                  0                  0                    0\n5                0                  0                  0                    0\n6                0                  0                 16                   12\n  Festuca rubra Geranium molle Holcus mollis Hypochaeris radicata\n1             0              0           159                    0\n2             0              0            81                    0\n3             0              0           144                    0\n4             0              0           124                    0\n5             0              0           115                    0\n6             0              0           120                    0\n  Lactuca serriola Medicago lupulina  Myosotis arvensis Plantago lanceolata \n1                0                  0                 0                    0\n2                0                  0                 0                    0\n3                0                  0                 0                    0\n4                0                  0                 0                    0\n5                0                  0                 0                    0\n6                0                  0                 0                    0\n  Polpaver dubium Raphanus raphanistrum  Rumex acetosella Rumex crispus\n1               0                      0               25             0\n2               0                      0                0             0\n3               0                      0                0             0\n4               0                      0                6             0\n5               0                      0                1             0\n6               0                      0                1             0\n  Rumex obtusifolia Semecio jacobaea Sonchus asper Spergula arvensis\n1                 0                0             0                 0\n2                 0                0             0                 0\n3                 0                0             0                 0\n4                 0                0             0                 0\n5                 0                0             0                 0\n6                 0                0             0                 2\n  Stellaria graminea Taraxacum officinale Tripleurospermum inodorum \n1                  8                    0                         11\n2                  4                    0                          1\n3                  1                    0                          0\n4                  0                    0                          0\n5                  1                   10                          0\n6                  0                    0                          0\n  Veronica arvensis Veronica persica Viola arvensis Vulpia myuros \n1                 0                1              0              0\n2                 0                0              3              0\n3                 0                0              1              0\n4                 0                0              1              0\n5                 0                0              0              7\n6                 0                2              0              0\n\n\nFinally, let’s convert the data to long format. For this, we need the reshape2 package:\n\n\n\n\n\n\nNote\n\n\n\nBoth library() and require() are commands/functions to load packages. The difference is that require() is designed for use inside other functions, so it returns FALSE and gives a warning, whereaslibrary() returns an error by default if the package does not exist.\n\n\n\nrequire(reshape2)# load the reshape2 package\n\nLoading required package: reshape2\n\n\nCheck out the help for the melt command of reshape2: ?melt.\nOK finally, let’s wrangle this dataset into submission!\n\nMyWrangledData <- melt(TempData, id=c(\"Cultivation\", \"Block\", \"Plot\", \"Quadrat\"), variable.name = \"Species\", value.name = \"Count\")\nhead(MyWrangledData); tail(MyWrangledData)\n\n  Cultivation Block Plot Quadrat              Species Count\n1     october     a    1      Q1 Achillea millefolium     4\n2     october     a    1      Q2 Achillea millefolium     8\n3     october     a    1      Q3 Achillea millefolium     3\n4     october     a    1      Q4 Achillea millefolium    20\n5     october     a    1      Q5 Achillea millefolium     6\n6         may     a    2      Q1 Achillea millefolium     4\n\n\n     Cultivation Block Plot Quadrat        Species Count\n2414         may     d   12      Q4 Vulpia myuros      0\n2415     october     d   11      Q1 Vulpia myuros      0\n2416     october     d   11      Q2 Vulpia myuros      0\n2417     october     d   11      Q3 Vulpia myuros      0\n2418     october     d   11      Q4 Vulpia myuros      0\n2419     october     d   11      Q5 Vulpia myuros      0\n\n\nThis is the long format we have been extolling! It looks much simpler, with all the count data in a single, long column with each count value associated with some “attributes”. It is now clear that the fundamental unit of analysis is a species’ count in a quadrat, each nested within a plot, each in turn nested within a block, and finally, each of which is nested within a cultivation month (the main treatment).\nWe can also now assign the correct data types to each column:\n\nMyWrangledData[, \"Cultivation\"] <- as.factor(MyWrangledData[, \"Cultivation\"])\nMyWrangledData[, \"Block\"] <- as.factor(MyWrangledData[, \"Block\"])\nMyWrangledData[, \"Plot\"] <- as.factor(MyWrangledData[, \"Plot\"])\nMyWrangledData[, \"Quadrat\"] <- as.factor(MyWrangledData[, \"Quadrat\"])\nMyWrangledData[, \"Count\"] <- as.integer(MyWrangledData[, \"Count\"])\nstr(MyWrangledData)\n\n'data.frame':   2419 obs. of  6 variables:\n $ Cultivation: Factor w/ 3 levels \"march\",\"may\",..: 3 3 3 3 3 2 2 2 2 2 ...\n $ Block      : Factor w/ 4 levels \"a\",\"b\",\"c\",\"d\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Plot       : Factor w/ 12 levels \"1\",\"10\",\"11\",..: 1 1 1 1 1 5 5 5 5 5 ...\n $ Quadrat    : Factor w/ 6 levels \"Q1\",\"Q2\",\"Q3\",..: 1 2 3 4 5 1 2 3 4 5 ...\n $ Species    : Factor w/ 41 levels \"Achillea millefolium\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Count      : int  4 8 3 20 6 4 0 0 0 0 ..."
  },
  {
    "objectID": "data_wrangling.html#on-to-data-exploration",
    "href": "data_wrangling.html#on-to-data-exploration",
    "title": "",
    "section": "On to data exploration",
    "text": "On to data exploration\nOnce you have wrangled the Pound Hill data to its long format, you are ready to go! You may want to start by examining and visualizing the basic properties of the data, such as the number of tree species (41) in the dataset, number of quadrats (replicates) per plot and cultivation treatment, etc.\nWe will learn about visualization next. After that, you can return to this dataset and try out some visual data exploration. For example, a useful visualization would be to make a histogram of abundances of species, grouped by different factors. For example, you can look at distributions of species’ abundances grouped by the fundamental treatment, Cultivation.\n\nAnd then came tidyverse\nSo if you think this is the end of the options you have for data wrangling in R, think again. There is a whole data science “ecosystem” you can use in R through the tidyverse package. This meta-package includes dplyr, the next iteration of plyr that addresses the speed issues in the latter, and tidyr, essentially a nicer wrapper to the reshape2 package with additional functions, and ggplot2 (coming up later).\nPlease install and load the tidyverse package.\n\nrequire(tidyverse)\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nThis loads a number of packages at one go (and you might get some name-conflict-masking messages; see the note below). You can list these packages:\n\ntidyverse_packages(include_self = TRUE) # the include_self = TRUE means list \"tidyverse\" as well \n\n [1] \"broom\"         \"cli\"           \"crayon\"        \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"googledrive\"  \n [9] \"googlesheets4\" \"ggplot2\"       \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"readr\"        \n[21] \"readxl\"        \"reprex\"        \"rlang\"         \"rstudioapi\"   \n[25] \"rvest\"         \"stringr\"       \"tibble\"        \"tidyr\"        \n[29] \"xml2\"          \"tidyverse\"    \n\n\nThat’s a lot of packages!\nThe first time you load tidyverse, you will get some feedback from R, including something like the following:\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nDepending on which package you are loading, you might get such a warning message, that “so and so” function is being “masked” from another one. Basically, any additional packages you load may have one or more functions with the same names(s) as existing function(s) (which will typically do something completely different) in base R’s packages. This creates name conflicts. Therefore, R will prevent you from using the new function(s) that you have enabled by using the same name of an pre-existing function. It gives priority to the pre-existing one. So, for example, if you call just filter(), the command that will be used is the one from stats, and not the one from dplyr. In this scenario, you can use the new function by using the double colon notation :: like so: dplyr::filter().\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe :: notation is like the dot notation in Python — it allows you to access a particular function (like a Python module) from a package. So you can also use the command at the end of the :: (try just as_tibble(MyWrangledData), for example), but it’s just safer to use the :: because then you can rest assured that you are actually using the function you intended to.\n\n\n\nMore of :: usage coming up below!\nOK, let’s try data exploration in tidyverse.\nFirst, let’s convert the dataframe to a “tibble”:\n\ntibble::as_tibble(MyWrangledData) \n\n# A tibble: 2,419 × 6\n   Cultivation Block Plot  Quadrat Species              Count\n   <fct>       <fct> <fct> <fct>   <fct>                <int>\n 1 october     a     1     Q1      Achillea millefolium     4\n 2 october     a     1     Q2      Achillea millefolium     8\n 3 october     a     1     Q3      Achillea millefolium     3\n 4 october     a     1     Q4      Achillea millefolium    20\n 5 october     a     1     Q5      Achillea millefolium     6\n 6 may         a     2     Q1      Achillea millefolium     4\n 7 may         a     2     Q2      Achillea millefolium     0\n 8 may         a     2     Q3      Achillea millefolium     0\n 9 may         a     2     Q4      Achillea millefolium     0\n10 may         a     2     Q5      Achillea millefolium     0\n# … with 2,409 more rows\n\n\nOK, there’s the funky double colon (::) notation again…\nAlso, note that we didn’t have to reassign MyWrangledData back to its original name because tibble just does the needful to the data frame in its current place.\n\n\n\n\n\n\nNote\n\n\n\nA “tibble” in tidyverse is equivalent to R’s traditional data.frame. Tibbles are modified data frames, but they make data exploration even easier.\n\n\n\ndplyr::glimpse(MyWrangledData) #like str(), but nicer!\n\nRows: 2,419\nColumns: 6\n$ Cultivation <fct> october, october, october, october, october, may, may, may…\n$ Block       <fct> a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, b, b, b, b…\n$ Plot        <fct> 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4…\n$ Quadrat     <fct> Q1, Q2, Q3, Q4, Q5, Q1, Q2, Q3, Q4, Q5, Q1, Q2, Q3, Q4, Q5…\n$ Species     <fct> Achillea millefolium, Achillea millefolium, Achillea mille…\n$ Count       <int> 4, 8, 3, 20, 6, 4, 0, 0, 0, 0, 1, 2, 5, 12, 2, 0, 0, 0, 0,…\n\n\nOr this:\nutils::View(MyWrangledData) #same as fix()\n\ndplyr::filter(MyWrangledData, Count>100) #like subset(), but nicer!\n\n   Cultivation Block Plot Quadrat            Species Count\n1      october     b    6      Q1  Agrostis gigantea   143\n2      october     c    8      Q1  Agrostis gigantea   198\n3      october     c    8      Q5  Agrostis gigantea   351\n4        march     d   10      Q1  Agrostis gigantea   259\n5        march     d   10      Q4  Agrostis gigantea   291\n6          may     d   12      Q2  Agrostis gigantea   145\n7      october     d   11      Q4  Agrostis gigantea   113\n8          may     c    9      Q2 Anisantha sterilis   120\n9      october     a    1      Q1      Holcus mollis   159\n10     october     a    1      Q3      Holcus mollis   144\n11     october     a    1      Q4      Holcus mollis   124\n12     october     a    1      Q5      Holcus mollis   115\n13         may     a    2      Q1      Holcus mollis   120\n14     october     b    6      Q2      Holcus mollis   129\n15     october     b    6      Q3      Holcus mollis   123\n16     october     c    8      Q2      Holcus mollis   256\n17     october     c    8      Q3      Holcus mollis   147\n18     october     c    8      Q4      Holcus mollis   128\n19     october     d   11      Q1      Holcus mollis   104\n20     october     d   11      Q2      Holcus mollis   131\n21       march     d   10      Q3 Medicago lupulina    114\n\n\n\ndplyr::slice(MyWrangledData, 10:15) # Look at an arbitrary set of data rows\n\n  Cultivation Block Plot Quadrat              Species Count\n1         may     a    2      Q5 Achillea millefolium     0\n2       march     a    3      Q1 Achillea millefolium     1\n3       march     a    3      Q2 Achillea millefolium     2\n4       march     a    3      Q3 Achillea millefolium     5\n5       march     a    3      Q4 Achillea millefolium    12\n6       march     a    3      Q5 Achillea millefolium     2\n\n\nLearning to use tidyverse involves learning some new syntax and a lot of new commands, but if you plan to do a lot of data wrangling and exploration, you will benefit from getting to know them well.\nHave a look at some tidyverse-related cheatsheets to explore further.\nOne additional thing that you would need to get used to in the tidyverse is piping multiple commands using the %% notation, which makes your code ore compact, and in theory, more readable."
  },
  {
    "objectID": "data_wrangling.html#handling-big-data-in-r",
    "href": "data_wrangling.html#handling-big-data-in-r",
    "title": "",
    "section": "Handling Big Data in R",
    "text": "Handling Big Data in R\nThe buzzword ‘Big Data’ basically refers to datasets that have the following properties:\n\nA dataset that does not fit into available RAM on one system (say, 2 gigabytes).\nA dataset that has so many rows (when in it’s long format — see above sections) that it significantly slows down your analysis or simulation without vectorization (that is, when looping).\n\nBoth these criteria are programming language-, and computer hardware-dependent, of course. For example, a 32-bit OS can only handle ~2 GB of RAM, so this computer will struggle every time you try to handle a dataset in that size range.\nR reads data into RAM all at once when you using the read.table (or its wrapper, read.csv() — maybe you have realized by now that read.csv() is basically calling read.table() with a particular set of options. That is, objects in R live in memory entirely, and big-ish data in RAM will cause R to choke.\nThere are a few options (which you can combine, of course) if you are actually using datasets that are so large:\n\nImport large files smartly; e.g., using scan() in R, and then create subsets of the data that you need. Also, use the reshape or tidyr packages to covert your data in the most “square” (so neither too long or too wide) format as possible. Of course, you will need subsets of data in long format for analysis (see sections above).\nUse the bigmemory package to load data in the gb range (e.g., use read.big.matrix() instead of read.table(). This package also has other useful functions, such as foreach() instead of for() for better memory management.\nUse a 64 bit version of R with enough memory and preferably on UNIX/Linux!\nVectorize your analyses/simulations to the extent possible.\nUse databases (more on this below).\nUse distributed computing (distribute the analysis/simulation across multiple CPU’s).\n\n\nDatabases and R\nR can be used to link to and extract data from online databases such as PubMed and GenBank, or to manipulate and access your own. Computational Biology datasets are often quite large, and it makes sense to access their data by querying the databases instead of manually downloading them. So also, your own data may be complex and large, in which case you may want to organize and manage those data in a proper relational database.\nPractically all the data wrangling principles in the previous sections are a part and parcel of relational databases.\nThere are many R packages that provide an interface to databases (SQLite, MySQL, Oracle, etc). Check out R packages DBI and RMySQL ."
  },
  {
    "objectID": "data_wrangling.html#data-visualization",
    "href": "data_wrangling.html#data-visualization",
    "title": "",
    "section": "Data visualization",
    "text": "Data visualization\nNow that you have learned how to wrangle data, let’s learn some key principles of graphics and visualization, and how to implement them in R. You can use R to build a complete, reproducible workflow for data visualization for both exploratory and publication purposes. We will start with some basic plotting and data exploration. You will then learn to generate publication-quality visualizations using the ggplot2 package.\n\nData exploration with basic plotting\nBefore you do any statistical analyses with data, you must clean, explore, and visualize it. And eventually, you want to produce a finished product that presents visualizations of your data and your results clearly and concisely.\nUltimately, at both, the data exploration and the finished product stages, the goal of graphics is to present information such that it provides intuitive ideas. As Edward Tufte says: “Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.”\nR can produce beautiful graphics without the time-consuming and fiddly methods that you might have used in Excel or equivalent. You should also make it a habit to quickly plot the data for exploratory analysis. So we are going to learn some basic plotting first.\n\nBasic plotting commands in R\nHere is a menu of basic R plotting commands (use ?commandname to learn more about it):\n\n\n\nCommand\nWhat it does\n\n\n\n\nplot(x,y)\nScatterplot\n\n\nplot(y~x)\nScatterplot with y as a response variable\n\n\nhist(mydata)\nHistogram\n\n\nbarplot(mydata)\nBar plot\n\n\npoints(y1$\\sim$x1)\nAdd another series of points\n\n\nboxplot(y$\\sim$x)\nBoxplot\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nR graphics devices: In all that follows, you may often end up plotting multiple plots on the same graphics window without intending to do so, because R by default keeps plotting in the most recent plotting window that was opened. You can close a particular graphics window or “device” by using dev.off(), and all open devices/windows with graphics.off(). By default, dev.off() will close the most recent figure device that was opened.\n\n\n\nNote that there are invisible devices as well! For example, if you are printing to pdf (coming up below), the device or graphics window will not be visible on your computer screen.\nNow let’s try some simple plotting for data exploration. We will use a dataset from a recent paper by our team at Imperial College London. In this paper (Huxley et al. 2021), we investigated how temperature and resource supply can affect an important vector’s life history traits (development, survival, size, fecundity) and its maximal population growth rate.\n\\star In VecTraits, locate and download dataset #558 into your own data directory. Then, launch R and read in these data to a dataframe (note the relative path):\n\nMyDF <- read.csv('activities/data/VByte_558_longevity.csv',stringsAsFactors = TRUE)\ndim(MyDF) #check the size of the data frame you loaded\n\n[1] 156  42\n\n\nLet’s look at what the data contain. Type MyDF and hit the TAB key twice in the R command line.\n\n\n\n\n\n\nTip\n\n\n\n\n\n##Remember Tabbing:In your smart code editor or IDE such as vscode or RStudio, you will see a drop-down list of all the column headers in a dataframe when you hit TAB once after MyDF$\n\n\n\nYou can also use str() and head():\n\nstr(MyDF)\n\n'data.frame':   156 obs. of  42 variables:\n $ Id                       : int  92846 92848 92850 92852 92854 92856 92858 92860 92862 92864 ...\n $ DatasetID                : int  558 558 558 558 558 558 558 558 558 558 ...\n $ IndividualID             : Factor w/ 156 levels \"PHX331\",\"PHX332\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ OriginalTraitName        : Factor w/ 1 level \"longevity\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OriginalTraitDef         : Factor w/ 1 level \"individual-level duration of life stage\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OriginalTraitValue       : num  8 5 5 6 4 6 9 1 7 7 ...\n $ OriginalTraitUnit        : Factor w/ 1 level \"days\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Habitat                  : Factor w/ 1 level \"terrestrial\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LabField                 : Factor w/ 1 level \"laboratory\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SecondStressor           : Factor w/ 1 level \"resource supply\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SecondStressorDef        : Factor w/ 1 level \"food quantity supplied at a constant rate \": 1 1 1 1 1 1 1 1 1 1 ...\n $ SecondStressorValue      : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ SecondStressorUnit       : Factor w/ 1 level \"mg individual-1 day-1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LocationText             : Factor w/ 1 level \"Fort Myers Florida\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LocationType             : Factor w/ 1 level \"field\": 1 1 1 1 1 1 1 1 1 1 ...\n $ CoordinateType           : Factor w/ 1 level \"decimal\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Latitude                 : num  26.6 26.6 26.6 26.6 26.6 ...\n $ Longitude                : num  -81.8 -81.8 -81.8 -81.8 -81.8 ...\n $ Interactor1              : Factor w/ 1 level \"Aedes aegypti\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Common        : Factor w/ 1 level \"Yellow fever mosquito\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Kingdom       : Factor w/ 1 level \"Animalia\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Phylum        : Factor w/ 1 level \"Arthropoda\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Class         : Factor w/ 1 level \"Insecta\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Order         : Factor w/ 1 level \"Diptera\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Family        : Factor w/ 1 level \"Culicidae\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Genus         : Factor w/ 1 level \"Aedes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Species       : Factor w/ 1 level \"aegypti\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Stage         : Factor w/ 1 level \"adult (starved)\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Sex           : Factor w/ 1 level \"female\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Temp          : num  22 22 22 22 22 22 22 22 22 22 ...\n $ Interactor1TempUnit      : Factor w/ 1 level \"Celsius\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1GrowthTemp    : num  26 26 26 26 26 26 26 26 26 26 ...\n $ Interactor1GrowthTempUnit: Factor w/ 1 level \"Celsius\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Size          : num  2.93 2.59 2.41 2.62 2.28 2.39 2.99 2.5 2.75 2.82 ...\n $ Interactor1SizeUnit      : Factor w/ 1 level \"mm\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1SizeType      : Factor w/ 1 level \"wing length\": 1 1 1 1 1 1 1 1 1 1 ...\n $ FigureTable              : Factor w/ 1 level \"supplementary material\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Citation                 : Factor w/ 1 level \"Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. \"| __truncated__: 1 1 1 1 1 1 1 1 1 1 ...\n $ DOI                      : Factor w/ 1 level \"10.1098/rspb.2020.3217\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SubmittedBy              : Factor w/ 1 level \"Paul Huxley\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ContributorEmail         : Factor w/ 1 level \"phuxly@gmail.com\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Notes                    : Factor w/ 2 levels \"\",\"wing length was not measured for this individual: zero assigned \": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nhead(MyDF)\n\n     Id DatasetID IndividualID OriginalTraitName\n1 92846       558       PHX331         longevity\n2 92848       558       PHX332         longevity\n3 92850       558       PHX333         longevity\n4 92852       558       PHX334         longevity\n5 92854       558       PHX335         longevity\n6 92856       558       PHX336         longevity\n                         OriginalTraitDef OriginalTraitValue OriginalTraitUnit\n1 individual-level duration of life stage                  8              days\n2 individual-level duration of life stage                  5              days\n3 individual-level duration of life stage                  5              days\n4 individual-level duration of life stage                  6              days\n5 individual-level duration of life stage                  4              days\n6 individual-level duration of life stage                  6              days\n      Habitat   LabField  SecondStressor\n1 terrestrial laboratory resource supply\n2 terrestrial laboratory resource supply\n3 terrestrial laboratory resource supply\n4 terrestrial laboratory resource supply\n5 terrestrial laboratory resource supply\n6 terrestrial laboratory resource supply\n                           SecondStressorDef SecondStressorValue\n1 food quantity supplied at a constant rate                  0.1\n2 food quantity supplied at a constant rate                  0.1\n3 food quantity supplied at a constant rate                  0.1\n4 food quantity supplied at a constant rate                  0.1\n5 food quantity supplied at a constant rate                  0.1\n6 food quantity supplied at a constant rate                  0.1\n     SecondStressorUnit       LocationText LocationType CoordinateType Latitude\n1 mg individual-1 day-1 Fort Myers Florida        field        decimal 26.61667\n2 mg individual-1 day-1 Fort Myers Florida        field        decimal 26.61667\n3 mg individual-1 day-1 Fort Myers Florida        field        decimal 26.61667\n4 mg individual-1 day-1 Fort Myers Florida        field        decimal 26.61667\n5 mg individual-1 day-1 Fort Myers Florida        field        decimal 26.61667\n6 mg individual-1 day-1 Fort Myers Florida        field        decimal 26.61667\n  Longitude   Interactor1     Interactor1Common Interactor1Kingdom\n1 -81.83333 Aedes aegypti Yellow fever mosquito           Animalia\n2 -81.83333 Aedes aegypti Yellow fever mosquito           Animalia\n3 -81.83333 Aedes aegypti Yellow fever mosquito           Animalia\n4 -81.83333 Aedes aegypti Yellow fever mosquito           Animalia\n5 -81.83333 Aedes aegypti Yellow fever mosquito           Animalia\n6 -81.83333 Aedes aegypti Yellow fever mosquito           Animalia\n  Interactor1Phylum Interactor1Class Interactor1Order Interactor1Family\n1        Arthropoda          Insecta          Diptera         Culicidae\n2        Arthropoda          Insecta          Diptera         Culicidae\n3        Arthropoda          Insecta          Diptera         Culicidae\n4        Arthropoda          Insecta          Diptera         Culicidae\n5        Arthropoda          Insecta          Diptera         Culicidae\n6        Arthropoda          Insecta          Diptera         Culicidae\n  Interactor1Genus Interactor1Species Interactor1Stage Interactor1Sex\n1            Aedes            aegypti  adult (starved)         female\n2            Aedes            aegypti  adult (starved)         female\n3            Aedes            aegypti  adult (starved)         female\n4            Aedes            aegypti  adult (starved)         female\n5            Aedes            aegypti  adult (starved)         female\n6            Aedes            aegypti  adult (starved)         female\n  Interactor1Temp Interactor1TempUnit Interactor1GrowthTemp\n1              22             Celsius                    26\n2              22             Celsius                    26\n3              22             Celsius                    26\n4              22             Celsius                    26\n5              22             Celsius                    26\n6              22             Celsius                    26\n  Interactor1GrowthTempUnit Interactor1Size Interactor1SizeUnit\n1                   Celsius            2.93                  mm\n2                   Celsius            2.59                  mm\n3                   Celsius            2.41                  mm\n4                   Celsius            2.62                  mm\n5                   Celsius            2.28                  mm\n6                   Celsius            2.39                  mm\n  Interactor1SizeType            FigureTable\n1         wing length supplementary material\n2         wing length supplementary material\n3         wing length supplementary material\n4         wing length supplementary material\n5         wing length supplementary material\n6         wing length supplementary material\n                                                                                                                                  Citation\n1 Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. R. Soc. B. 288: 20203217.\n2 Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. R. Soc. B. 288: 20203217.\n3 Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. R. Soc. B. 288: 20203217.\n4 Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. R. Soc. B. 288: 20203217.\n5 Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. R. Soc. B. 288: 20203217.\n6 Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. R. Soc. B. 288: 20203217.\n                     DOI SubmittedBy ContributorEmail Notes\n1 10.1098/rspb.2020.3217 Paul Huxley phuxly@gmail.com      \n2 10.1098/rspb.2020.3217 Paul Huxley phuxly@gmail.com      \n3 10.1098/rspb.2020.3217 Paul Huxley phuxly@gmail.com      \n4 10.1098/rspb.2020.3217 Paul Huxley phuxly@gmail.com      \n5 10.1098/rspb.2020.3217 Paul Huxley phuxly@gmail.com      \n6 10.1098/rspb.2020.3217 Paul Huxley phuxly@gmail.com      \n\n\nYou might want to try the dplyr way to do this data inspection/exploration as well, as you did above.\n\ndplyr::glimpse(MyDF)\n\nRows: 156\nColumns: 42\n$ Id                        <int> 92846, 92848, 92850, 92852, 92854, 92856, 92…\n$ DatasetID                 <int> 558, 558, 558, 558, 558, 558, 558, 558, 558,…\n$ IndividualID              <fct> PHX331, PHX332, PHX333, PHX334, PHX335, PHX3…\n$ OriginalTraitName         <fct> longevity, longevity, longevity, longevity, …\n$ OriginalTraitDef          <fct> individual-level duration of life stage, ind…\n$ OriginalTraitValue        <dbl> 8, 5, 5, 6, 4, 6, 9, 1, 7, 7, 7, 5, 9, 7, 7,…\n$ OriginalTraitUnit         <fct> days, days, days, days, days, days, days, da…\n$ Habitat                   <fct> terrestrial, terrestrial, terrestrial, terre…\n$ LabField                  <fct> laboratory, laboratory, laboratory, laborato…\n$ SecondStressor            <fct> resource supply, resource supply, resource s…\n$ SecondStressorDef         <fct> food quantity supplied at a constant rate , …\n$ SecondStressorValue       <dbl> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,…\n$ SecondStressorUnit        <fct> mg individual-1 day-1, mg individual-1 day-1…\n$ LocationText              <fct> Fort Myers Florida, Fort Myers Florida, Fort…\n$ LocationType              <fct> field, field, field, field, field, field, fi…\n$ CoordinateType            <fct> decimal, decimal, decimal, decimal, decimal,…\n$ Latitude                  <dbl> 26.61667, 26.61667, 26.61667, 26.61667, 26.6…\n$ Longitude                 <dbl> -81.83333, -81.83333, -81.83333, -81.83333, …\n$ Interactor1               <fct> Aedes aegypti, Aedes aegypti, Aedes aegypti,…\n$ Interactor1Common         <fct> Yellow fever mosquito, Yellow fever mosquito…\n$ Interactor1Kingdom        <fct> Animalia, Animalia, Animalia, Animalia, Anim…\n$ Interactor1Phylum         <fct> Arthropoda, Arthropoda, Arthropoda, Arthropo…\n$ Interactor1Class          <fct> Insecta, Insecta, Insecta, Insecta, Insecta,…\n$ Interactor1Order          <fct> Diptera, Diptera, Diptera, Diptera, Diptera,…\n$ Interactor1Family         <fct> Culicidae, Culicidae, Culicidae, Culicidae, …\n$ Interactor1Genus          <fct> Aedes, Aedes, Aedes, Aedes, Aedes, Aedes, Ae…\n$ Interactor1Species        <fct> aegypti, aegypti, aegypti, aegypti, aegypti,…\n$ Interactor1Stage          <fct> adult (starved), adult (starved), adult (sta…\n$ Interactor1Sex            <fct> female, female, female, female, female, fema…\n$ Interactor1Temp           <dbl> 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, …\n$ Interactor1TempUnit       <fct> Celsius, Celsius, Celsius, Celsius, Celsius,…\n$ Interactor1GrowthTemp     <dbl> 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, …\n$ Interactor1GrowthTempUnit <fct> Celsius, Celsius, Celsius, Celsius, Celsius,…\n$ Interactor1Size           <dbl> 2.93, 2.59, 2.41, 2.62, 2.28, 2.39, 2.99, 2.…\n$ Interactor1SizeUnit       <fct> mm, mm, mm, mm, mm, mm, mm, mm, mm, mm, mm, …\n$ Interactor1SizeType       <fct> wing length, wing length, wing length, wing …\n$ FigureTable               <fct> supplementary material, supplementary materi…\n$ Citation                  <fct> Huxley et al. 2021. The effect of resource l…\n$ DOI                       <fct> 10.1098/rspb.2020.3217, 10.1098/rspb.2020.32…\n$ SubmittedBy               <fct> Paul Huxley, Paul Huxley, Paul Huxley, Paul …\n$ ContributorEmail          <fct> phuxly@gmail.com, phuxly@gmail.com, phuxly@g…\n$ Notes                     <fct> , , , , , , , , , , , , , , , , , , , , , , …\n\n\nTo make things clearer, let’s rename some of our columns and remove individuals that don’t have wing lengths (i.e., zeros).\nLet’s also define temp and food level as factors because we’ll need this for later.\nThe tidyverse is particularly useful for this kind of thing!\n\nMyDF <- as_tibble(MyDF) %>% \n        rename(winglength = Interactor1Size,\n               temp =  Interactor1Temp,\n               foodlevel = SecondStressorValue,\n               longevity = OriginalTraitValue) %>%\n        mutate(temp = factor(temp,\n              levels = c('22', '26', '32')),\n              foodlevel = factor(foodlevel,\n              levels = c('0.1', '1'))) %>%\n        filter(winglength != '0')\n\nstr(MyDF)\n\ntibble [149 × 42] (S3: tbl_df/tbl/data.frame)\n $ Id                       : int [1:149] 92846 92848 92850 92852 92854 92856 92858 92860 92862 92864 ...\n $ DatasetID                : int [1:149] 558 558 558 558 558 558 558 558 558 558 ...\n $ IndividualID             : Factor w/ 156 levels \"PHX331\",\"PHX332\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ OriginalTraitName        : Factor w/ 1 level \"longevity\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OriginalTraitDef         : Factor w/ 1 level \"individual-level duration of life stage\": 1 1 1 1 1 1 1 1 1 1 ...\n $ longevity                : num [1:149] 8 5 5 6 4 6 9 1 7 7 ...\n $ OriginalTraitUnit        : Factor w/ 1 level \"days\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Habitat                  : Factor w/ 1 level \"terrestrial\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LabField                 : Factor w/ 1 level \"laboratory\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SecondStressor           : Factor w/ 1 level \"resource supply\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SecondStressorDef        : Factor w/ 1 level \"food quantity supplied at a constant rate \": 1 1 1 1 1 1 1 1 1 1 ...\n $ foodlevel                : Factor w/ 2 levels \"0.1\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SecondStressorUnit       : Factor w/ 1 level \"mg individual-1 day-1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LocationText             : Factor w/ 1 level \"Fort Myers Florida\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LocationType             : Factor w/ 1 level \"field\": 1 1 1 1 1 1 1 1 1 1 ...\n $ CoordinateType           : Factor w/ 1 level \"decimal\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Latitude                 : num [1:149] 26.6 26.6 26.6 26.6 26.6 ...\n $ Longitude                : num [1:149] -81.8 -81.8 -81.8 -81.8 -81.8 ...\n $ Interactor1              : Factor w/ 1 level \"Aedes aegypti\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Common        : Factor w/ 1 level \"Yellow fever mosquito\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Kingdom       : Factor w/ 1 level \"Animalia\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Phylum        : Factor w/ 1 level \"Arthropoda\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Class         : Factor w/ 1 level \"Insecta\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Order         : Factor w/ 1 level \"Diptera\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Family        : Factor w/ 1 level \"Culicidae\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Genus         : Factor w/ 1 level \"Aedes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Species       : Factor w/ 1 level \"aegypti\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Stage         : Factor w/ 1 level \"adult (starved)\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1Sex           : Factor w/ 1 level \"female\": 1 1 1 1 1 1 1 1 1 1 ...\n $ temp                     : Factor w/ 3 levels \"22\",\"26\",\"32\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1TempUnit      : Factor w/ 1 level \"Celsius\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1GrowthTemp    : num [1:149] 26 26 26 26 26 26 26 26 26 26 ...\n $ Interactor1GrowthTempUnit: Factor w/ 1 level \"Celsius\": 1 1 1 1 1 1 1 1 1 1 ...\n $ winglength               : num [1:149] 2.93 2.59 2.41 2.62 2.28 2.39 2.99 2.5 2.75 2.82 ...\n $ Interactor1SizeUnit      : Factor w/ 1 level \"mm\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Interactor1SizeType      : Factor w/ 1 level \"wing length\": 1 1 1 1 1 1 1 1 1 1 ...\n $ FigureTable              : Factor w/ 1 level \"supplementary material\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Citation                 : Factor w/ 1 level \"Huxley et al. 2021. The effect of resource limitation on the temperature-dependance of mosquito fitness. Proc. \"| __truncated__: 1 1 1 1 1 1 1 1 1 1 ...\n $ DOI                      : Factor w/ 1 level \"10.1098/rspb.2020.3217\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SubmittedBy              : Factor w/ 1 level \"Paul Huxley\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ContributorEmail         : Factor w/ 1 level \"phuxly@gmail.com\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Notes                    : Factor w/ 2 levels \"\",\"wing length was not measured for this individual: zero assigned \": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\n\nScatter Plots\nLet’s start by plotting wing length vs. adult lifespan\n\nplot(MyDF$winglength, MyDF$longevity)\n\n\n\n\nThat doesn’t look very meaningful! Let’s try taking logarithms. Why? - Because taking a log allows you to inspect the data in a meaningful (logarithmic) scale and reveals the true relationship. This also illustrates a important point. Just like statistical analyses, the effectiveness of data visualization too depends on the type of distribution of the data.\n\nplot(log(MyDF$winglength),log(MyDF$longevity))\n\n\n\n\nLet’s look at the same using a base-10 log transform:\n\nplot(log10(MyDF$longevity),log10(MyDF$winglength))\n\n\n\n\nUsing a log10 transform is often a good idea because then you can see things in terms of “orders of magnitude”, (10^1, 10^2, 10^3, etc), which makes it easier to determine what the actual values in the original scale are.\nNotice also the labels can be changed using xlab and ylab.\nWe can change almost any aspect of the resulting graph; let’s change the symbols by specifying the plot characters using pch:\n\nplot(log10(MyDF$longevity),log10(MyDF$winglength), xlab = 'adult longevity (days)', ylab = 'wing length (mm)', pch = 20)\n\n\n\n\nA really great summary of basic R graphical parameters can be found here.\n\n\nHistograms\nWhy did we have to take a logarithm to see the relationship between adult lifespan and wing length? Plotting histograms of the two classes should be insightful, as we can then see the “marginal” distributions of the two variables.\nLet’s first plot a histogram of wing length:\n\nhist(MyDF$winglength, xlab = 'wing length (mm)', ylab = \"Count\")\n\n\n\n\nLet’s now take a logarithm and see if we can get a better idea of what the distribution of winglengths look like:\n\nhist(log10(MyDF$winglength), xlab = 'wing length (mm)', ylab = \"Count\")\n\n\n\n\n\nhist(log10(MyDF$winglength),xlab=\"log10(wing length (mm))\",ylab=\"Count\", \n     col = \"lightblue\", border = \"pink\") # Change bar and borders colors\n\n\n\n\nSo, taking a log really makes clearer what the distribution of wing lengths.\n\n\nSubplots\nLets explore another aspect of this dataset. We will look at the relation of food level to the winglengths. Lets do this by making two subplots next to each other using par. par is a tool to specify the locations of plots relative to each other. The first step is to break the food level aspect into high and low:\n\nMyDF_lowfood  <-  MyDF %>% filter(foodlevel == '0.1')\nMyDF_highfood <-  MyDF %>% filter(foodlevel == '1')\n\nNow we can start to look at whats going on with it.\n\npar(mfcol=c(2,1)) # initialize multi-paneled plot\npar(mfg = c(1,1)) # specify which sub-plot to use first \nhist(log10(MyDF_lowfood$winglength), # low-histogram\n     xlab = \"log10(wing length (mm))\", ylab = \"Count\", col = \"lightblue\", border = \"pink\", \n     main = '0.1 mg/larva/day') # Add title\npar(mfg = c(2,1)) # Second sub-plot\nhist(log10(MyDF_highfood$winglength), xlab=\"log10(wing length (mm))\",\n     ylab=\"Count\", col = \"lightgreen\", border = \"pink\", main = '1 mg/larva/day')\n\n\n\n\nThe par() function can set multiple graphics parameters (not just multi-panel plots), including figure margins, axis labels, and more. Check out the help for this function.\nAnother option for making multi-panel plots is the layout function.\n\n\nOverlaying plots\nBetter still, we can overlay plots:\n\npar(mfrow=c(1,1)) #initialize multi-paneled plot\nhist(log10(MyDF_lowfood$winglength), # high-food histogram\n     xlab=\"log10(wing length (mm))\", ylab=\"Count\", \n     col = rgb(1, 0, 0, 0.5), # Note 'rgb', fourth value is transparency\n     main = \"Larval food-level size overlap\") \nhist(log10(MyDF_highfood$winglength), col = rgb(0, 0, 1, 0.5), add = T) # Plot high food\nlegend('topleft',c('0.1','1'),   # Add legend\n       fill=c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5))) # Define legend colors\n\n\n\n\nPlot annotation with text can be done with either single or double quotes, i.e., ‘Plot Title’ or “Plot Title”, respectively. But it is generally a good idea to use double quotes because sometimes you would like to use an apostrophe in your title or axis label strings.\n\n\nBoxplots\nNow, let’s try plotting boxplots instead of histograms. These are useful for getting a visual summary of the distribution of your data.\n\nboxplot(log10(MyDF$winglength), xlab = \"\", ylab = \"log10(wing length (mm))\", main = \"Wing length\")\n\n\n\n\nNow let’s see how many temperatures are in included in the data:\n\nboxplot(log(MyDF$winglength) ~ MyDF$temp+MyDF$foodlevel, # Why the tilde?\n        xlab = \"temperature\", ylab = \"wing length (mm)\",\n        main = \"Wing length by temperature\")\n\n\n\n\nNote the tilde (~). This is to tell R to subdivide or categorize your analysis and plot by the factor location. More on this later.\nTo understand this plotting method, think of the full graph area as going from (0,0) in the lower left corner to (1,1) in the upper right corner. The format of the fig= parameter is a numerical vector of the form c(x1, x2, y1, y2), corresponding to c(bottom, left, top, right). First, par(fig=c(0,0.8,0,0.8)) sets up the scatterplot going from 0 to 0.8 on the x axis and 0 to 0.8 on the y axis, leaving some area for the boxplots at the top and right. The top boxplot goes from 0 to 0.8 on the x axis and 0.4 to 1 on the y axis. The right hand boxplot goes from 0.55 to 1 on the x axis and 0 to 0.8 on the y axis. You can experiment with these proportions to change the spacings between plots.\nThis plot is useful, because it shows you what the marginal distributions of the two variables are.\nBelow you will learn to use ggplot to produce a much more elegant, pretty multi-panel plots.\n\n\nSaving your graphics\nAnd you can also save the figure in a vector graphics format like a pdf. It is important to learn to do this, because you want to be able to save your plots in good resolution, and want to avoid the manual steps of clicking on the figure, doing “save as”, etc. So let’s save the figure as a PDF:\n\npar(fig=c(0,0.8,0,0.8))\npdf(\"larvalfoodwing.pdf\", # Open blank pdf page using a relative path\n    11.7, 8.3) # These numbers are page dimensions in inches\nhist(log(MyDF_lowfood$winglength), # Plot predator histogram (note 'rgb')\n     xlab=\"wing length (mm)\", ylab=\"Count\", col = rgb(1, 0, 0, 0.5), main = \"Overlap in wing length by larval food level\") \nhist(log(MyDF_highfood$winglength), # Plot prey weights\n     col = rgb(0, 0, 1, 0.5), \n     add = T)  # Add to same plot = TRUE\nlegend('topleft',c('0.1 mg/larva/day','1 mg/larva/day'), # Add legend\n       fill=c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5))) \n\ngraphics.off() # you can also use dev.off()\n\nAlways try to save results in a vector format, which can be scaled up to any size. For more on vector vs raster images/graphics, see this.\nNote that you are saving to the results directory now. This is a recommended project organization and workflow: store and retrieve data from a Data directory, keep your code and work from a Code directory, and save outputs to a results directory.\nYou can also try other graphic output formats. For example, png() (a raster format) instead of pdf(). As always, look at the help documentation of each of these commands!"
  },
  {
    "objectID": "data_wrangling.html#beautiful-graphics-in-r",
    "href": "data_wrangling.html#beautiful-graphics-in-r",
    "title": "",
    "section": "Beautiful graphics in R",
    "text": "Beautiful graphics in R\nR can produce beautiful visualizations, but it typically takes a lot of work to obtain the desired result. This is because the starting point is pretty much a “bare” plot, and adding features commonly required for publication-grade figures (legends, statistics, regressions, sub-plotting etc.) can require a lot of small and painful additional arguments to the plotting commands at the same time, or even additional steps).\nMoreover, it is very difficult to switch from one representation of the data to another (i.e., from boxplots to scatterplots), or to plot several datasets together. The R package ggplot2 overcomes these issues, and produces truly high-quality, publication-ready graphics suitable for papers, theses and reports.\n\n\n\n\n\n\nNote\n\n\n\n\n\n3D plots: Currently, ggplot2 cannot be used to create 3D graphs or mosaic plots (but see this). In any case, most of you won’t be needing 3D plots. If you do, there are many ways to do 3D plots using other plotting packages in R. In particular, look up the scatterplot3d and plot3D packages. I don’t think this is true anymore, I made a weird 3d monster for a paper a while ago.\n\n\n\nggplot2 differs from other approaches as it attempts to provide a “grammar” for graphics in which each layer is the equivalent of a verb, subject etc. and a plot is the equivalent of a sentence. All graphs start with a layer showing the data, other layers and attributes/styles are added to modify the plot. Specifically, according to this grammar, a statistical graphic is a “mapping” from data to geometric objects (points, lines, bars; set using geom) with aesthetic attributes (colour, shape, size; set using aes).\nFor more on the ideas underlying ggplot, see the book “ggplot2: Elegant Graphics for Data Analysis”, by H. Wickham (in your Reading directory). Also, the ggplot2 website is an excellent resource.\nggplot can be used in two ways: with qplot (for quick plotting) and ggplot for fully customized plotting.\nNote that ggplot2 only accepts data in data frames.\n\nQuick plotting with qplot\nqplot can be used to quickly produce graphics for exploratory data analysis, and as a base for more complex graphics. It uses syntax that is closer to the standard R plotting commands.\nWe will use the same Huxley et al. dataset again – you will soon see how much nicer the same types of plots you made above look when done with ggplot!\nFirst, load the package:\n\nrequire(ggplot2)\nMyDF <- subset(MyDF, winglength != 'NA') # omit individuals that did not survive to adulthood (i.e. wing length = NA)\n\n\nScatterplots\nLet’s start plotting the wing length vs adult lifespan:\n\nqplot(winglength, longevity, data = MyDF)\n\n\n\n\nAs before, let’s take logarithms and plot:\n\nqplot(log(winglength), log(longevity), data = MyDF)\n\n\n\n\nNow, color the points according to the food level:\n\nqplot(log(winglength), log(longevity), col=foodlevel, data = MyDF)\n\n\n\n\nBut the figure’s aspect ratio is not very nice. Let’s change it using the asp option:\n\nqplot(log(winglength), log(longevity), col=foodlevel, data = MyDF,asp = 1)\n\n\n\n\nThe same as above, but changing the shape:\n\nqplot(log(winglength), log(longevity), shape=foodlevel, data = MyDF,asp = 1)\n\n\n\n\n\n\nAesthetic mappings\nThese examples demonstrate a key difference between qplot (and indeed, ggplot2’s approach) and the standard plot command: When you want to assign colours, sizes or shapes to the points on your plot, using the plot command, it’s your responsibility to convert (i.e., “map”) a categorical variable in your data (e.g., type of feeding interaction in the above case) onto colors (or shapes) that plot knows how to use (e.g., by specifying “red”, “blue”, “green”, etc).\nggplot does this mapping for you automatically, and also provides a legend! This makes it really easy to quickly include additional data (e.g., if a new feeding interaction type was added to the data) on the plot.\nInstead of using ggplot’s automatic mapping, if you want to manually set a color or a shape, you have to use I() (meaning “Identity”).\n\n\nSetting transparency\nBecause there are so many points, we can make them semi-transparent using alpha so that the overlaps can be seen:\n\nqplot(log(winglength), log(longevity), col=foodlevel,alpha=I(0.5), data = MyDF,asp = 1)\n\n\n\n\nHere, try using alpha = .5 instead of alpha = I(.5) and see what happens."
  },
  {
    "objectID": "data_wrangling.html#box-plots-in-ggplot",
    "href": "data_wrangling.html#box-plots-in-ggplot",
    "title": "",
    "section": "Box plots in ggplot",
    "text": "Box plots in ggplot\nBoxplots will work largely the same way they have before. We need to make the geom argument in ggplot.\n\nqplot(temp, log(winglength), geom = 'boxplot', data = MyDF)\n\n\n\n\n\nSaving your plots\nFinally, let’s save a pdf file of the figure (same approach as we used before):\n\npdf(\"MyFirst-ggplot2-Figure.pdf\")\nprint(qplot(winglength, longevity, data = MyDF,log=\"xy\",\n            main = \"Relationship between wing length and adult lifespan\", \n            xlab = \"log(wing length) (mm)\", \n            ylab = \"log(adult lifespan) (days)\") + theme_bw())\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nUsing print ensures that the whole command is kept together and that you can use the command in a script.\n\n\nSome more important ggplot options\nOther important options to keep in mind:\n\n\n\n\n\n\n\nOption\n\n\n\n\n\nxlim\nlimits for x axis: xlim = c(0,12)\n\n\nylim\nlimits for y axis\n\n\nlog\nlog transform variable log = \"x\", log = \"y\", log = \"xy\"\n\n\nmain\ntitle of the plot main = \"My Graph\"\n\n\nxlab\nx-axis label\n\n\nylab\ny-axis label\n\n\nasp\naspect ratio asp = 2, asp = 0.5\n\n\nmargins\nwhether or not margins will be displayed\n\n\n\n\n\nThe geom argument\ngeom Specifies the geometric objects that define the graph type. The geom option is expressed as a R character vector with one or more entries. geom values include “point”, “smooth”, “boxplot”, “line”, “histogram”, “density”, “bar”, and “jitter”.\n\n\nAdvanced plotting: ggplot\nThe command qplot allows you to use only a single dataset and a single set of “aesthetics” (x, y, etc.). To make full use of ggplot2, we need to use the command ggplot, which allows you to use “layering”. Layering is the mechanism by which additional data elements are added to a plot. Each layer can come from a different dataset and have a different aesthetic mapping, allowing us to create plots that could not be generated using qplot(), which permits only a single dataset and a single set of aesthetic mappings.\nFor a ggplot plotting command, we need at least:\n\nThe data to be plotted, in a data frame;\nAesthetics mappings, specifying which variables we want to plot, and how;\nThe geom, defining the geometry for representing the data;\n(Optionally) some stat that transforms the data or performs statistics using the data.\n\nTo start a graph, we must specify the data and the aesthetics:\n\np <- ggplot(MyDF, aes(x = log(winglength),\n                      y = log(longevity),\n                      colour = foodlevel))\n\nHere we have created a graphics object p to which we can add layers and other plot elements.\nNow try to plot the graph:\n\np\n\n\n\n\nPlot is blank because we are yet to specify a geometry — only then can we see the graph:\n\nq <- p + geom_point()\nq\n\n\n\n\nWe can use the “+” sign to concatenate different commands:\n\nq <- p + geom_point(size=I(2), shape=I(10)) +\n  theme_bw() + # make the background white\n  theme(aspect.ratio=1) #make the plot square\nq\n\n\n\n\nLet’s remove the legend:\n\nq + theme(legend.position = \"none\") + theme(aspect.ratio=1)\n\n\n\n\nTo make it easier to read, we can plot the smoothed density of the data:\n\np <- ggplot(MyDF, aes(x = log(winglength/longevity), fill = foodlevel )) + geom_density()\np\n\n\n\n\n\np <- p + geom_density(alpha=0.5)\np\n\n\n\n\n\np <- p + geom_density()+facet_wrap( .~ foodlevel)\np\n\n\n\n\nYou can also combine categories like this\n\noptions(repr.plot.width=12, repr.plot.height= 14) # Change plot size (in cm)\n\nYou can also create a multifaceted plot:\n\nggplot(MyDF, aes(x = log(winglength), y = log(longevity))) +\ngeom_point() + facet_wrap( .~ temp + foodlevel, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIn facet_wrap, You can also free up just the x or y scales; look up the documentation for this function."
  },
  {
    "objectID": "data_wrangling.html#mathematical-display",
    "href": "data_wrangling.html#mathematical-display",
    "title": "",
    "section": "Mathematical display",
    "text": "Mathematical display\nLet’s try mathematical annotation on a axis, and in the plot area.\nFirst create some linear regression “data”:\n\nx <- seq(0, 100, by = 0.1)\ny <- -4. + 0.25 * x + rnorm(length(x), mean = 0., sd = 2.5)\n\nand put them in a dataframe\n\nmy_data <- data.frame(x = x, y = y)\n\nperform a linear regression\n\nmy_lm <- summary(lm(y ~ x, data = my_data))\n\n\n# plot the data\np <-  ggplot(my_data, aes(x = x, y = y,colour = abs(my_lm$residual))) +\n  geom_point() +\n  scale_colour_gradient(low = \"black\", high = \"red\") +\n  theme(legend.position = \"none\") +\n  scale_x_continuous(expression(alpha^2 * pi / beta * sqrt(Theta)))\n\np\n\n\n\n\nadd the regression line and throw some math on the plot\n\np <- p + geom_abline(intercept = my_lm$coefficients[1][1],\n         slope = my_lm$coefficients[2][1], colour = \"red\") +\n         geom_text(aes(x = 60, y = 0,label = \"sqrt(alpha) * 2* pi\"), \n         parse = TRUE, size = 6, colour = \"blue\")\n\np\n\n\n\n\nVoila! You have just explored complicated data visually!"
  },
  {
    "objectID": "data_wrangling.html#readings-resources",
    "href": "data_wrangling.html#readings-resources",
    "title": "",
    "section": "Readings & Resources",
    "text": "Readings & Resources\nCheck out DataDataData!, Visualization and R, under readings on the TheMulQuaBio repository.\n\nData management, reformatting and cleaning\n\nBrian McGill’s Ten commandments for data management\nThis paper covers similar ground (available in the readings directory): Borer et al (2009). Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America, 90(2), 205-214.\nDasu, T. & Johnson, T. Exploratory Data Mining and Data Cleaning. (John Wiley & Sons, Inc., 2003). doi:10.1002/0471448354\nWide vs. long data\nHadley Wickham’s excellent article about “tidy” data.\nWrangler\nAn interactive framework for data cleaning\n\n\n\nVisualization\n\nTen Simple Rules for Better Figures\nhttps://clauswilke.com/dataviz/\nRolandi et al. “A Brief Guide to Designing Effective Figures for the Scientific Paper”, doi:10.1002/adma.201102518\nThe classic Tufte; Available in the Imperial College Central Library. I have also added extracts and a related book in pdf on the git repository. BTW, check out what Tufte thinks of PowerPoint.\nTufte in R"
  },
  {
    "objectID": "data_wrangling.html#introduction-1",
    "href": "data_wrangling.html#introduction-1",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nIn this section you will learn how to use R to explore your data and determine appropriate statistical analyses.\nIdeally, you would like to design experiments (manipulations and/or observations) that are appropriate for the question you want to answer. However, you still need to explore you data to determine what kind of statistical analyses are appropriate for your data, because:\n\nYour experiments or observations may not go as planned (do they ever?), and\nYou might have somebody else’s data to analyse (very common in this era of “Big Data”).\n\nBy the time you have worked through this section, you should be able to:\n\nProvided sufficient information is available, be able to judge whether the sampling design used to generate a particular dataset was appropriate\nCalculate basic statistical measures on your data to determine its properties.\nDetermine if your sample sizes are adequate, especially for a specific statistical test.\n\nWe are going to start off with the simplest of scenarios for statistical testing — that you want to determine whether a sample, or a pair of samples meet some expectation (hypothesis) or not.\nFirst, some let’s revisit some key concepts and terminology."
  },
  {
    "objectID": "data_wrangling.html#some-statistical-parlance",
    "href": "data_wrangling.html#some-statistical-parlance",
    "title": "",
    "section": "Some statistical parlance",
    "text": "Some statistical parlance\nThe following terms are important for you to get familiar with:\n\n(Statistical) Population: A complete set of items that share at least one attribute of interest. This attribute of interest is the target of your statistical analysis. For example, if we are interested in studying the weight of year-old cod in the Oceans, the population consists of all year-old cod, but more specifically, the weight measurements of all the individuals of the cod population is what we want to analyse.\n\n\n\n\n\n\n\n\n\n\n\n\n(Statistical) Distribution: A mathematical description (expressed as a mathematical equation) of the properties of a population of interest. Theoreticians have come up with a bunch of distributions (e.g., Gaussian or Normal, Poisson, Binomial, etc.) that are appropriate for different kinds of data. Figuring out which distribution best describes a population of interest is one of the first steps in a statistical analysis. The primary goal of experimental design is to collect and measure sufficient individuals of a population to adequately characterize the statistical properties of an attribute (e.g., body weight of yearling cod) of interest. That is, the statistical distribution that best characterizes the attribute.\n(Data or Population) Sample: A data sample is a set of measurements of the attribute of interest collected from a (statistical) population (all the individuals of interest) by a defined procedure (sampling methodology). In the cod example above, this could be the weight of every individual of a subset (the sample) of the year-old cod population. This could be from a particular location, such as the Atlantic ocean.\n(Statistical) Parameter : A measure of some attribute of the (theoretical) statistical distribution that is supposed to represent your population. An example would be the average weight of all yearling cod, which presumably follow some sort of distribution. In practice, this is not measurable because the population is much too large or incompletely inaccessible/invisible — imagine measuring the weight of every year-old cod individual in the Atlantic ocean!\nStatistic (singular): An estimate of a statistical parameter of the population of interest, obtained by calculating the measure for a sample. An example would be the average or mean weight of individuals in a sample of one-year old cod in the Atlantic Ocean. This is also known as a descriptive statistic. Therefore, a Statistic is to a Statistical Parameter what a Sample is to the (Statistical) Population. For example, the average of a sample of cod weights is a statistic that estimates the “real” average of the weights of the entire one-year Cod population (which is its statistical parameter). This real average is also the mean value of the theoretical distribution (e.g., Gaussian) that the population is expected to follow.\nHypothesis: An informed postulate about an attribute of your population of interest. For example, you may hypothesize that the one-year old cod population’s mean weight has declined over the last two decades because of preferential fishing of larger individuals. You will typically confront your main hypothesis with a Null Hypothesis, to minimize the risk of making a significant Type I error. This is the probability of wrongly accepting an alternative (or main) hypothesis even is not really true, and rejecting the null hypothesis (e.g., the yearling cods have in reality not declined in weight, but you wrongly infer that they have). This is a big NO NO from a scientific and philosophical standpoint. The rate or probability of the Type I error is denoted by the Greek letter \\alpha, and equals the significance level of a statistical test. Wrongly rejecting a true alternative (main) hypothesis is also a Type II error."
  },
  {
    "objectID": "data_wrangling.html#descriptive-statistics",
    "href": "data_wrangling.html#descriptive-statistics",
    "title": "",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe key statistical measures that describe a sample (or a population) are:\n\nMean (or average, or more precisely, the arithmetic mean): This is a measure of the central tendency of the sample and population (what values most of the data tend to have). Given a sample x_1,x_2,\\ldots,x_n of size n, the mean is typically denoted by a \\bar{x}:\n\n\\bar{x} =  \\frac{x_{1} + x_{2} + \\dots +x_{n}}{n} = \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}\nThat is, it is the sum of all the values in a sample divided by the number, n, of items in the sample.\n\nStandard deviation: A measure of spread of the population around the mean. This is typically denoted by s,and is calculated as:\n\ns = \\sqrt{\\frac{(\\bar{x} - x_{1})^{2} + (\\bar{x} - x_{2})^2 + \\dots + (\\bar{x} - x_{n})^{2}}{n-1}} =  \\sqrt{\\frac{\\sum_{i=1}^n (\\bar{x} - x_{i})^2}{n-1}}\nThat is, it is the square root of the sum of squares (“SS”) of the differences between each item in the sample and the mean, divided by the degrees of freedom, “df” remaining in the data set (n-1). df is the sample size, n, minus the number of statistical parameters estimated from the data set. This is to reduce the bias in your estimate of the statistic, as you are calculating it from the sample, and not the whole theoretical population.\nThus, the formula for s above has n-1 in its denominator because, to work out the standard deviation, you must have already estimated the mean (\\bar{x}) from the same data set. This removes 1 degree of freedom. Also, note that the sample variance, s^2 is the square of standard deviation. Or, in other words, the standard deviation is the square-root of the variance!\n\nMedian: The above two statistics (mean and sd) are particularly meaningful when the sample and population have a symmetric distribution (e.g., normal or gaussian). When the distribution is not symmetric (that is, it is skewed), the median is a better measure of central tendency. This is the middle value in the ordered set of data. That is, exactly 50% of the data lie below and 50% lie above the median.\nOther descriptive statistics you should keep in mind is the range (difference between the largest and smallest values), and the quartiles (values lying in the data divided into the intervals [{1\\over4}, {1\\over 2},{3\\over 4}, 1] or at 1% intervals (percentiles). Box-plots, which you have seen, represent a number of these statistics in one figure, as you will soon learn in practice below."
  },
  {
    "objectID": "data_wrangling.html#descriptive-statistics-in-r",
    "href": "data_wrangling.html#descriptive-statistics-in-r",
    "title": "",
    "section": "Descriptive statistics in R",
    "text": "Descriptive statistics in R\nHere are the R commands for the key descriptive statistics:\n\n\n\nCommand\nFunctionality\n\n\n\n\nmean(x)\nCompute mean (of a vector or matrix)\n\n\nsd(x)\nStandard deviation\n\n\nvar(x)\nVariance\n\n\nmedian(x)\nMedian\n\n\nquantile(x,0.05)\nCompute the 0.05 quantile\n\n\nrange(x)\nRange of the data\n\n\nmin(x)\nMinimum\n\n\nmax(x)\nMaximum\n\n\nsum(x)\nSum all elements"
  },
  {
    "objectID": "data_wrangling.html#data-types-and-statistical-distributions",
    "href": "data_wrangling.html#data-types-and-statistical-distributions",
    "title": "",
    "section": "Data types and (statistical) distributions",
    "text": "Data types and (statistical) distributions\nYou will typically encounter or sample the following main types of data:\n\nContinuous numeric: Data that can take decimal values (real numbers) such as human height or weight. These may be unbounded (any value between negative infinity to positive infinity), or bounded (e.g., between or zero and some upper positive number) like human weight. This is the numeric or real data type in R.\nDiscrete numeric: Integer (whole) numbers such as counts of individuals in a population, e.g., The number of bacteria in a ml of pond water. This is the int data type in R.\nPercentage (proportion): A particular kind of numeric data that is strictly bounded between 0 and 100. The fact that you can never get samples of percentages that exceed these bounds makes such data tricky to analyse. This also falls under the numeric or real data type in R.\nCategorical: Consisting of a fixed number of discrete categories. These are typically stored as a factor in R, with the categories stored as levels in character (string) data type format. For example, the factor “Type.of.feeding.interaction” from the predator-prey dataset you have used previously had five levels: “insectivorous”, “piscivorous”, “planktivorous”, “predacious”, and “predacious/piscivorous”.\nBinary (presence/absence): A special type of categorical data are binary, where only two categories/states/values are possible: (1, 0) (or “present”, “absent”) (e.g., a disease symptom). These may be stored as integer, character, or boolean (TRUE/FALSE) in R.\n\nWhile designing experiments or exploring data, you need to keep in mind that each data type will often be best-represented by a particular statistical distribution. For example, continuous numeric data are often normally distributed. On the other hand, count data are likely to be distributed according to the Poisson distribution.\nIf you are lucky, you will mostly have to deal with data that are continuous or discrete numeric, which are the most straightforward to analyse using Linear models (more on that in subsequent sections). However, some of the most interesting and important problems in biology involve proportion (percentage), categorical and binary data (e.g., Presence or absence of a disease symptom).\nFor example, think about what type of data, and what type of distribution, a sample of the following is likely to be:\n\nWavelength of light\nTemperature\nEgg clutch size\nRate of a reaction\nEye-colour\nScore in Scrabble\nUndergraduate Degree class\nGround-cover of grass in a quadrat\nWinning side in chess"
  },
  {
    "objectID": "data_wrangling.html#sampling-from-distributions-in-r",
    "href": "data_wrangling.html#sampling-from-distributions-in-r",
    "title": "",
    "section": "Sampling from distributions in R",
    "text": "Sampling from distributions in R\nYou can generate samples form many statistical distributions in R. This is a handy thing to know because this allows you to simulate a sampling “experiment” . In particular, the following R commands are important:\n\n\n\nCommand\nFunctionality\n\n\n\n\nrnorm(n, m=0, sd=1)\nDraw n normally-distributed random numbers with mean = 0 and sd = 1\n\n\ndnorm(x, m=0, sd=1)\nDensity function of the normal distribution, which can be used to calculate the probability of a particular event or a set of independent events\n\n\nqnorm(x, m=0, sd=1)\nCumulative density function\n\n\nrunif(n, min=0, max=2)\nTwenty random numbers from uniform distribution with bounds [0, 2]\n\n\nrpois(n, lambda=10)\nTwenty random numbers from the Poisson(\\lambda) distribution\n\n\n\nLet’s try some of these. Generate a random sample of 10:\n\nMySample <- rnorm(10, m=0, sd=1)\n\n\nMySample\n\n [1] -1.5494623  0.0032529 -0.4018656 -0.5701882 -1.1574766  0.2600550\n [7] -1.7426185 -0.1654233 -0.2322285 -1.7138582\n\n\n\nhist(MySample)\n\n\n\n\nProbability of getting a value of 1 or -1 from a normally distributed random number with mean = 0 and sd = 1:\n\ndnorm(1, m=0, sd=1)\n\n[1] 0.2419707\n\n\n\ndnorm(-1, m=0, sd=1)\n\n[1] 0.2419707\n\n\nProbability of getting large values given the same distribution:\n\ndnorm(10, m=0, sd=1)\n\n[1] 7.694599e-23\n\n\nVery small!\n\ndnorm(100, m=0, sd=1)\n\n[1] 0\n\n\nZero!\nLook up the documentation and examples for the other commands/functions listed above and many others that are available."
  },
  {
    "objectID": "data_wrangling.html#two-basic-rules-of-experimental-design-and-sampling",
    "href": "data_wrangling.html#two-basic-rules-of-experimental-design-and-sampling",
    "title": "",
    "section": "Two basic rules of experimental design and sampling",
    "text": "Two basic rules of experimental design and sampling\nIn general, while designing experiments, and sampling from a population, there are two key (and simple) rules:\n\nThe more you sample, the more your sample’s distribution will look like the population distribution (obviously!)\nThe more you sample, the closer will your sample statistic be to the population’s statistical parameter (the central limit theorem; when the statistical parameter is the mean, this is the “law of large numbers”)\n\nLet’s test rule 1 using R. We will perform a “experiment” by generating random samples of increasing size from the normal distribution:\n\nMySample5 <- rnorm(5, m=0, sd=1) # Draw 5 normal random nos w/ mean 0 and s.d. 1:\nMySample10 <- rnorm(10, m=0, sd=1) \nMySample20 <- rnorm(20, m=0, sd=1) \nMySample40 <- rnorm(40, m=0, sd=1)\nMySample80 <- rnorm(80, m=0, sd=1)\nMySample160 <- rnorm(160, m=0, sd=1)\n\nNow let’s visualize these “samples”:\n\npar(mfcol = c(2,3)) #initialize multi-paneled plot\npar(mfg = c(1,1)); hist(MySample5, col = rgb(1,1,0), main = 'n = 5') \npar(mfg = c(1,2)); hist(MySample10, col = rgb(1,1,0), main = 'n = 10') \npar(mfg = c(1,3)); hist(MySample20, col = rgb(1,1,0), main = 'n = 20') \npar(mfg = c(2,1)); hist(MySample40, col = rgb(1,1,0), main = 'n = 40') \npar(mfg = c(2,2)); hist(MySample80, col = rgb(1,1,0), main = 'n = 80') \npar(mfg = c(2,3)); hist(MySample160, col = rgb(1,1,0), main = 'n = 160') \n\n\n\n\nRule 2 above states that if I was to repeat even n = 5 sufficient number of times, you would get a good estimate of mean (= 0) and standard deviation (= 1) of the normal distribution we sampled from. Try doing it. You can take increasing numbers of samples of 5, take their mean, and check how close to get to 0 as you increase your number of samples of 5.\nIf you give due regard to the two rules of experimental design above, and consider the type of distribution your population of interest follows, you will have taken some basic, important steps towards designing an effective study.\nA more rigorous method for designing experiments is to perform a power analysis. Power analyses allow you to estimate the minimum sample size required to be able to detect (while minimizing Type I error) an effect of a given size. Covering this is outside the scope of the current course, but you might want to have a look at this resource.\nOK, so you have performed your experiments or field sampling, or some data have fallen into your lap. Let’s move on to data exploration."
  },
  {
    "objectID": "data_wrangling.html#data-exploration",
    "href": "data_wrangling.html#data-exploration",
    "title": "",
    "section": "Data exploration",
    "text": "Data exploration\n\nSome general tips\nStatistics is a bit of an art.\nThat is why a priori visualization is important, and directly cutting to the (statistical) chase can often leave one floundering with confusing coefficient estimates and model fits (or over-fits) and overall uncertainty about whether you have picked the right statistical test.\n\nSo no matter what, always first look at the distribution of the response (the “raw data”). If you see multi-modality (multiple peaks), it might mean that some process or effect is generating it. So, in the dragonfly-damselfly example below, a preliminary visualization of the densities of genome size immediately tell you that there are actually two populations (e.g., two levels of effects, or a process that generates the two populations with different central tendencies \\mu_A and \\mu_B).\nIf it is a regression-type problem, look at the marginal distributions (distributions of the x and y variables) - similar insights can be gained.\nAlso, one can look at the distributions within the effects to look for consistency of both shape (sample of the underlying population’s distribution type) and spread (sample of the underlying population’s variance).\n\nIf there is consistency in both, the battle is pretty much won, because normal or not, there are statistical methods (GLMs, for example) that can be used.\nIf there is consistency in distribution type but not in variance, half the battle is won. Unequal variances very often tend to be a problem with unbalanced sampling (across effects/treatments), so you can throw in mixed effects (so, GLMMs) which allows you to use empirical information (the data) across effects to help correct for it.\nAnd also, very often, tests are robust to unequal variances (you would need to find a statistical paper, or do your own simulations to support this - so very often better to stick with mixed effects).\nIf there is consistency in variance but not shape, or inconsistency in both, then you have a harder problem, and may have to resort to transforming some data and not others (not ideal, to be avoided).\n\n\n\n\nA case study\nAs a case study, we will use data from a paper looking at the relationship between genome size and body size across species of dragonflies and damselflies (Odonata):\n\nArdila-Garcia, AM & Gregory, TR (2009) ‘An exploration of genome size diversity in dragonflies and damselflies (Insecta: Odonata)’ Journal of Zoology, 278, 163 - 173\n\nYou will work with the script file ExpDesign.R, which performs exploratory analyses on the data in GenomeSize.csv. Let’s go through the code block by block.\n\\star Get the script ExpDesign.R from the TheMulQuaBio repository and put it in your own Code directory.\n\\star Also get GenomeSize.csv\n\\star Open the script ExpDesign.R in RStudio (or some other text editor).\nUse the shift and arrow keys to select the code in block (2), including the comments.\n\ngenome <- read.csv('activities/data/GenomeSize.csv')\n\nNote the relative path ../, which will work assuming that you are working from your code directory (that is, you have set your working directory (using setwd()) to code).\nThis first line (block (1)) reads in the data, as you have learned previously.\n\\star Now run the code in block (2) line by line.\n\nhead(genome) # this won't look so nice on your computer!\n\n    Suborder    Family              Species GenomeSize GenomeSE GenomeN\n1 Anisoptera Aeshnidae    Aeshna canadensis       2.20       NA       1\n2 Anisoptera Aeshnidae    Aeshna constricta       1.76     0.06       4\n3 Anisoptera Aeshnidae       Aeshna eremita       1.85       NA       1\n4 Anisoptera Aeshnidae Aeshna tuberculifera       1.78     0.10       2\n5 Anisoptera Aeshnidae       Aeshna umbrosa       2.00       NA       1\n6 Anisoptera Aeshnidae    Aeshna verticalis       1.59       NA       1\n  BodyWeight TotalLength HeadLength ThoraxLength AdbdomenLength ForewingLength\n1      0.159       67.58       6.83        11.81          48.94          45.47\n2      0.228       71.97       6.84        10.72          54.41          46.00\n3      0.312       78.80       6.27        16.19          56.33          51.24\n4      0.218       72.44       6.62        12.53          53.29          49.84\n5      0.207       73.05       4.92        11.11          57.03          46.51\n6      0.220       66.25       6.48        11.64          48.13          45.91\n  HindwingLength ForewingArea HindwingArea MorphologyN\n1          45.40       369.57       483.61           2\n2          45.48       411.15       517.38           3\n3          49.47       460.72       574.33           1\n4          48.82       468.74       591.42           2\n5          45.97       382.48       481.44           1\n6          44.91       400.40       486.97           1\n\n\n\nstr(genome) # Check what the data columns contain\n\n'data.frame':   100 obs. of  16 variables:\n $ Suborder      : chr  \"Anisoptera\" \"Anisoptera\" \"Anisoptera\" \"Anisoptera\" ...\n $ Family        : chr  \"Aeshnidae\" \"Aeshnidae\" \"Aeshnidae\" \"Aeshnidae\" ...\n $ Species       : chr  \"Aeshna canadensis\" \"Aeshna constricta\" \"Aeshna eremita\" \"Aeshna tuberculifera\" ...\n $ GenomeSize    : num  2.2 1.76 1.85 1.78 2 1.59 1.44 1.16 1.44 1.2 ...\n $ GenomeSE      : num  NA 0.06 NA 0.1 NA NA NA NA NA NA ...\n $ GenomeN       : int  1 4 1 2 1 1 1 1 1 1 ...\n $ BodyWeight    : num  0.159 0.228 0.312 0.218 0.207 0.22 0.344 0.128 0.392 0.029 ...\n $ TotalLength   : num  67.6 72 78.8 72.4 73 ...\n $ HeadLength    : num  6.83 6.84 6.27 6.62 4.92 6.48 7.53 5.74 8.05 5.28 ...\n $ ThoraxLength  : num  11.8 10.7 16.2 12.5 11.1 ...\n $ AdbdomenLength: num  48.9 54.4 56.3 53.3 57 ...\n $ ForewingLength: num  45.5 46 51.2 49.8 46.5 ...\n $ HindwingLength: num  45.4 45.5 49.5 48.8 46 ...\n $ ForewingArea  : num  370 411 461 469 382 ...\n $ HindwingArea  : num  484 517 574 591 481 ...\n $ MorphologyN   : int  2 3 1 2 1 1 4 1 1 1 ...\n\n\nHave a good look at the data. There are three factors (categorical variables): Suborder, splitting the species into dragonflies (Anisoptera) and damselflies (Zygoptera); Family, splitting the species further into 9 taxonomic families; and Species, giving the latin binomial for each species in the table. The remaining columns are measurements of genome size (in picograms) and measurements of body size and morphology (in grams, mm and mm^2). There are two columns ending with an N that show the sample size from which the observations for each species are taken and a column ending SE showing standard errors.\nOne thing you should see in the output from head or str is that there are some observations marked as NA – this is the way R shows missing data. It is important to check how much missing data there are in a dataset, so we’ll use another function that includes this information. Many R functions refuse to use variables containing missing data — this is just R being careful and you can add na.rm=TRUE into most functions to avoid this problem.\n\\star Run the summary line from the script window (block 3).\n\nsummary(genome)\n\n   Suborder            Family            Species            GenomeSize    \n Length:100         Length:100         Length:100         Min.   :0.4100  \n Class :character   Class :character   Class :character   1st Qu.:0.7375  \n Mode  :character   Mode  :character   Mode  :character   Median :0.9400  \n                                                          Mean   :1.0143  \n                                                          3rd Qu.:1.1800  \n                                                          Max.   :2.3600  \n                                                                          \n    GenomeSE          GenomeN        BodyWeight       TotalLength   \n Min.   :0.00600   Min.   : 1.00   Min.   :0.00200   Min.   :22.82  \n 1st Qu.:0.02000   1st Qu.: 1.00   1st Qu.:0.01200   1st Qu.:32.35  \n Median :0.03000   Median : 1.00   Median :0.04000   Median :41.41  \n Mean   :0.03398   Mean   : 2.36   Mean   :0.07486   Mean   :43.76  \n 3rd Qu.:0.04000   3rd Qu.: 3.00   3rd Qu.:0.10975   3rd Qu.:51.74  \n Max.   :0.10000   Max.   :12.00   Max.   :0.39200   Max.   :82.39  \n NA's   :55                        NA's   :2         NA's   :2      \n   HeadLength    ThoraxLength    AdbdomenLength  ForewingLength \n Min.   :1.11   Min.   : 3.150   Min.   :13.29   Min.   :12.07  \n 1st Qu.:1.91   1st Qu.: 5.645   1st Qu.:24.62   1st Qu.:20.76  \n Median :3.75   Median : 7.585   Median :28.39   Median :27.55  \n Mean   :3.62   Mean   : 8.283   Mean   :31.89   Mean   :29.74  \n 3rd Qu.:4.93   3rd Qu.:10.720   3rd Qu.:38.12   3rd Qu.:36.89  \n Max.   :8.05   Max.   :16.190   Max.   :61.35   Max.   :55.99  \n NA's   :3      NA's   :2        NA's   :2       NA's   :4      \n HindwingLength   ForewingArea     HindwingArea     MorphologyN    \n Min.   :11.45   Min.   : 21.04   Min.   : 19.39   Min.   : 1.000  \n 1st Qu.:19.95   1st Qu.: 62.94   1st Qu.: 56.66   1st Qu.: 1.000  \n Median :26.80   Median :145.95   Median :177.03   Median : 2.000  \n Mean   :28.54   Mean   :174.32   Mean   :205.84   Mean   : 2.949  \n 3rd Qu.:35.40   3rd Qu.:226.91   3rd Qu.:279.36   3rd Qu.: 4.000  \n Max.   :54.59   Max.   :527.92   Max.   :632.77   Max.   :13.000  \n NA's   :4       NA's   :4        NA's   :4        NA's   :2       \n\n\nNote that each column gets a separate summary! Look carefully at the output. There is a column for each variable: for factors, it provides a short table of the number of observations in each level and for continuous variables, it provides some simple summary statistics about the distribution (range, quartiles, mean and median), and the number of missing values."
  },
  {
    "objectID": "data_wrangling.html#visualise-distributions-of-the-variables",
    "href": "data_wrangling.html#visualise-distributions-of-the-variables",
    "title": "",
    "section": "Visualise distributions of the variables",
    "text": "Visualise distributions of the variables\nThe summary function shows us the basic distribution (range, quartiles, mean and median) of a continuous variable, but this is easier to interpret if we visualise it. We’ll look at two ways:\n\nHistogram: In the simplest form, this shows the number of observations of the variable falling into a set of bins spanning the range of the variable. The option breaks allows you to change the number of bins.\nDensity plot: Rather than showing blocks of counts, the density plot shows a continuous smooth line. This is a smoothed estimate of the how frequently data is observed across the range of values and the bandwidth (bw=0.1) controls the degree of the smoothing.\n\n\\star Go to block (4) of the script and run each line separately, looking at the output.\n\nhist(genome$GenomeSize, breaks=10)\n\n\n\n\n\nplot(density(genome$GenomeSize, bw=0.1))\n\n\n\n\nIn your code editor, change the values of breaks and bw (gfor example breaks=5 and bw=0.05), and re-run these lines to see how this affects the graph. Basically, with both types of graph you can look at the data too coarsely or too finely.\nThe graphs you’ve just created look at genome size. Add a copy of those two lines of code in the script and change them to look at the variable TotalLength. You will need to alter the density function to ignore missing values (na.rm=TRUE) and to play around with the bandwidth. You should get something like this:"
  },
  {
    "objectID": "data_wrangling.html#take-a-quick-look-at-effects-of-certain-factors",
    "href": "data_wrangling.html#take-a-quick-look-at-effects-of-certain-factors",
    "title": "",
    "section": "Take a quick look at effects of certain factors",
    "text": "Take a quick look at effects of certain factors\nR has a special way of describing a model that defines the response variable and the explanatory variables (“factors”). This is called a ‘formula’ and is used to define linear models (more on these in a later sections). The same structure is used in many plotting functions and will put the response variable on the y axis and the explanatory variable on the x axis. The structure is “response variable ~ explanatory variables”. We will look at multiple explanatory variables in a later section but an example with one explantory variable (factor) is:\nGenome Size ~ Suborder\nThis formula tells R to model genome size ‘as a function of’ (~) the suborders of Odonata. When using this syntax in a plot function, the result will be to plot genome size as a function of the suborders."
  },
  {
    "objectID": "data_wrangling.html#compare-distribution-of-the-variable-across-levels-of-a-factor",
    "href": "data_wrangling.html#compare-distribution-of-the-variable-across-levels-of-a-factor",
    "title": "",
    "section": "Compare distribution of the variable across levels of a factor",
    "text": "Compare distribution of the variable across levels of a factor\nAlthough looking at the distribution of variables is a good first step, we often want to compare distributions. In this case, we might want to know how genome size varies between dragonflies and damselflies. The first way we will look at is using boxplots — these show the median and the 25% and 75% quantiles as a box, with whiskers extending to the minimum and maximum. More extreme outliers are plotted independently as points. The plot function in R automatically generates a boxplot when the explanatory variable is a factor.\n\\star Go to block 5 of the script and run the first line, looking at genome size between the two suborders:\nDuplicate and alter this line to look at the same plot for total length. You should get a plot like this:\n\n\n\n\n\n\n\n\n\nAlthough histograms are great for one variable, plotting two histograms on top of one another rarely works well because the overlapping bars are hard to interpret (recall the predator-prey body size example). Density plots don’t have this problem, but it takes a bit more code to create the plot.\n\\star Block 6 of the script uses the subset function to create two new data frames separating the data for dragonflies and damselflies. Run the first two lines of this block:\n\nAnisoptera <- subset(genome, Suborder=='Anisoptera') #The dragonflies\nZygoptera <- subset(genome, Suborder=='Zygoptera') #The damselflies\n\nRemember that the arrow symbol (<-) is used to save the output of a function into a new object in R — if you use ls() in the console, you will see the two new data frames.\nIn the console, use str and summary to explore these two new dataframes. For example:\n\nhead(Anisoptera)\n\n    Suborder    Family              Species GenomeSize GenomeSE GenomeN\n1 Anisoptera Aeshnidae    Aeshna canadensis       2.20       NA       1\n2 Anisoptera Aeshnidae    Aeshna constricta       1.76     0.06       4\n3 Anisoptera Aeshnidae       Aeshna eremita       1.85       NA       1\n4 Anisoptera Aeshnidae Aeshna tuberculifera       1.78     0.10       2\n5 Anisoptera Aeshnidae       Aeshna umbrosa       2.00       NA       1\n6 Anisoptera Aeshnidae    Aeshna verticalis       1.59       NA       1\n  BodyWeight TotalLength HeadLength ThoraxLength AdbdomenLength ForewingLength\n1      0.159       67.58       6.83        11.81          48.94          45.47\n2      0.228       71.97       6.84        10.72          54.41          46.00\n3      0.312       78.80       6.27        16.19          56.33          51.24\n4      0.218       72.44       6.62        12.53          53.29          49.84\n5      0.207       73.05       4.92        11.11          57.03          46.51\n6      0.220       66.25       6.48        11.64          48.13          45.91\n  HindwingLength ForewingArea HindwingArea MorphologyN\n1          45.40       369.57       483.61           2\n2          45.48       411.15       517.38           3\n3          49.47       460.72       574.33           1\n4          48.82       468.74       591.42           2\n5          45.97       382.48       481.44           1\n6          44.91       400.40       486.97           1\n\n\nNow that we’ve got the data separated we can go about plotting the two curves.\n\\star Run the next two lines of code in block 6. The first draws the plot for damselflies and the second adds a line for the dragonflies:\n\nplot(density(Zygoptera$GenomeSize), xlim=c(0.1, 2.7), ylim=c(0,1.7))\nlines(density(Anisoptera$GenomeSize), col='red')\n\n\n\n\nDuplicate these last two lines of code and edit them to generate a similar plot for total body length. You will need to edit the code to change the range of the x and y axes (xlim and ylim) to get both curves to fit neatly on to the graph. It should look like this:"
  },
  {
    "objectID": "data_wrangling.html#explore-further-by-scatter-plotting-two-variables",
    "href": "data_wrangling.html#explore-further-by-scatter-plotting-two-variables",
    "title": "",
    "section": "Explore further by scatter-plotting two variables",
    "text": "Explore further by scatter-plotting two variables\nOnce we’ve looked at the distribution of variables, the next thing is to look at the relationships between continuous variables using scatterplots. The plot function in R automatically generates a scatterplot when the explanatory variable is continuous, so we can use the same syntax and structure as for the boxplot above.\n\\star Go to block (7) of the script and run the plot commands. The third one plots genome size as a function of body weight:\n\nplot(GenomeSize ~ BodyWeight, data = genome)\n\n\n\n\nThe scatterplot seems to show a weak relationship between genome size and morphology. But maybe dragonflies and damselflies show different relationships, and we can’t distinguish between them! To explore this possibility, we need to plot the two orders using different colours or plot characters. In the next code block, we will customize the plots to show different types of points for each suborder. It is done by using indexing.\n\\star Run the first three lines of code in block 8. There are two levels of suborder and these two lines set up a colour and a plot symbol that will be used for each one.\n\nstr(genome$Suborder) #Confirm that there are two levels under suborders \n\n chr [1:100] \"Anisoptera\" \"Anisoptera\" \"Anisoptera\" \"Anisoptera\" ...\n\n\nYou can see that there are two levels, with Anisoptera first and then Zygoptera. You can also see that these are stored as numeric values: 1 refers to the first level and 2 the second. We can use these as indices to pair the colours and plot symbols to each suborder. These are set in the plot function using the options col= and pch=, which stands for ” plot character”.\n\nmyColours <- c('red', 'blue') # So choose two colours\nmySymbols <- c(1,3) # And two different markers\n\nRun the next plot command to see the resulting plot:\nThus each point gets the appropriate colour and symbol for its group. This is the indexing: myColours[Suborder] and mySymbols[Suborder] automatically assign the two colors and two symbols to the two suborders.\nThere are a lot of built in colours and plot symbols in R, so the next thing to experiment with is changing these to your own versions.\n\\star In the R commandline, type in the function colors().\nYou’ll see a long list of options to choose from, so pick two to replace red and blue in the plot above.\nThe options for the plot symbols are shown below. Pick two to replace the current symbol choices.\n\n\n\n\n\n\n\n\n\nRerun the plot function and see what you get!"
  },
  {
    "objectID": "data_wrangling.html#save-your-results",
    "href": "data_wrangling.html#save-your-results",
    "title": "",
    "section": "Save your results",
    "text": "Save your results\n\nSaving the exploratory graphics\nThe function pdf opens a new empty pdf file which can then be used to plot graphs. You can set the width and the height of the page size in the pdf but note that this is set in inches. When you have finished creating a plot, the function dev.off closes the pdf file and makes it readable.\n\\star Open ‘GenomeSize.pdf’ in a PDF reader. It uses the original colours and plot symbols. Close the file and then delete it from the folder.\n\\star Now go back to the script in R and select and run all the code in block (9)\nGo back to the results folder. The pdf file should have been recreated — open it and it should now use your choice of colours and symbols.\n\n\nSaving other results\nYou can also save the data and variables in R format — the original data, two subsets of the data and the two sets of colours and symbols. This will make it easy to restart where you left off. However, We can recreate the data subsets easily, so we’ll just save the data and your colour sets.\n\\star Go to the script window and run the final line in block (10)\nStill in the script window, choose ‘File \\triangleright Save’ to save your changes to the script file.\nQuit from R by typing q() at the R command prompt. You can also use ctrl+D in Unix)."
  },
  {
    "objectID": "data_wrangling.html#from-data-exploration-to-statistical-analysis",
    "href": "data_wrangling.html#from-data-exploration-to-statistical-analysis",
    "title": "",
    "section": "From data exploration to statistical analysis",
    "text": "From data exploration to statistical analysis\nAfter you have performed your data exploration, you are in a position to make an informed decision about what statistical analyses to perform. Here is a decision tree that you can use, and which includes the methods you will learn in the following sections:"
  },
  {
    "objectID": "data_wrangling.html#practice-problem-for-data-wrangling",
    "href": "data_wrangling.html#practice-problem-for-data-wrangling",
    "title": "",
    "section": "Practice Problem for Data Wrangling",
    "text": "Practice Problem for Data Wrangling\nOrganise (or wrangle!) the data, as you learned in the lesson. The dataset provided wranglingdataset.csv contains values on trait-temperature relationships in three important vector/pest species. Develop a hypothesis (or a question) related to how you expect a trait(s) responds to temperature and explore it. Present your group’s findings using descriptive statistics and visualizations."
  },
  {
    "objectID": "linear_mod_activity.html",
    "href": "linear_mod_activity.html",
    "title": "Linear Models",
    "section": "",
    "text": "Main Materials"
  },
  {
    "objectID": "linear_mod_activity.html#introduction",
    "href": "linear_mod_activity.html#introduction",
    "title": "Linear Models",
    "section": "Introduction",
    "text": "Introduction\nLinear Regression is a class of Linear Models that is frequently a good choice if both, your response (dependent) and your predictor (independent) variables are continuous. In this section you will learn:\n\nTo explore your data in order to determine whether a Linear Model is a good choice by\n\nVisualizing your data (in R)\nCalculating correlations between variables\n\nTo fit a Linear Regression Model to data\nTo determine whether the Model fits adequately your data (its “significance”) by\n\nPlotting the data and the fitted Model together\nCalculating goodness of fit measures and statistics\nUsing diagnostic plots\n\n\nIt is expected that you have already been introduced to, or are familiar with the concepts (and/or theory) underlying Linear Models. If not, you may want to watch the video.\nWe will use the genome size data.\nSo,\n\\star Open R and setwd to your code directory.\n\\star Create a new blank script called Regression.R and add some introductory comments.\n\\star Add code to your script to load the genome size data into R and check it (again, using the relative path prefix `, assuming that you working directory iscode`):\n\ngenome <- read.csv('activities/data/GenomeSize.csv',stringsAsFactors = T) \nhead(genome)\n\n    Suborder    Family              Species GenomeSize GenomeSE GenomeN\n1 Anisoptera Aeshnidae    Aeshna canadensis       2.20       NA       1\n2 Anisoptera Aeshnidae    Aeshna constricta       1.76     0.06       4\n3 Anisoptera Aeshnidae       Aeshna eremita       1.85       NA       1\n4 Anisoptera Aeshnidae Aeshna tuberculifera       1.78     0.10       2\n5 Anisoptera Aeshnidae       Aeshna umbrosa       2.00       NA       1\n6 Anisoptera Aeshnidae    Aeshna verticalis       1.59       NA       1\n  BodyWeight TotalLength HeadLength ThoraxLength AdbdomenLength ForewingLength\n1      0.159       67.58       6.83        11.81          48.94          45.47\n2      0.228       71.97       6.84        10.72          54.41          46.00\n3      0.312       78.80       6.27        16.19          56.33          51.24\n4      0.218       72.44       6.62        12.53          53.29          49.84\n5      0.207       73.05       4.92        11.11          57.03          46.51\n6      0.220       66.25       6.48        11.64          48.13          45.91\n  HindwingLength ForewingArea HindwingArea MorphologyN\n1          45.40       369.57       483.61           2\n2          45.48       411.15       517.38           3\n3          49.47       460.72       574.33           1\n4          48.82       468.74       591.42           2\n5          45.97       382.48       481.44           1\n6          44.91       400.40       486.97           1"
  },
  {
    "objectID": "linear_mod_activity.html#exploring-the-data",
    "href": "linear_mod_activity.html#exploring-the-data",
    "title": "Linear Models",
    "section": "Exploring the data",
    "text": "Exploring the data\nWe can use plot to create a scatterplot between two variables. If you have a set of variables to explore, writing code for each plot is tiresome, so R provides a function pairs, which creates a grid of scatter plots between each pair of variables. This will make a plot off all the different combinations in your dataset to better explore the dataset. All it needs is a dataset.\nFirst, plot all pairs:\n\nlibrary(repr); options(repr.plot.res = 100, repr.plot.width = 10, repr.plot.height = 10)\n\n\npairs(genome)\n\n\n\n\n That’s messy! There are too many columns that we are plotting against each other, so its hard to get a meaningful picture (literally). So let’s try color-coding the scatterplot markers by suborder:\n\npairs(genome, col=genome$Suborder)\n\n\n\n\n The result is still too messy! There are far too many variables in genome for this to be useful. Before we proceed further,\n\\star Add pairs(genome, col=genome$Suborder) into your script and run the code:\nSo, we need to cut down the data to fewer variables. Previously (Experimental design section) we used indices to select colours; here, we can use indices to select columns from the data frame. This again uses square brackets (x[]), but a data frame has two dimensions, rows and columns, so you need to provide an index for each dimension, separated by commas. If an index is left blank, then all of that dimension (i.e. all rows or columns) are selected. Try the following to re-acquaint yourself to access data frame content using indices:\n\nlibrary(repr); options(repr.plot.res = 100, repr.plot.width = 8, repr.plot.height = 8) # change plot size\n\n\n# create a small data frame:\ndat <- data.frame(A = c(\"a\", \"b\", \"c\", \"d\", \"e\"), B = c(1, 2, 3, 4, 5))\ndat[1, ] # select row 1 (all columns selected)\n\n  A B\n1 a 1\n\n\n\ndat[, 2] # select column 2 (all rows selected)\n\n[1] 1 2 3 4 5\n\n\n\ndat[2, 1] # select row 2, column 1\n\n[1] \"b\"\n\n\n Now let’s get resume the actual analysis. We will look at five key variables: genome size, body weight, total length, forewing length and forewing area. If you look at the output of str(genome), you’ll see that these are in columns 4, 7, 8, 12 and 14. We can record the indices of these columns and use this to select the data in the pairs plot:\n\nmorpho_vars <- c(4, 7, 8, 12, 14) # store the indices\npairs(genome[, morpho_vars], col = genome$Suborder)\n\n\n\n\n\\star Add the code above to your script and run it.\nIn the figure above, each scatterplot is shown twice, with the variables swapping between the x and y axes. You can see immediately that the relationships between the four morphological measurements and genome size are fairly scattered but that the plots comparing morphology show much clearer relationships."
  },
  {
    "objectID": "linear_mod_activity.html#correlations",
    "href": "linear_mod_activity.html#correlations",
    "title": "Linear Models",
    "section": "Correlations",
    "text": "Correlations\nOne way of summarising how strong the pair-wise relationships between these variables is to calculate a correlation coefficient. Pearson correlations look at the difference of each point from the mean of each variable (and since it uses means, it is a parametric statistic).\nIt is calculated using the differences from the mean on each axis. The key calculation is — for each point – to get the product of the differences on each axis and add them up. If the points are mostly top left (-x, y) or bottom right (x, -y) then these products are mostly negative (-xy); if the points are mostly top right (x, y) or bottom left (-x, -y) then the products are mostly positive (xy).\n\n\n\n\nIllustration of what the Pearson correlation coefficient means.\n\n\n\nThe plots above show three clear cases where all the values of xy are negative or positive or where both are present and sum to zero. The Pearson correlation coefficient simply scales these sums of xy to be between -1 (perfectly negatively correlated) and 1 (perfectly positively correlated) via zero (no correlation). Specifically, this correlation coefficient (r) is equal to the average product of the standardized values of the two variables (let’s call them x and y):\nr_{xy}={\\frac{\\sum _{i=1}^{n}\\left({\\frac {x_{i}-{\\bar {x}}}{s_{x}}}\\right)\\left({\\frac {y_{i}-{\\bar {y}}}{s_{y}}}\\right)}{n-1}}\nwhere,\n\nn is sample size\nx_i, y_i are the individual sample points indexed with i (i = 1,2,\\ldots, n)\n$ {x} = $ is the the sample mean.\n$ s_x = $ (sample standard deviation)\nThe quantities \\left({\\frac {x_{i}-{\\bar {x}}}{s_{x}}}\\right) and \\left(\\frac{y_i-\\bar{y}}{s_y} \\right) are the Z-scores (aka standard scores) of x and y. This conversion of the raw scores (x_i’s and y_i’s) to Z-scores is called standardizing (or sometimes, normalizing).\n\nThus the coefficient (r_{xy}) will be positive (negative) if the x_i and y_i’s tend to move in the same (opposite) direction relative to their respective means (as illustrated in the figure above).\nIn R, we will use two functions to look at correlations. The first is cor, which can calculate correlations between pairs of variables, so is a good partner for pairs plots. The second is cor.test, which can only compare a single pair of variables, but uses a t test to assess whether the correlation is significant.\n\\star Try the following (and include it in your R script file):\n\ncor(genome[, morpho_vars], use = \"pairwise\")\n\n               GenomeSize BodyWeight TotalLength ForewingLength ForewingArea\nGenomeSize      1.0000000  0.3430934   0.3407077      0.2544432    0.3107247\nBodyWeight      0.3430934  1.0000000   0.9167995      0.8944228    0.9198821\nTotalLength     0.3407077  0.9167995   1.0000000      0.9225974    0.9077555\nForewingLength  0.2544432  0.8944228   0.9225974      1.0000000    0.9829803\nForewingArea    0.3107247  0.9198821   0.9077555      0.9829803    1.0000000\n\n\n This is the correlation matrix. Then:\n\ncor.test(genome$GenomeSize, genome$TotalLength, use = \"pairwise\")\n\n\n    Pearson's product-moment correlation\n\ndata:  genome$GenomeSize and genome$TotalLength\nt = 3.5507, df = 96, p-value = 0.0005972\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1526035 0.5049895\nsample estimates:\n      cor \n0.3407077 \n\n\n The use='pairwise' tells R to omit observations with missing data and use complete pairs of observations. The first function confirms our impressions from the graphs: the correlations between genome size and morphology are positive but comparatively weak and the correlations between morphological measurements are positive and very strong (i.e. close to 1). The correlation test tells us that genome size and body length are positively correlated (r=0.34, t = 3.5507, df = 96, p = 0.0006).\nAlso, in case you are wondering about what the t-value in the cor.test signifies:    \nThe t-test is used within cor.test to establish if the correlation coefficient is significantly different from zero, that is, to test if there is a significant association between the two variables. The t-test, in general, can be used to test if the mean of a sample is significantly different from some reference value (1-sample t-test). Here, the “mean of the sample” is the observed correlation coefficient, and the reference value is 0 (the null hypothesis, that there is no association)."
  },
  {
    "objectID": "linear_mod_activity.html#transformations-and-allometric-scaling",
    "href": "linear_mod_activity.html#transformations-and-allometric-scaling",
    "title": "Linear Models",
    "section": "Transformations and allometric scaling",
    "text": "Transformations and allometric scaling\nThere is one problem with the correlations above: the correlation coefficient calculation assumes a straight line relationship. Some of the scatterplots above are fairly straight but there are some strongly curved relationships. This is due to allometric scaling, where one body measure changes (or grows) disproportionately with respect to another. Here, two of the variables are in linear units (total and forewing length), one is in squared units (forewing area) and one in cubic units (body weight, which is approximately volume). That these measures are in different units itself guarantees that they will scale allometrically with respect to each other.\nThe relationships between these variables can be described using a power law:\ny = ax^b\nFortunately, if we log transform this equation, we get \\log(y) = \\log(a) + b \\log(x). This is the equation of a straight line (y=a+bx), so we should be able to make these plots straighter by logging both axes. We can create a new logged variable in the data frame like this:\n\ngenome$logGS <- log(genome$GenomeSize)\n\n \\star Using this command as a template, create a new logged version of the five variables listed above:\n\ngenome$logGS <- log(genome$GenomeSize)\ngenome$logBW <- log(genome$BodyWeight)\ngenome$logTL <- log(genome$TotalLength)\ngenome$logFL <- log(genome$ForewingLength)\ngenome$logFA <- log(genome$ForewingArea)\n\n \\star Then, using str, work out which column numbers the logged variables are and create a new variable called logmorpho containing these numbers:\n\nstr(genome)\n\n'data.frame':   100 obs. of  21 variables:\n $ Suborder      : Factor w/ 2 levels \"Anisoptera\",\"Zygoptera\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Family        : Factor w/ 9 levels \"Aeshnidae\",\"Calopterygidae\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Species       : Factor w/ 100 levels \"Aeshna canadensis\",..: 1 2 3 4 5 6 8 17 46 53 ...\n $ GenomeSize    : num  2.2 1.76 1.85 1.78 2 1.59 1.44 1.16 1.44 1.2 ...\n $ GenomeSE      : num  NA 0.06 NA 0.1 NA NA NA NA NA NA ...\n $ GenomeN       : int  1 4 1 2 1 1 1 1 1 1 ...\n $ BodyWeight    : num  0.159 0.228 0.312 0.218 0.207 0.22 0.344 0.128 0.392 0.029 ...\n $ TotalLength   : num  67.6 72 78.8 72.4 73 ...\n $ HeadLength    : num  6.83 6.84 6.27 6.62 4.92 6.48 7.53 5.74 8.05 5.28 ...\n $ ThoraxLength  : num  11.8 10.7 16.2 12.5 11.1 ...\n $ AdbdomenLength: num  48.9 54.4 56.3 53.3 57 ...\n $ ForewingLength: num  45.5 46 51.2 49.8 46.5 ...\n $ HindwingLength: num  45.4 45.5 49.5 48.8 46 ...\n $ ForewingArea  : num  370 411 461 469 382 ...\n $ HindwingArea  : num  484 517 574 591 481 ...\n $ MorphologyN   : int  2 3 1 2 1 1 4 1 1 1 ...\n $ logGS         : num  0.788 0.565 0.615 0.577 0.693 ...\n $ logBW         : num  -1.84 -1.48 -1.16 -1.52 -1.58 ...\n $ logTL         : num  4.21 4.28 4.37 4.28 4.29 ...\n $ logFL         : num  3.82 3.83 3.94 3.91 3.84 ...\n $ logFA         : num  5.91 6.02 6.13 6.15 5.95 ...\n\n\n\nlogmorpho <- c(17,18,19,20,21)\n\n We can now use the pairs and cor test as before for the columns in logmorpho:\n\npairs(genome[, logmorpho], col=genome$Suborder)\n\n\n\n\n\ncor(genome[, logmorpho], use='pairwise')\n\n           logGS      logBW     logTL     logFL      logFA\nlogGS 1.00000000 0.08406293 0.2224443 0.1150025 0.06808306\nlogBW 0.08406293 1.00000000 0.8891899 0.9456492 0.94995683\nlogTL 0.22244431 0.88918989 1.0000000 0.9157695 0.86207098\nlogFL 0.11500250 0.94564919 0.9157695 1.0000000 0.97916470\nlogFA 0.06808306 0.94995683 0.8620710 0.9791647 1.00000000\n\n\n The scatterplots show that logging the data has very successfully addressed curvature (non-linearity) due to allometric scaling between variables in the data."
  },
  {
    "objectID": "linear_mod_activity.html#performing-the-regression-analysis",
    "href": "linear_mod_activity.html#performing-the-regression-analysis",
    "title": "Linear Models",
    "section": "Performing the Regression analysis",
    "text": "Performing the Regression analysis\nWe’ll now look at fitting the first linear model of this course, to explore whether log genome size explains log body weight. The first thing to do is to plot the data:\n\nmyColours <- c('red', 'blue') # Choose two colours\nmySymbols <- c(1,3) # And two different markers\ncolnames(genome)\n\n [1] \"Suborder\"       \"Family\"         \"Species\"        \"GenomeSize\"    \n [5] \"GenomeSE\"       \"GenomeN\"        \"BodyWeight\"     \"TotalLength\"   \n [9] \"HeadLength\"     \"ThoraxLength\"   \"AdbdomenLength\" \"ForewingLength\"\n[13] \"HindwingLength\" \"ForewingArea\"   \"HindwingArea\"   \"MorphologyN\"   \n[17] \"logGS\"          \"logBW\"          \"logTL\"          \"logFL\"         \n[21] \"logFA\"         \n\n\n\nlibrary(repr); options(repr.plot.res = 100, repr.plot.width = 6, repr.plot.height = 6) # change plot size\n\n\nplot(logBW ~ GenomeSize , data = genome, #Now plot again\ncol = myColours[Suborder], pch = mySymbols[Suborder],\nxlab='Genome size (pg)', ylab='log(Body weight) (mg)')\n\nlegend(\"topleft\", legend=levels(genome$Suborder), #Add legend at top left corner\n       col= myColours, pch = mySymbols)\n\n\n\n\n It is clear that the two suborders have very different relationships: to begin with we will look at dragonflies (Anisoptera). We will calculate two linear models:\n\nThe Null Model: This is the simplest linear model: nothing is going on and the response variable just has variation around the mean: y = \\beta_1. This is written as an R formula as y ~ 1.\nThe Linear (Regression) Model: This models a straight line relationship between the response variable and a continuous explanatory variable: y= \\beta_1 + \\beta_{2}x.\n\nThe code below fits these two models.\n\nnullModelDragon       <- lm(logBW ~ 1, data = genome, subset = Suborder ==  \"Anisoptera\")\ngenomeSizeModelDragon <- lm(logBW ~ logGS, data = genome, subset =  Suborder == \"Anisoptera\")\n\n - Note the long names for the models. Short names are easier to type but calling R objects names like mod1, mod2, xxx swiftly get confusing!*\n\\star Add these model fitting commands into your script and run them.\nNow we want to look at the output of the model. Remember from the lecture that a model has coefficients (the \\beta values in the equation of the model) and terms which are the explanatory variables in the model. We’ll look at the coefficients first:\n\nsummary(genomeSizeModelDragon) \n\n\nCall:\nlm(formula = logBW ~ logGS, data = genome, subset = Suborder == \n    \"Anisoptera\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3243 -0.6124  0.0970  0.5194  1.3236 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.39947    0.09085 -26.413  < 2e-16 ***\nlogGS        1.00522    0.23975   4.193 9.54e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6966 on 58 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2326,    Adjusted R-squared:  0.2194 \nF-statistic: 17.58 on 1 and 58 DF,  p-value: 9.539e-05\n\n\n There is a lot of information there: the model description (‘Call’), a summary of the residuals, a table of coefficients and then information on residual standard error, r squared and an F test.\nAll of these will become clearer during this course (in particular, the meaning of Adjusted R-square will be explained in the ANOVA section — for the moment, concentrate on the coefficients table.\nThere are two rows in the coefficient table, one for each coefficient in y=\\beta_1 + \\beta_2x — these are the intercept and the slope of the line. The rest the details on each row are a t test of whether the slope and intercept are significantly different from zero.\nThe (least-squares) estimate of the slope coefficient (logGS) is equal to the correlation coefficient between the dependent variable (y, here logBW) and the independent variable (x, here logGS) times the ratio of the (sample) standard deviations of the two (see above for the definitions of these):\n \\text{Slope} = \\beta_2 = r_{xy} \\frac{s_y}{s_x}\nThus you can see that the regression slope is proportional to the correlation coefficient; the ratio of standard deviations serves to scale the correlation coefficient (which is unit-less) appropriately to the actual units in which the variables are measured.\nThe (least-squares) estimate of the intercept is the mean of the dependent variable minus the estimated slope coefficient times the mean of the independent variable:\n \\text{Intercept} = \\beta_1 = \\bar{y} - \\beta_2 \\bar{x}  \nThe standard error of the model (“Residual standard error” in the output above, also referred to as the standard error of the regression) is equal to the square root of the sum of the squared residuals divided by n-2. The sum of squared residuals is divided by n-2 in this calculation rather than n-1 because an additional degree of freedom for error has been used up by estimating two parameters (a slope and an intercept) rather than only one (the mean) in fitting the model to the data.\nAs you can see in the output above, each of the two model parameters, the slope and intercept, has its own standard error, and quantifies the uncertainty in these two estimates. These can be used to construct the Confidence Intervals around these estimates, which we will learn about later.\nThe main take-away from all this is that the standard errors of the coefficients are directly proportional to the standard error of the regression and inversely proportional to the square root of the sample size (n). Thus, that “noise” in the data (measured by the residual standard error) affects the errors in the coefficient estimates in exactly the same way. Thus, 4 times as much data will tend to reduce the standard errors of the all coefficients by approximately a factor of 2.\nNow we will look at the terms of the model using the anova function. We will have a proper look at ANOVA (Analysis of Variance) later.\nMeanwhile, recall from your lecture that ANOVA tests how much variation in the response variable is explained by each explanatory variable. We only have one variable and so there is only one row in the output:\n\nanova(genomeSizeModelDragon)\n\nAnalysis of Variance Table\n\nResponse: logBW\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nlogGS      1  8.5294  8.5294  17.579 9.539e-05 ***\nResiduals 58 28.1417  0.4852                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n This table is comparing the variation in log body weight explained by log genome size to the total variation in log body weight. We are interested in how much smaller the residuals are for the genome size model than the null model. Graphically, how much shorter are the red residuals than the blue residuals:\n\n\n\n\nComparison of the null and the regression models.\n\n\n\nWe can get the sums of the squares of these residuals from the two models using the function resid, and then square them and add them up:\n\nsum(resid(nullModelDragon) ^ 2)\n\n[1] 36.67107\n\n\n\nsum(resid(genomeSizeModelDragon) ^ 2)\n\n[1] 28.14168\n\n\n So we have five columns in the ANOVA table:\n\nDf: This shows the degrees of freedom. Each fitted parameter/coefficient takes up a degree of freedom from the total sample size, and the left over are the residuals degree of freedom. In this case, genome size adds a slope (compare the null model y=\\beta_1 and this model y=\\beta_1 + \\beta_2x — there is one more \\beta).\nSum Sq: This shows sums of squares. The bottom line is the residual sum of squares for the model and the one above is the variation explained by genome size. Using the two values from above, the sums of square residuals for the null model are 36.67. In the genome size model, the sum of square residuals are 28.14 and so 36.67-28.14=8.53 units of variance have been explained by this model.\nMean Sq: These are just the Sum Sq (Sum of Squares) values divided by the degrees of freedom. The idea behind this is simple: if we explain lots of variation with one coefficient, that is good (the null model), and if we explain a small amount of variation with a loss of degree of freedom (by adding and then estimating more parameters), then that is bad.\nF value: This is the ratio of the Mean Sq for the variable and the residual Mean Sq. This is used to test whether the explained variation is large or small.\nPr(>F): This is the p value — the probability of the x-variable (the fitted model) explaining this much variance by chance.\n\nIn this case, it is clear that genome size explains a significant variation in body weight.\n\\star Include the summary and anova commands for genomeSizeModelDragon in your script, run them and check you are happy with the output.\n\nExercise\nUsing the above code as a template, create a new model called genomeSizeModelDamsel that fits log body weight as a function of log genome size for damselflies. Write and run code to get the summary and anova tables for this new model. For example, the first step would be:\n\ngenomeSizeModelDamsel <- lm(logBW ~ logGS, data=genome,subset=Suborder=='Zygoptera')\n\n\n\nPlotting the fitted Regression model\nNow we can plot the data and add lines to show the models. For simple regression models, we can use the function abline(modelName) to add a line based on the model.\n\\star Create a scatterplot of log body weight as a function of log genome size, picking your favourite colours for the points.\nUse abline to add a line for each model and use the col option in the function to colour each line to match the points.\nFor example: abline(genomeSizeModelDragon, col='red').\n\nmyCol <- c('red','blue')\nlibrary(repr); options(repr.plot.res = 100, repr.plot.width = 8, repr.plot.height = 8) # change plot size\nplot(logBW ~ logGS, data=genome, col=myCol[Suborder], xlab='log Genome Size (pg)', ylab='log Body Weight (g)')\n\nabline(genomeSizeModelDragon, col='red')\n\n\n\n\n Your final figure should look something like this:\n\n\n\nLinear regression models fitted to the body weight vs. genome size to the Dragonfly (red) and Damselfly (blue) subsets of the data."
  },
  {
    "objectID": "linear_mod_activity.html#model-diagnostics",
    "href": "linear_mod_activity.html#model-diagnostics",
    "title": "Linear Models",
    "section": "Model diagnostics",
    "text": "Model diagnostics\nNow that we have our models, we need to check that they are appropriate for the data. For this, we will inspect “diagnostic plots”. Producing diagnostic plots is easy in R — if you plot the model object, then R produces a set of diagnostic plots!\n\\star Quick note on par: this is used to tell R where you want your plots when you build.\n\\star Try the following code (and include in the R script file):\n\npar(mfrow = c(2, 2), mar = c(5, 5, 1.5, 1.5))\nplot(genomeSizeModelDragon)\n\n\n\n\n These are the diagnostics for the lm fit to the Dragonfly data subset.\nLet’s also plot the diagnostic pots for the model fitted to the Damselfly subset:\n\npar(mfrow = c(2, 2), mar = c(5, 5, 1.5, 1.5))\nplot(genomeSizeModelDamsel)\n\n\n\n\n The diagnostic plots are:\n\nResiduals vs Fitted: This plot is used to spot if the distribution of the residuals (the vertical distance from a point to the regression line) has similar variance for different predicted values (the y-value on the line corresponding to each x-value). There should be no obvious patterns (such as curves) or big gaps. If there was no scatter, if all the points fell exactly on the line, then all of the dots on this plot would lie on the gray horizontal dashed line. The red line is a smoothed curve to make it easier to see trends in the residuals. It is flat in the Dragonfly model fit above, and a bit more wavy than we would like in the in the Damselfly model fit, but there are no clear trends in either, which is what you hope to see.\nNormal Q–Q: This plot is to check whether the residuals are normally distributed — are the values of the observed residuals similar to those expected under a normal distribution? Ideally, the points should form a perfectly straight line, indicating that the observed residuals exactly match the expected. Here, note that the points lie pretty close to the dashed line in both sets of diagnostic Figures above, but deviate at the ends, especially for Damselflies. However, some deviation is to be expected near the ends — here these deviations are just about acceptable.\nScale–Location: The x-axis on this plot is identical to the Residuals vs Fitted plot – these are the fitted values. The y-axis is the square root of the standardized residuals, which are residuals rescaled so that they have a mean of zero and a variance of one. As a result, all y-axis values are positive. Thus large residuals (both positive and negative) plot at the top, and small residuals plot at the bottom (so only their scale is retained). Thus, all of the numbered points (which will be the same in all plots) plot at the top here. The red line here shows the trend, just like the Residuals vs Fitted plot. The regression analysis has assumed homoscedasticity, that the variance in the residuals doesn’t change as a function of the predictor. If that assumption is correct, the red line should be relatively flat. It is not quite as flat as we would like, especially for the Dragonfly analysis.\nResiduals vs Leverage: This plot shows the standardized residuals against leverage. “Leverage” is a measure of how much each data point influences the linear model’s coefficient estimates. Because the regression line must pass through the centroid (“pivot point”) of the data, points that lie far from the centroid have greater leverage, and their leverage increases if there are fewer points nearby. Here is an illustration:\n\n\n\n\n\nLeverage of data points on slope of a regression. The points further away from the centroid in the x-axis direction have more leverage, and can therefore move the regression line up or down (dashed red lines)\n\n\nThere are two key things to note about this plot:\n\nThe standardized residuals (y-axis) are centered around zero and reach 2-3 standard deviations away from zero. They should also lie symmetrically about zero, as would be expected for a normal distribution. This is the case for the Damselfly plot , but not so much for the Dragonfly plot.\nThe contours values show Cook’s distance (only visible in the Damsefly plot), which measures how much the regression would change if a point was deleted. Cook’s distance is increased by leverage and by large residuals: a point far from the centroid with a large residual can severely distort the coefficient estimates from the regression. On this plot, you want to see that the red smoothed line stays close to the horizontal gray dashed line and that no points have a large Cook’s distance (i.e, >0.5). Both are true here.\n\nThis is an important diagnostic plot in regression analyses in particular because it tells you whether your estimate of the slope coefficient in particular is strongly affected by certain data points.\nNote that certain points are numbered in all the diagnostic plots — these are points to pay special attention to because they are potential outliers. The numbers correspond to the row number for that dataset in your data frame. You can easily identify these points in your data plot because the order of the points along the fitted values axis (y-axis) in the diagnostic plot matches the order along the x-axis in the data plot. So, for example here, in the dragonfly diagnostic plots the two numbered points (46, 10) near the bottom correspond in the data plot to the two red points near the center-left that lie farthest below the red line (see the plot with regression lines fitted to the data).\nThus, neither the Drangonfly nor the Damselfly diagnostic plots look perfect, but this level of deviation from assumptions of linear models is acceptable. The main worrying factors are that the Q-Q plot for Damselflies indicates the observed residuals are a bit more extreme than expected, and the Scale–Location plot for Dragonflies suggests some pattern in the standardized residuals wrt location of the fitted values.\n\\star Copy the code to create the diagnostic plots into your script to keep a record of the code and run it."
  },
  {
    "objectID": "linear_mod_activity.html#reporting-the-model",
    "href": "linear_mod_activity.html#reporting-the-model",
    "title": "Linear Models",
    "section": "Reporting the model",
    "text": "Reporting the model\nNow we know that the models are appropriate and we have a plot, the last thing is to report the statistics. For the damselfly model, here is one summary that would do: log genome size explains significant variation in log body weight in dameselflies (F=10.5, df=1,36, p=0.0025) and shows that body weight decreases with genome size (intercept: -4.65, se=0.09; slope: -1.14, se=0.35)."
  },
  {
    "objectID": "linear_mod_activity.html#introduction-1",
    "href": "linear_mod_activity.html#introduction-1",
    "title": "Linear Models",
    "section": "Introduction",
    "text": "Introduction\nAnalysis of Variance, is very often a good choice if your response (dependent) variable is continuous, and your predictor (independent) variables is categorical.\nIn this section, you will learn to perform an ANOVA, that is, fit this linear model to the data. Specifically, you will learn to^{[1]}:\n\nVisualize the data by plotting boxplots and barplots\nFit an ANOVA to test whether certain factors can explain (partition) the variation in your data\nPerform diagnostics to determine whether the factors are explanatory, and whether the Linear Model is appropriate for your data\nExplore and compare how much the different levels of a factor explain the variation in your data"
  },
  {
    "objectID": "linear_mod_activity.html#what-is-anova",
    "href": "linear_mod_activity.html#what-is-anova",
    "title": "Linear Models",
    "section": "What is ANOVA?",
    "text": "What is ANOVA?\nAnalysis Of Variance (ANOVA) is an extremely useful class of Linear models. It is very often appropriate when your response (dependent) variable is continuous, while your predictor (independent) variable is categorical. Specifically, One-way ANOVA is used to compare means of two or more samples representing numerical, continuous data in response to a single categorical variable (factor).\n\n\n\nA dataset where an ANOVA is appropriate.  Performing an ANOVA on this dataset is the same as fitting the linear model y = \\beta_1 + \\beta_2 x_s + \\beta_3 x_a, where x_s and x_a are two levels (“treatments”, representing statistically separate populations) within the factor (games console ownership). Here, the first treatment, the control, is captured by the baseline value \\beta_1 (the sample with the lowest value, on the far left).\n\n\n(One-way) ANOVA tests the null hypothesis that samples from two or more groups (the treatments or factors) are drawn from populations with the same mean value. That is, the null hypothesis is that all the groups are random samples from the same population (no statistical difference in their means). To do this, ANOVA compares the variance in the data explained by fitting the linear model, to the unexplained variance (the null hypothesis.\nIn other words, in effect, ANOVA asks whether a linear model with a predictor (or explanatory variable) with at least two categorical levels (or factors), better accounts for the variance (Explained Sum of Squares, ESS, see below) than a null model of the form y = \\beta_1. Thus, ANOVA is just a type of linear model.\nBy the end of this section, it will also make more sense to you how/why fitting a linear regression model to the data of the form y = \\beta_1 + \\beta_2 x (where x is a continuous predictor variable), requires an ANOVA to determine if the model better fits than a null model of the form y = \\beta_1.\nTypically, one-way ANOVA is used to test for differences among at least three groups, since the two-group (or levels or factors) case can be covered by a t-test. When there are only two means to compare, the t-test and the F-test are equivalent; the relation between ANOVA and t is given by F = t^2.\nAn extension of one-way ANOVA is two-way analysis of variance that examines the influence of two different categorical independent variables on one dependent variable. We will explore multiple predictors in later segments."
  },
  {
    "objectID": "linear_mod_activity.html#calculating-the-anova-test-statistic",
    "href": "linear_mod_activity.html#calculating-the-anova-test-statistic",
    "title": "Linear Models",
    "section": "Calculating the ANOVA test statistic",
    "text": "Calculating the ANOVA test statistic\nANOVA uses the F-Statistic. To this end, an ANOVA “partitions” variability in your data as follows:\nTotal sum of squares (TSS): This is the sum of the squared difference between the observed dependent variable (y) and the mean of the response variable y (denoted by \\bar{y}), i.e.,\n\\text{TSS} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\nTSS tells us how much variation there is in the dependent variable without having any other information (your null model). You might notice that TSS is the numerator of the sample variance (or it’s square-root, the sample standard deviation).\nExplained sum of squares (ESS): Sum of the squared differences between the predicted y’s (denoted \\hat{y}’s) and \\bar{y}, or, \\text{ESS} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 ESS tells us how much of the variation in the dependent variable our alternative (linear) model was able to explain. That is, it’s the reduction in uncertainty that occurs when the linear model is used to predict the responses.\nResidual sum of squares (RSS): Sum of the squared differences between the observed y’s (denoted by y_i) and the predicted \\hat{y}, or, \\text{RSS} = \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 RSS tells us how much of the variation in the dependent variable our model could not explain. That is, it’s the uncertainty that remains even after the linear model is used. The linear model is considered to be statistically significant if it can account for a large amount of variability in the response.\n\nAnd of course, TSS = ESS + RSS. That is, the OLS method “decomposes” the total variation in the dependent variable into an explained component (ESS; explained by the predictor) and an unexplained or residual component (the RSS).\n\nThe sums of squares used to calculate the statistical significance of the linear model (Regression, ANOVA, etc) through the F value are as follows:\n\n\n\n\n\n\n\n\n\nType of Sum of Squares (SS)\nSS Calculation\nDegrees of Freedom (DF)\nMean Sum of Squares (MSS)\n\n\n\n\nTSS\n\\sum_{i=1}^{n}(y_i - \\bar{y})^2\nn-1\n\\frac{TSS}{n-1}\n\n\nESS\n\\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\nn_c-1\n\\frac{ESS}{n_c-1}\n\n\nRSS\n\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\nn-n_c\n\\frac{RSS}{n-n_c}\n\n\n\nLet’s try to make sense of these calculations. Firstly, because we are dealing with samples understanding the degrees of freedom is critical.\n\nDegrees of freedom\nEach sum of squares has a corresponding degrees of freedom (DF) associated with it that gives the Mean Sum of Squares (MSS) — the Sums of Squares divided by the corresponding degrees of freedom.\n\nThe TSS DF is one less than the number of observations n-1. This is because calculating TSS, needs \\bar y , which imposes loss of one degree of freedom. Note that MSS is thus nothing but the sample variance.\nThe ESS DF is one less than the number of coefficients (n_c) (estimated parameters) in the model: n_c-1. Note that in the case where the linear model is an ANOVA, the number of coefficients equals the number of “treatments” (the categories or levels in the predictor or factor). So for example, in Figure 1, there are three treatments (predictors) and therefore three coefficients (\\beta_1, \\beta_2, \\beta_3), which means that the ESS degrees of freedom there is n_c-1 = 2.\nThe RSS DF is the sample size n minus the number of coefficients that you need to estimate (n_c), that is, n - n_c, because each estimated coefficient is an unknown parameter.\n\n\n\nThe F-Value (or Ratio)\nThe F-Value or F-Ratio, the test statistic used to decide whether the linear model fit is statistically significant, is the ratio of the Mean ESS to the Mean RSS:\n\nF = \\frac{\\left(\\frac{ESS}{n_c-1}\\right)}{\\left(\\frac{RSS}{n-n_c}\\right)}\n\nIf the null hypothesis that the group means are drawn from sub-populations with the same mean were indeed true, the between-group variance (numerator in this F-ratio) should be lower than the within-group variance (denominator). The null hypothesis is rejected if this F-Ratio is large — the model explains a significant amount of variance, and we can conclude that the samples were drawn from populations with different mean values.\nThe p-value is calculated for the overall model fit using the F-distribution.\nAlso note that the Root Mean Square Error (RMSE), also known as the standard error of the estimate, is the square root of the Mean RSS. It is the standard deviation of the data about the Linear model, rather than about the sample mean.\n\n\nThe R^{2}\nThe R^{2}, also called the Coefficient of Determination, is the proportion of total error (TSS) explained by the model (ESS), so the ratio ESS/TSS. That is it is the proportion of the variability in the response that is explained by the fitted model. Since TSS = ESS + RSS, R^{2} can be rewritten as (TSS-RSS)/TSS = 1 - RSS/TSS. If a model perfectly fits the data, R^{2}=1, and if it has no predictive capability R^{2}=0. In reality, R^{2} will never be exactly 0 because even a null model will explain some variance just by chance due to sampling error. Note that R, the square root of R^2, is the multiple correlation coefficient: the correlation between the observed values (y), and the predicted values (\\hat{y}).\n\nAdjusted R^{2}\nAs additional predictors (and therefore linear model coefficients) are added to a linear model, R^2 increases even when the new predictors add no real predictive capability. The adjusted-R^2 tries to address this problem of over-specification or over-fitting by including the degrees of freedom: Adjusted R^2 = 1 - (RSS/n-n_c-2)/(TSS/n-1) ^{[2]}.\nThus, additional predictors with little explanatory capability will increase the ESS (and reduce the RSS), but they will also have lower RSS degrees of freedom (because of the additional number of fitted coefficients, n_c’s)^{[3]}. Thus if the additional predictors have poor predictive capability, these two reductions will cancel each other out. In other words, the Adjusted R^2 penalizes the addition of new predictors to the linear model, so you should always have a look at the Adjusted R^2 as a corrected measure of R^2."
  },
  {
    "objectID": "linear_mod_activity.html#an-example-anova",
    "href": "linear_mod_activity.html#an-example-anova",
    "title": "Linear Models",
    "section": "An example ANOVA",
    "text": "An example ANOVA\nIn this section, we will use a dataset from a recent paper on the effects of temperature and larval resource supply on Aedes aegypti life history traits and the vector’s maximal population growth rate (Huxley et al. 2021).\n\nThe data\n\\star Download the file traitdata_Huxleyetal_2021.csv and save to your Data directory.\n\\star Create a new blank script called ANOVA_Prac.R and add some introductory comments.\n\\star Use read.csv to load the data in the data frame mozwing and then str and summary to examine the data:\n\nrequire('tidyverse')\n\nLoading required package: tidyverse\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nrequire('gplots')\n\nLoading required package: gplots\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nrequire('repr')\n\nrm(list=ls())\ngraphics.off()\n\n\nmozwing      <- read.csv('activities/data/traitdata_Huxleyetal_2021.csv', stringsAsFactors = T)\nmozwing$temp <- as_factor(mozwing$temp) # define temperature and food level as categorical\nmozwing$food_level <- as_factor(mozwing$food_level) \n\n\nstr(mozwing)\n\n'data.frame':   270 obs. of  33 variables:\n $ ID                    : int  4 6 7 11 11 12 19 20 16 21 ...\n $ exp_no                : int  3 3 3 3 3 3 3 3 3 3 ...\n $ rep                   : Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 3 1 1 1 3 1 ...\n $ dens                  : num  0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ...\n $ rearing_vessel        : Factor w/ 1 level \"tub\": 1 1 1 1 1 1 1 1 1 1 ...\n $ temp                  : Factor w/ 3 levels \"22\",\"26\",\"32\": 1 1 1 1 1 1 1 1 1 1 ...\n $ food_level            : Factor w/ 2 levels \"0.1\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ treatment             : Factor w/ 6 levels \"22_0.1\",\"22_1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ feeding_interval      : int  24 24 24 24 24 24 24 24 24 24 ...\n $ egg_sub               : Factor w/ 2 levels \"29/03/2019 12:00\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ l_emerg               : Factor w/ 2 levels \"30/03/2019 12:00\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ l_emerg_days_frm_sub  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ l_death               : Factor w/ 29 levels \"01/04/2019 12:00\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ l_surv                : int  0 0 0 0 0 0 0 0 0 0 ...\n $ pupal_ref             : int  1 3 4 7 2 8 12 13 6 14 ...\n $ p_emerg               : Factor w/ 27 levels \"01/05/2019 12:00\",..: 24 25 26 3 26 3 5 7 27 7 ...\n $ l_to_p_devtime        : int  28 29 30 34 30 34 35 36 32 36 ...\n $ l_to_p_devrate        : num  0.0357 0.0345 0.0333 0.0294 0.0333 ...\n $ p_death               : Factor w/ 8 levels \"03/04/2019 12:00\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ p_surv                : int  0 0 0 0 0 0 0 0 0 0 ...\n $ a_emerg               : Factor w/ 30 levels \"01/05/2019 12:00\",..: 29 30 1 7 1 7 9 13 3 11 ...\n $ p_to_a_devtime        : int  3 3 3 3 3 3 3 4 3 3 ...\n $ p_to_a_devrate        : num  0.333 0.333 0.333 0.333 0.333 ...\n $ a_death               : Factor w/ 39 levels \"01/05/2019 12:00\",..: 9 5 7 15 5 15 23 12 14 21 ...\n $ adult_lifespan        : int  8 5 5 6 4 6 9 1 7 7 ...\n $ adult_mort_rate       : num  0.125 0.2 0.2 0.167 0.25 ...\n $ hatch_to_a_devtime    : int  31 32 33 37 33 37 38 40 35 39 ...\n $ hatch_to_adult_devrate: num  0.0323 0.0312 0.0303 0.027 0.0303 ...\n $ sex                   : Factor w/ 1 level \"female\": 1 1 1 1 1 1 1 1 1 1 ...\n $ im_surv               : int  0 0 0 0 0 0 0 0 0 0 ...\n $ j_lifetime            : int  31 32 33 37 33 37 38 40 35 39 ...\n $ total_lifespan        : int  39 37 38 43 37 43 47 41 42 46 ...\n $ length_mm             : num  2.93 2.59 2.41 2.62 2.28 2.39 2.99 2.5 2.75 2.82 ...\n\n\n There are 33 variables but only a subset of these are of interest to us here. Let’s summarize the data to explore further:\n\nsummary(mozwing)\n\n       ID            exp_no  rep         dens     rearing_vessel temp   \n Min.   : 1.00   Min.   :3   A:87   Min.   :0.2   tub:270        22:90  \n 1st Qu.: 9.00   1st Qu.:3   B:87   1st Qu.:0.2                  26:90  \n Median :17.00   Median :3   C:96   Median :0.2                  32:90  \n Mean   :16.07   Mean   :3          Mean   :0.2                         \n 3rd Qu.:24.00   3rd Qu.:3          3rd Qu.:0.2                         \n Max.   :30.00   Max.   :3          Max.   :0.2                         \n                                                                        \n food_level  treatment  feeding_interval             egg_sub   \n 0.1:135    22_0.1:45   Min.   :24       29/03/2019 12:00:153  \n 1  :135    22_1  :45   1st Qu.:24       30/03/2019 12:00:117  \n            26_0.1:45   Median :24                             \n            26_1  :45   Mean   :24                             \n            32_0.1:45   3rd Qu.:24                             \n            32_1  :45   Max.   :24                             \n                                                               \n             l_emerg    l_emerg_days_frm_sub             l_death   \n 30/03/2019 12:00:153   Min.   :1            31/03/2019 12:00: 35  \n 31/03/2019 12:00:117   1st Qu.:1            01/04/2019 12:00:  7  \n                        Median :1            15/05/2019 12:00:  6  \n                        Mean   :1            18/04/2019 12:00:  5  \n                        3rd Qu.:1            14/04/2019 12:00:  4  \n                        Max.   :1            (Other)         : 46  \n                                             NA's            :167  \n     l_surv         pupal_ref                 p_emerg    l_to_p_devtime\n Min.   :0.0000   Min.   : 1.00   03/04/2019 12:00: 26   Min.   : 3.0  \n 1st Qu.:0.0000   1st Qu.: 6.00   04/04/2019 12:00: 22   1st Qu.: 6.0  \n Median :0.0000   Median :11.00   07/04/2019 12:00: 22   Median : 9.0  \n Mean   :0.3815   Mean   :11.65   05/04/2019 12:00: 13   Mean   :14.9  \n 3rd Qu.:1.0000   3rd Qu.:17.00   06/04/2019 12:00: 10   3rd Qu.:26.0  \n Max.   :1.0000   Max.   :27.00   (Other)         : 74   Max.   :38.0  \n                  NA's   :103     NA's            :103   NA's   :103   \n l_to_p_devrate                p_death        p_surv       \n Min.   :0.02632   05/04/2019 12:00:  3   Min.   :0.00000  \n 1st Qu.:0.03846   08/04/2019 12:00:  2   1st Qu.:0.00000  \n Median :0.11111   03/04/2019 12:00:  1   Median :0.00000  \n Mean   :0.11582   06/04/2019 12:00:  1   Mean   :0.06587  \n 3rd Qu.:0.16667   07/05/2019 12:00:  1   3rd Qu.:0.00000  \n Max.   :0.33333   (Other)         :  3   Max.   :1.00000  \n NA's   :103       NA's            :259   NA's   :103      \n             a_emerg    p_to_a_devtime  p_to_a_devrate               a_death   \n 05/04/2019 12:00: 20   Min.   :1.000   Min.   :0.2500   07/04/2019 12:00: 11  \n 10/04/2019 12:00: 20   1st Qu.:2.000   1st Qu.:0.3333   12/04/2019 12:00: 11  \n 06/04/2019 12:00: 18   Median :2.000   Median :0.5000   13/04/2019 12:00: 10  \n 07/04/2019 12:00: 10   Mean   :2.436   Mean   :0.4605   02/05/2019 12:00:  9  \n 09/04/2019 12:00: 10   3rd Qu.:3.000   3rd Qu.:0.5000   08/04/2019 12:00:  8  \n (Other)         : 78   Max.   :4.000   Max.   :1.0000   (Other)         :107  \n NA's            :114   NA's   :114     NA's   :114      NA's            :114  \n adult_lifespan  adult_mort_rate   hatch_to_a_devtime hatch_to_adult_devrate\n Min.   : 1.00   Min.   :0.06667   Min.   : 5.00      Min.   :0.02439       \n 1st Qu.: 4.00   1st Qu.:0.11111   1st Qu.: 8.00      1st Qu.:0.03571       \n Median : 6.00   Median :0.16667   Median :12.00      Median :0.08333       \n Mean   : 6.59   Mean   :0.23194   Mean   :17.44      Mean   :0.08661       \n 3rd Qu.: 9.00   3rd Qu.:0.25000   3rd Qu.:28.00      3rd Qu.:0.12500       \n Max.   :15.00   Max.   :1.00000   Max.   :41.00      Max.   :0.20000       \n NA's   :114     NA's   :114       NA's   :114        NA's   :114           \n     sex         im_surv         j_lifetime    total_lifespan    length_mm    \n female:156   Min.   :0.0000   Min.   : 1.00   Min.   : 1.00   Min.   :2.030  \n NA's  :114   1st Qu.:0.0000   1st Qu.: 6.00   1st Qu.: 8.00   1st Qu.:2.510  \n              Median :0.0000   Median :12.00   Median :18.00   Median :2.730  \n              Mean   :0.4222   Mean   :15.97   Mean   :19.78   Mean   :2.742  \n              3rd Qu.:1.0000   3rd Qu.:27.00   3rd Qu.:30.00   3rd Qu.:3.020  \n              Max.   :1.0000   Max.   :47.00   Max.   :49.00   Max.   :3.270  \n                                                               NA's   :121    \n\n\n You will see from the output of summary that there are data on several vector traits, such as juvenile development, adult longevity and wing length.\n\n\nExploring the data with boxplots\nThe body size of arthropod vectors is expected to have an important effect on vector-borne disease transmission risk, because it is associated with traits, such as longevity and biting rate. In organisms with complex life histories, including mosquitoes, adult size is strongly affected by larval rearing conditions.\nHere, we are interested in finding out whether wing length varies predictably across temperatures (a typical one-way ANOVA question).\n\nBefore we fit any models, we want to plot the data to see if the means within these groupings look different. We also want to check whether the variance looks similar for each group: constant normal variance! A simple way is to look at box and whisker plots, showing the median and range of the data:\n\n\\star Generate a boxplot of the differences in wing length between temperatures:\n\nplot(length_mm ~ temp, mozwing)\n\n\n\n\n Looking at the plots, it is clear that there is more spread in the data below the median than above. Create a new variable logwinglength in the mozwing data frame containing logged millimeter values.\n\\star Now create a boxplot of log wing length values within temperatures:\n\nmozwing$loglength <- log(mozwing$length_mm)\n\n ### Differences in means with barplots\nBox and whisker plots show the median and spread in the data very clearly, but we want to test whether the means are different. This is t test territory — how different are the means given the standard error — so it is common to use barplots and standard error bars to show these differences.\nWe’re going to use some R code to construct a barplot by hand. We need to calculate the means and standard errors within temperature groups, but before we can do that, we need a new functions to calculate the standard error of a mean:\n\nseMean <- function(x){ # get standard error of the mean from a set of values (x)\n    x  <- na.omit(x) # get rid of missing values\n\n    se <- sqrt(var(x)/length(x)) # calculate the standard error\n\n    return(se)  # tell the function to return the standard error\n}\n\n Now we can use the function tapply: it splits a variable up into groups from a factor and calculates statistics on each group using a function.\n\nlengthMeans <- tapply(mozwing$loglength, mozwing$temp, FUN = mean, na.rm = TRUE)\n\nprint(lengthMeans)\n\n       22        26        32 \n1.0737861 0.9824950 0.9131609 \n\n\n And similarly, let’s calculate the standard error of the mean using the function we made:\n\nlengthSE <- tapply(mozwing$loglength, mozwing$temp, FUN = seMean)\n\nprint(lengthSE)\n\n        22         26         32 \n0.01330309 0.01431165 0.01230405 \n\n\n Now we have to put these values together on the plot:\n\n# get the upper and lower limits of the error bars\nupperSE <- lengthMeans + lengthSE\nlowerSE <- lengthMeans - lengthSE\n\n# get a barplot\n# - this function can report where the middle of the bars are on the x-axis\n# - set the y axis limits to contain the error bars\n\nbarMids <- barplot(lengthMeans, ylim=c(0, max(upperSE)), ylab = 'ln(wing length, mm)')\n\n# Now use the function to add error bars\n# - draws arrows between the points (x0,y0) and (x1,y1)\n# - arrow heads at each end (code=3) and at 90 degree angles\n\narrows(barMids, upperSE, barMids, lowerSE, ang=90, code=3)\n\n\n\n\n Now we need to draw all these pieces together into a script and get used to using them.\n\\star Add all the lines of code from this section into your script. Run it and check you get the graph above.\n\\star Use the second two chunks as a model to plot a similar graph for food level\n\n\nseMeanfood <- function(x){ # get standard error of the mean from a set of values (x)\n         x <- na.omit(x) # get rid of missing values\n        \n         se <- sqrt(var(x)/length(x)) # calculate the standard error\n        \n        return(se)  # tell the function to return the standard error\n}\n\n\nlengthMeansfood <- tapply(mozwing$loglength, mozwing$food_level, FUN = mean, na.rm = TRUE)\n\nprint(lengthMeansfood)\n\n      0.1         1 \n0.9111556 1.0670992 \n\n\n\nlengthSEfood <- tapply(mozwing$loglength, mozwing$food_level, FUN = seMeanfood)\n\nprint(lengthSEfood)\n\n        0.1           1 \n0.011887620 0.008825873 \n\n\n Lets get the upper and lower limits of the error bars:\n\nupperSEfood <- lengthMeansfood + lengthSEfood\nlowerSEfood <- lengthMeansfood - lengthSEfood\n\n Now we should build a barplot - this function can report where the middle of the bars are on the x-axis - set the y axis limits to contain the error bars\n\nbarMidsfood <- barplot(lengthMeansfood, ylim=c(0, max(upperSE)), ylab = 'ln(wing length, mm)')\n\n\n\n\n Now use the function to add error bars - draws arrows between the points (x0,y0) and (x1,y1) - arrow heads at each end (code=3) and at 90 degree angles\n\nbarMidsfood <- barplot(lengthMeansfood, ylim=c(0, max(upperSE)), ylab = 'ln(wing length, mm)')\narrows(barMidsfood, upperSEfood, barMidsfood, lowerSEfood, ang=90, code=3)\n\n\n\n\n\n\nAn alternative to barplots\nThat is a lot of work to go through for a plot. Doing it the hard way uses some useful tricks, but one strength of R is that there is a huge list of add-on packages that you can use to get new functions that other people have written.\nWe will use the gplots package to create plots of group means and confidence intervals. Rather than plotting the means \\pm 1 standard error, the option p=0.95 uses the standard error and the number of data points to get 95% confidence intervals. The default connect=TRUE option adds a line connecting the means, which isn’t useful here.\n\\star Replicate the code below into your script and run it to get the plots below.\n\n#Load the gplots package\nlibrary(gplots)\n\n# Get plots of group means and standard errors\npar(mfrow=c(1,2))\nplotmeans(loglength ~ temp, data=mozwing, p=0.95, connect=FALSE)\nplotmeans(loglength ~ food_level, data=mozwing, p=0.95, connect=FALSE)"
  },
  {
    "objectID": "linear_mod_activity.html#analysis-of-variance",
    "href": "linear_mod_activity.html#analysis-of-variance",
    "title": "Linear Models",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nHopefully, those plots should convince you that there are differences in wing length across temperatures and food levels. We’ll now use a linear model to test whether those differences are significant.\n\\star Using your code from the regression section as a guide, create a linear model called lengthLM which models wing length as a function of temperature.\nUse anova and summary to look at the analysis of variance table and then the coefficients of the model.\nThe ANOVA table for the model should look like the one below: temperature explains highly significant variation in wing length(F= 30.5, \\textrm{df}=2 \\textrm{ and } 146, p =8.45e-12)\n\nlengthLM <- lm(loglength ~ temp, data=mozwing)\n\n\nanova(lengthLM)\n\nAnalysis of Variance Table\n\nResponse: loglength\n           Df  Sum Sq  Mean Sq F value    Pr(>F)    \ntemp        2 0.58891 0.294454  30.517 8.446e-12 ***\nResiduals 146 1.40873 0.009649                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n Note the style of reporting the result - the statistic (F), degrees of freedom and p value are all provided in support.\nPay close attention to the sum of squares column. This model is good, but some will not be. The important ratio is called r^2, a measure of explanatory power, and shows that, although a model can be very significant, it might not be very explanatory. We care about explanatory power or effect size, *not* p values.\nThe coefficients table for the model looks like this:\n\nsummary(lengthLM)\n\n\nCall:\nlm(formula = loglength ~ temp, data = mozwing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27446 -0.06218  0.02685  0.07266  0.14568 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.07379    0.01290  83.252  < 2e-16 ***\ntemp26      -0.09129    0.01832  -4.983 1.75e-06 ***\ntemp32      -0.16063    0.02122  -7.571 3.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09823 on 146 degrees of freedom\n  (121 observations deleted due to missingness)\nMultiple R-squared:  0.2948,    Adjusted R-squared:  0.2851 \nF-statistic: 30.52 on 2 and 146 DF,  p-value: 8.446e-12\n\n\n It shows the following:\n\nThe reference level (or intercept) is for 22^\\circC. The summary output indicates that wing length at this temperature is significantly different from zero - this is not an exciting finding!\n\n\nTukeyLength <- TukeyHSD(aov(lengthLM))\nprint(TukeyLength)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lengthLM)\n\n$temp\n             diff        lwr         upr     p adj\n26-22 -0.09129112 -0.1346712 -0.04791107 0.0000052\n32-22 -0.16062514 -0.2108632 -0.11038706 0.0000000\n32-26 -0.06933402 -0.1197347 -0.01893334 0.0039744\n\n\n The table shows the following:\n\nThe differences between the three possible pairs and then the lower and upper bounds of the 95% confidence interval for the difference and a p value.\nIn each case, we want to know if the difference could be zero: does the 95% confidence interval for each pair include zero?\nFor all pairs, the confidence intervals do not include zero, so they are significantly different. For example, comparison between 32 and 22^\\circC, the interval does include zero (difference = -0.16, 95% CI’s limits are -0.21 & -0.11), so these groups are significantly different.\nThe p values for the top pairs are broadly consistent with the summary table. However, it may be harder to find significant results with a Tukey test in other cases.\n\nYou can visualise these confidence intervals by plotting the Tukey test. You have to tweak the graphics parameters to get a clean plot though.\n\noptions(repr.plot.res = 100, repr.plot.width = 10, repr.plot.height = 10)\n\n\npar(mfrow=c(1,1))\npar(las=1, mar=c(5,5,5,5))\n# las= 1 turns labels horizontal\n# mar makes the left margin wider (bottom, left, top, right)\nplot(TukeyLength)\n\n\n\n\n \\star Include the Tukey test in your script for both the temperature and food models.\n\nAre the factors independent?\nWe’ve looked at two models, using temperature and food level. It is worth asking whether these are independent factors? This is important to know because otherwise, a two-way ANOVA would not be appropriate. We will look at interactions later.\nOK, so we want to know whether the two factors are independent. This is a job for the \\chi^2 test!"
  },
  {
    "objectID": "linear_mod_activity.html#the-chi-square-test-and-count-data",
    "href": "linear_mod_activity.html#the-chi-square-test-and-count-data",
    "title": "Linear Models",
    "section": "The Chi-square test and count data",
    "text": "The Chi-square test and count data\nThe Chi-square test, also known as \\chi^{2} test or chi-square test, is designed for scenarios where you want to statistically test how likely it is that an observed distribution of values is due to chance. It is also called a “goodness of fit” statistic, because it measures how well the observed distribution of data fits with the distribution that is expected if the variables of which measurements are made are independent. In our wing length example below, the two variables are temperature and food level.\nNote that a \\chi^{2} test is designed to analyze categorical data. That is the data have been counted (count data) and divided into categories. It is not meant for continuous data (such as body weight, genome size, or height). For example, if you want to test whether attending class influences how students perform on an exam, using test scores (from 0-100) as data would not be appropriate for a Chi-square test. However, arranging students into the categories “Pass” and “Fail” and counting up how many fall in each categories would be appropriate. Additionally, the data in a Chi-square table (see below) should not be in the form of percentages – only count data are allowed!\n\nThe Chi-square test with the wing length data\nWe can easily build a table for a Chi-square test on the wing length data as follows:\n\nmozwing <- mozwing[!is.na(mozwing$loglength),] # subset data to omit cells for individuals that did not survive to adulthood\n\nfactorTable <- table(mozwing$food_level,mozwing$temp)\nprint(factorTable)\n\n     \n      22 26 32\n  0.1 23 29 10\n  1   35 28 24\n\n\n Now let’s run the test:\n\nchisq.test(factorTable)\n\n\n    Pearson's Chi-squared test\n\ndata:  factorTable\nX-squared = 4.1883, df = 2, p-value = 0.1232\n\n\nThe “X-squared” value is the \\chi^{2} test statistic, akin to the t-value of the t-test or W value in the Wilcox test.\nThe \\chi^{2} statistic is calculated as the sum of the quantity\n\\frac{(\\mathrm{Observed} - \\mathrm{Expected})^2}{\\mathrm{Expected}}\nacross all the cells/categories in the table (so the sum would be over 6 categories in our current wing length example).\n“Observed” is the observed proportion of data that fall in a certain category. For example, at 22°C there are 23 individuals observed in the food level, 0.1 category, and 35 in the food level, 1 category.\n“Expected” is what count would be expected if the values in each category were truly independent. Each cell has its own expected value, which is simply calculated as the count one would expect in each category if the value were generated in proportion to the total number seen in that category. So in our example, the expected value for the food level, 0.1 category would be\n23+35 \\mathrm{~(Total~number~of~low~food~individuals)} \\times \\frac{23+29+10 \\mathrm{~(Total~number~in~the~\"0.1\"~category)}}{23+35+29+28+10+24 \\mathrm{~(Total~number~of~individuals)}} = 58 \\times \\frac{62}{87} = 17.19\nThe sum of all six (one for each cell in the table above) such calculations would be the \\chi^{2} value that R gave you through the chisq.test() above — try it!\nNow back to the R output from the chisq.test() above. Why df = 2? This is calculated as DF = (r - 1) * (c - 1) where r and c are the number of rows and columns in the \\chi^{2} table, respectively. The same principle you learned before applies here; you lose one degree of freedom for each new level of information you need to estimate: there is uncertainity about the information (number of categories) in both rows and columns, so you need to lose one degree of freedom for each.\nFinally, note that the p-value is non-significant — we can conclude that the factors are independent. This is expected because individuals could only be exposed to one of the six temperature-food treatments in the study’s experimental design. This dataset only contains females, however, the chisq.test() would be even more useful if the dataset contained values on both sexes. Later, we analyse these data using “interactions” later in multiple explanatory variables.\n\\star Include and run the \\chi^2 test in your script."
  },
  {
    "objectID": "linear_mod_activity.html#saving-data",
    "href": "linear_mod_activity.html#saving-data",
    "title": "Linear Models",
    "section": "Saving data",
    "text": "Saving data\nThe last thing to do is to save a copy of the wing length data, including our new column of log data.\n\\star Use this code in your script to create the saved data in your Data directory :\n\nsave(mozwing, file='mozwing.Rdata')"
  },
  {
    "objectID": "linear_mod_activity.html#introduction-2",
    "href": "linear_mod_activity.html#introduction-2",
    "title": "Linear Models",
    "section": "Introduction",
    "text": "Introduction\nIn this section we will explore fitting a linear model to data when you have multiple explanatory (predictor) variables.\nThe aims of this section are^{[1]}:\n\nLearning to build and fit a linear model that includes several explanatory variables\nLearning to interpret the summary tables and diagnostics after fitting a linear model with multiple explanatory variables"
  },
  {
    "objectID": "linear_mod_activity.html#an-example",
    "href": "linear_mod_activity.html#an-example",
    "title": "Linear Models",
    "section": "An example",
    "text": "An example\nWe will now look at a single model that includes both explanatory variables.\nThe first thing to do is look at the data again.\n\nExploring the data\n\\star Create a new blank script called MulExpl.R in your Code directory and add some introductory comments.\n\nwings <- read.csv(\"activities/data/traitdata_Huxleyetal_2021.csv\")\n\n Look back at the end of the previous section to see how you saved the RData file. If loglength is missing, just add it to the imported data frame again (go back to the ANOVA section and have a look if you have forgotten how). Also, check to see if temp and food_level are defined as factors.\nUse ls(), and then str to check that the data has loaded correctly:\n\nstr(wings)\n\n'data.frame':   270 obs. of  33 variables:\n $ ID                    : int  4 6 7 11 11 12 19 20 16 21 ...\n $ exp_no                : int  3 3 3 3 3 3 3 3 3 3 ...\n $ rep                   : chr  \"A\" \"A\" \"A\" \"A\" ...\n $ dens                  : num  0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ...\n $ rearing_vessel        : chr  \"tub\" \"tub\" \"tub\" \"tub\" ...\n $ temp                  : int  22 22 22 22 22 22 22 22 22 22 ...\n $ food_level            : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ treatment             : chr  \"22_0.1\" \"22_0.1\" \"22_0.1\" \"22_0.1\" ...\n $ feeding_interval      : int  24 24 24 24 24 24 24 24 24 24 ...\n $ egg_sub               : chr  \"29/03/2019 12:00\" \"29/03/2019 12:00\" \"29/03/2019 12:00\" \"29/03/2019 12:00\" ...\n $ l_emerg               : chr  \"30/03/2019 12:00\" \"30/03/2019 12:00\" \"30/03/2019 12:00\" \"30/03/2019 12:00\" ...\n $ l_emerg_days_frm_sub  : int  1 1 1 1 1 1 1 1 1 1 ...\n $ l_death               : chr  NA NA NA NA ...\n $ l_surv                : int  0 0 0 0 0 0 0 0 0 0 ...\n $ pupal_ref             : int  1 3 4 7 2 8 12 13 6 14 ...\n $ p_emerg               : chr  \"26/04/2019 12:00\" \"27/04/2019 12:00\" \"28/04/2019 12:00\" \"02/05/2019 12:00\" ...\n $ l_to_p_devtime        : int  28 29 30 34 30 34 35 36 32 36 ...\n $ l_to_p_devrate        : num  0.0357 0.0345 0.0333 0.0294 0.0333 ...\n $ p_death               : chr  NA NA NA NA ...\n $ p_surv                : int  0 0 0 0 0 0 0 0 0 0 ...\n $ a_emerg               : chr  \"29/04/2019 12:00\" \"30/04/2019 12:00\" \"01/05/2019 12:00\" \"05/05/2019 12:00\" ...\n $ p_to_a_devtime        : int  3 3 3 3 3 3 3 4 3 3 ...\n $ p_to_a_devrate        : num  0.333 0.333 0.333 0.333 0.333 ...\n $ a_death               : chr  \"07/05/2019 12:00\" \"05/05/2019 12:00\" \"06/05/2019 12:00\" \"11/05/2019 12:00\" ...\n $ adult_lifespan        : int  8 5 5 6 4 6 9 1 7 7 ...\n $ adult_mort_rate       : num  0.125 0.2 0.2 0.167 0.25 ...\n $ hatch_to_a_devtime    : int  31 32 33 37 33 37 38 40 35 39 ...\n $ hatch_to_adult_devrate: num  0.0323 0.0312 0.0303 0.027 0.0303 ...\n $ sex                   : chr  \"female\" \"female\" \"female\" \"female\" ...\n $ im_surv               : int  0 0 0 0 0 0 0 0 0 0 ...\n $ j_lifetime            : int  31 32 33 37 33 37 38 40 35 39 ...\n $ total_lifespan        : int  39 37 38 43 37 43 47 41 42 46 ...\n $ length_mm             : num  2.93 2.59 2.41 2.62 2.28 2.39 2.99 2.5 2.75 2.82 ...\n\n\n In the regression section, we asked if temperature and food level had meaningful effects on wing length. Now we want to ask questions like: How is the size-temperature relationship affected by different food levels?\nWe need to look at plots within groups.\nBefore we do that, there is a lot of data in the data frame that we don’t need for our analysis and we should make sure that we are using the same data for our plots and models. We will subset the data down to the complete data for the three variables and log the length of the wings:\n\nwings <- wings %>%\n  mutate(loglength = log(length_mm))\nwings <- subset(wings, select = c(temp, food_level, loglength)) \nwings <- na.omit(wings)\nwings$temp <- as.factor(wings$temp)\nwings$food_level <- as.factor(wings$food_level)\nstr(wings)\n\n'data.frame':   149 obs. of  3 variables:\n $ temp      : Factor w/ 3 levels \"22\",\"26\",\"32\": 1 1 1 1 1 1 1 1 1 1 ...\n $ food_level: Factor w/ 2 levels \"0.1\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ loglength : num  1.075 0.952 0.88 0.963 0.824 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:121] 38 42 60 103 136 142 143 157 158 159 ...\n  ..- attr(*, \"names\")= chr [1:121] \"38\" \"42\" \"60\" \"103\" ...\n\n\n\nRemember that lattice, like base R, will automatically plot boxplots when the explanatory variables of interest are defined as factors.\n\n\n\nBoxplots within groups\nIn the regression section, we used the subset option to fit a model just to dragonflies. You can use subset with plots too.\n\\star Add par(mfrow=c(1,2)) to your script to split the graphics into two panels.\n\\star Copy over and modify the code from the ANOVA section to create a boxplot of genome size by trophic level into your script.\n\\star Now further modify the code to generate the plots shown in the figure below (you will have to subset your data for this, and also use the subset option of the plot command).\n\nYou can use the `plot` function's option `main = ` to add titles to a plot.\n\n\n\nlattice again\nRecall that the lattice package provides some very neat extra ways to plot data in groups. They look pretty but the downside is that they don’t use the same graphics system — all those par commands are useless for these graphs. The defaults look good though!\n\nlibrary(lattice)\nbwplot(loglength ~ temp | food_level, data= wings)\n\n\n\n\nThe code loglength ~ temp | food_level means plot the relationship between wing length and temperature, but group within levels of food supply. We are using the function bwplot, which is provided by lattice to create box and whisker plots.\n\\star Create the lattice plots above from within your script.\nRearrange this code to have three plots, showing the box and whisker plots for food_level, grouped within the levels of temp.\nTry reshaping the R plot window and running the command again. Lattice tries to make good use of the available space when creating lattice plots.\n\n\nBarplots again\nWe’re going to make the barplot code from the regression section even more complicated! This time we want to know the mean log wing length within combinations of temp and food_level. We can still use tapply, providing more than one grouping factor. We create a set of grouping factors like this:\n\ngroups     <- list(wings$temp, wings$food_level)\ngroupMeans <- tapply(wings$loglength, groups, FUN = mean)\nprint(groupMeans)\n\n         0.1         1\n22 0.9720317 1.1406533\n26 0.8937470 1.0744125\n32 0.8216252 0.9513008\n\n\n\\star Copy this code into your script and run it.\nUse this code and the script from the ANOVA section to get the set of standard errors for the groups groupSE:\n\nseMean <- function(x){\n    # get rid of missing values\n    x <- na.omit(x)\n    # calculate the standard error\n    se <- sqrt(var(x)/length(x))\n    # tell the function to report the standard error\n    return(se)\n}\n\n\ngroups <- list(wings$food_level, wings$temp)\n\ngroupMeans <- tapply(wings$loglength, groups, FUN=mean)\nprint(groupMeans)\n\n           22       26        32\n0.1 0.9720317 0.893747 0.8216252\n1   1.1406533 1.074412 0.9513008\n\n\n\ngroupSE <- tapply(wings$loglength, groups, FUN=seMean)\nprint(groupSE)\n\n             22          26          32\n0.1 0.017990977 0.014147233 0.019430021\n1   0.004732957 0.005898363 0.005579193\n\n\n Now we can use barplot. The default option for a barplot of a table is to create a stacked barplot, which is not what we want. The option beside=TRUE makes the bars for each column appear side by side.\nOnce again, we save the midpoints of the bars to add the error bars. The other options in the code below change the colours of the bars and the length of error bar caps.\n\n# get upper and lower standard error height\nupperSE <- groupMeans + groupSE\nlowerSE <- groupMeans - groupSE\n# create barplot\nbarMids <- barplot(groupMeans, ylim=c(0, max(upperSE)), beside=TRUE, ylab= ' log (wing length (mm)) ' , col=c( ' white ' , ' grey70 '))\narrows(barMids, upperSE, barMids, lowerSE, ang=90, code=3, len=0.05)\n\n\n\n\n\\star Generate the barplot above and then edit your script to change the colours and error bar lengths to your taste.\n\n\nPlotting means and confidence intervals\nWe’ll use the plotmeans function again as an exercise to change graph settings and to prepare figures for reports and write ups. This is the figure you should be able to reproduce the figure below.\n\nReplace image\n\n\nMeans and 95% confidence intervals for logged wing length (mm) in Aedes aegypti for (a) different temperature levels and (b) for different larval food levels\n\n\n\n\\star Use plotmeans from the ANOVA section and the subset option to generate the two plots below. You will need to set the ylim option for the two plots to make them use the same y axis.\n\\star Use text to add labels — the command par('usr') will show you the limits of the plot (x_{min}, x_{max}, y_{min}, y_{max}) and help pick a location for the labels.\n\\star Change the par settings in your code and redraw the plots to try and make better use of the space. In the example below, the box shows the edges of the R graphics window.\nNote the following about the the figure above (generated using plotmeans)):\n\nWhite space: The default options in R use wide margins and spaced out axes and take up a lot of space that could be used for plotting data. You’ve already seen the par function and the options mfrow for multiple plots and mar to adjust margin size. The option mgp adjusts the placement of the axis label, tick labels and tick locations. See ?par for help on the these options.\nMain titles: Adding large titles to graphs is also a bad idea — it uses lots of space to explain something that should be in the figure legend. With multiple plots in a figure, you have to label graphs so that the figure legend can refer to them. You can add labels using text(x,y,'label').\nFigure legends: A figure caption and legend should give a clear stand-alone description of the whole figure.\nReferring to figures: You must link from your text to your figures — a reader has to know which figures refer to which results. So: “There are clear differences in mean genome size between species at different trophic levels and between ground dwelling and other species, Figure xx”."
  },
  {
    "objectID": "linear_mod_activity.html#fitting-the-linear-model",
    "href": "linear_mod_activity.html#fitting-the-linear-model",
    "title": "Linear Models",
    "section": "Fitting the linear model",
    "text": "Fitting the linear model\nAll those exploratory visualizations suggest:\n\nBody size in Aedes aegytpi decreases with temperature, which is consistent with the size-temperature rule for many ectotherms.\nBody size in Aedes aegytpi increases with larval food supply\n\nWe suspected these things from the ANOVA section analyses, but now we can see that they might have separate effects. We’ll fit a linear model to explore this and add the two explanatory variables together.\n\\star This is an important section — read it through carefully and ask questions if you are unsure. Copy the code into your script and add comments. Do not just jump to the next action item!\n\\star First, fit the model:\n\nmodel <- lm(loglength ~ temp + food_level, data = mozwing)  \n\nWe’re going to do things right this time and check the model diagnostics before we rush into interpretation.\n\nlibrary(repr) ; options(repr.plot.res = 100, repr.plot.width = 7, repr.plot.height = 8) # Change plot size\n\n\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\n\nlibrary(repr) ; options(repr.plot.res = 100, repr.plot.width = 6, repr.plot.height = 6) # Change plot size\n\n Examine these diagnostic plots. There are six predicted values now - three temperature levels for each of the two food levels. Those plots look ok so now we can look at the analysis of variance table:\n\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: loglength\n            Df  Sum Sq Mean Sq F value    Pr(>F)    \ntemp         2 0.58891 0.29445  96.404 < 2.2e-16 ***\nfood_level   1 0.96584 0.96584 316.215 < 2.2e-16 ***\nResiduals  145 0.44288 0.00305                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n Ignore the p values! Yes, they’re highly significant but we want to understand the model, not rubber stamp it with ‘significant’.\nThe sums of squares for the variables are both large compared to the residual sums of squares — there is lots of explained variation! We can calculate the r^2 as explained sums of squares over total sums of squares:\n\\frac{0.59 + 0.97}{0.59 + 0.97 + 0.44} = \\frac{3.56}{16.77} = 0.78\nFood level explain more variation than temperature — this makes intuitive sense from the plots since there are big differences between in the figure we generated above (using plotmeans) (a vs b), but smaller differences within.\nWe could also calculate a significance for the whole model by merging the terms. The total explained sums of squares of 0.59 + 0.97 = 1.56 uses 2+1 =3 degrees of freedom, so the mean sums of squares for all the terms together is 1.56/3=0.52. Dividing this by the residual mean square of 0.003 gives an F of 0.52 / 0.003 = 173.33.\nNow we can look at the summary table to see the coefficients:\n\nsummary(model) \n\n\nCall:\nlm(formula = loglength ~ temp + food_level, data = mozwing)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.193070 -0.024347  0.004734  0.032994  0.121469 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.973804   0.009180 106.078  < 2e-16 ***\ntemp26      -0.072698   0.010361  -7.017 8.05e-11 ***\ntemp32      -0.177597   0.011975 -14.830  < 2e-16 ***\nfood_level1  0.165684   0.009317  17.782  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05527 on 145 degrees of freedom\nMultiple R-squared:  0.7783,    Adjusted R-squared:  0.7737 \nF-statistic: 169.7 on 3 and 145 DF,  p-value: < 2.2e-16\n\n\n Starting at the bottom of this output, summary has again calculated r^2 for us and also an F statistic for the whole model, which matches the calculation above.\nThe other important bits are the four coefficients. The intercept is now the reference level for two variables: it is the mean for individuals that were reared at 22°C with low food supply (0.1 mg day^-1). We then have differences from this value for individuals reared at the other temperatures. There is a big change in wing length associated with increased temperature and food level and both of these have large effects sizes. Increasing temperature from 22°C to 32°C introduced about a 20% difference in wing length. Increased food at 22°C increased wing by ~20%.\nBecause the difference is large and the standard error is small, the t value suggests that this difference did not arise just by chance. Put another way, it is significant.\nThe table below shows how these four coefficients combine to give the predicted values for each of the group means.\n\n\n\n\n\n\n\n\n\n\n22°C\n26°C\n32°C\n\n\n\n\nlow food\n0.98 = 0.98\n0.98 - 0.07 = 0.91\n0.98 - 0.18 = 0.70\n\n\nhigh food\n0.98 + 0.17 = 1.15\n0.98 - 0.07 + 0.17 =1.08\n0.98 - 0.18 + 0.17 = 0.97"
  },
  {
    "objectID": "linear_mod_activity.html#introduction-3",
    "href": "linear_mod_activity.html#introduction-3",
    "title": "Linear Models",
    "section": "Introduction",
    "text": "Introduction\nHere you will build on your skills in fitting linear models with multiple explanatory variables to data. You will learn about another commonly used Linear Model fitting technique: ANCOVA.\nWe will build two models in this section:\n\nModel 1: Is Ae. aegypti wing length predicted by interactions between temperature and food level?\nANCOVA: Is body size in Odonata predicted by interactions between genome size and taxonomic suborder?\n\nSo far, we have only looked at the independent effects of variables. For example, in the temperature and food level model from the first multiple explanatory variables section, we only looked for specific differences for being exposed to low larval food supply or high larval food supply, not for being specifically reared at a specific temperature-food level treatment. These independent effects of a variable are known as main effects and the effects of combinations of variables acting together are known as interactions — they describe how the variables interact.\n\nSection aims\nThe aims of this section are^{[1]}:\n\nCreating more complex Linear Models with multiple explanatory variables\nIncluding the effects of interactions between multiple variables in a linear model\nPlotting predictions from more complex (multiple explanatory variables) linear models\n\n\n\nFormulae with interactions in R\nWe’ve already seen a number of different model formulae in R. They all use this syntax:\nresponse variable ~ explanatory variable(s)\nBut we are now going to see two extra pieces of syntax:\n\ny ~ a + b + a:b: The a:b means the interaction between a and b — do combinations of these variables lead to different outcomes?\ny ~ a * b: This a shorthand for the model above. The means fit a and b as main effects and their interaction a:b."
  },
  {
    "objectID": "linear_mod_activity.html#model-1-aedes-aegypti-wing-length",
    "href": "linear_mod_activity.html#model-1-aedes-aegypti-wing-length",
    "title": "Linear Models",
    "section": "Model 1: Aedes aegypti wing length",
    "text": "Model 1: Aedes aegypti wing length\n\\star Make sure you have changed the working directory to Code in your stats coursework directory.\n\\star Create a new blank script called ‘Interactions.R’ and add some introductory comments.\n\\star Read in the data:\nFor practice, subset the dataset, define any categorical variables, and add any columns that might missing. Revist the previous sections, if you have forgotten what is needed!\nLet’s refit the model from the earlier multiple explanatory variable section, but including the interaction between temperature and food level. We’ll immediately check the model is appropriate:\n\nmodel <- lm(loglength ~ temp * food_level, data= wings)\npar(mfrow=c(2,2), mar=c(3,3,1,1), mgp=c(2, 0.8,0))\nplot(model)   \n\n\n\n\nNow, examine the anova and summary outputs for the model:\n\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: loglength\n                 Df  Sum Sq Mean Sq  F value Pr(>F)    \ntemp              2 0.58891 0.29445  97.8286 <2e-16 ***\nfood_level        1 0.96584 0.96584 320.8883 <2e-16 ***\ntemp:food_level   2 0.01247 0.00623   2.0714 0.1298    \nResiduals       143 0.43042 0.00301                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n Compared to the model from the first multiple explanatory variables section, there is an extra line at the bottom. The top two are the same and show that temperature and food level both have independent main effects. The extra line shows that there is also an interaction between the two. It doesn’t explain very much variation, and it is non-significant.\nAgain, we can calculate the r^2 for the model:\n\\frac{0.59 + 0.97 + 0.01}{0.59 + 0.97 + 0.01 + 0.43} = 0.78\nThe model from the first multiple explanatory variables section without the interaction had an r^2 = 0.78 — our new model explains 0% more of the variation in the data.\nThe summary table is as follows:\n\nsummary(model)\n\n\nCall:\nlm(formula = loglength ~ temp * food_level, data = wings)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18571 -0.02245  0.00357  0.03279  0.12510 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.97203    0.01144  84.971  < 2e-16 ***\ntemp26             -0.07828    0.01532  -5.110 1.01e-06 ***\ntemp32             -0.15041    0.02078  -7.238 2.57e-11 ***\nfood_level1         0.16862    0.01473  11.450  < 2e-16 ***\ntemp26:food_level1  0.01204    0.02069   0.582    0.561    \ntemp32:food_level1 -0.03895    0.02536  -1.536    0.127    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05486 on 143 degrees of freedom\nMultiple R-squared:  0.7845,    Adjusted R-squared:  0.777 \nF-statistic: 104.1 on 5 and 143 DF,  p-value: < 2.2e-16\n\n\n The lines in this output are:\n\nThe reference level (intercept) for low-food at 22°C. (The reference level is decided just by alphabetic/numeric order of the levels)\nTwo differences for being in different food levels.\nOne difference for being high-food at 22°C.\nTwo new differences that give specific differences for high-food at 26 and 32°C.\n\nThe first four lines, as in the model from the ANOVA section, which would allow us to find the predicted values for each group if the size of the differences did not vary between levels because of the interactions. That is, this part of the model only includes a single difference low-food and high-food, which has to be the same for each temperature because it ignores interactions between temperature and low- / high-food identity of each treatment. The last two lines then give the estimated coefficients associated with the interaction terms, and allow cause the size of differences to vary between levels because of the further effects of interactions.\nThe table below show how these combine to give the predictions for each group combination, with those two new lines show in red:\n\n\n\n\n\n\n\n\n\nlow food\nhigh food\n\n\n\n\n22°C\n0.97 = 0.97\n0.97 + 0.17 = 1.14\n\n\n26°C\n0.97 - 0.08 = 0.89\n0.97 - 0.08 + 0.17+ 0.01 = 1.07\n\n\n32°C\n0.97 - 0.15 = 0.82\n0.97 - 0.15 + 0.17- 0.04 = 0.95\n\n\n\nSo why are there two new coefficients? For interactions between two factors, there are always (n-1)\\times(m-1) new coefficients, where n and m are the number of levels in the two factors (low- or high-food: 2 levels and temperature: 3 levels, in our current example). So in this model, (3-1) \\times (2-1) =2. It might be easier to understand why graphically:\n\nThe prediction for the white boxes above can be found by adding the main effects together but for the grey boxes, we need to find specific differences. So there are (n-1)\\times(m-1) interaction coefficients (count the number of grey boxes) to add.\nIf we put this together, what is the model telling us?\n\nThe interaction model does’t tell us anything more than the model without interactions – body size in Ae. aegypti decreases with temperature and increases with food level.\n\nThis finding suggests that food limitation can exacerbate the negative effect that increased temperature has on body size. However, the interaction term in the anova output suggests that variation between food levels in degree to which body size decreases with temperature is non-significant.\n\\star Copy the code above into your script and run the model.\nMake sure you understand the output!\nJust to make sure the sums above are correct, we’ll use the same code as in earlier multiple explanatory variables section to get R to calculate predictions for us, similar to the way we did before:\nAgain, remember that the each = 2 option repeats each value twice in succession; the times = 3 options repeats the whole set of values (the whole vector) three times.\n\\star Include and run the code for gererating these predictions in your script.\nIf we plot these data points onto the barplot from the first multiple explanatory variables section, they now lie exactly on the mean values, because we’ve allowed for interactions. How much overlap would you expect, if you were to plot the predictions from the model without the interaction term?"
  },
  {
    "objectID": "linear_mod_activity.html#model-2-ancova-body-weight-in-odonata",
    "href": "linear_mod_activity.html#model-2-ancova-body-weight-in-odonata",
    "title": "Linear Models",
    "section": "Model 2 (ANCOVA): Body Weight in Odonata",
    "text": "Model 2 (ANCOVA): Body Weight in Odonata\nWe’ll go all the way back to the regression analyses from the Regression section. Remember that we fitted two separate regression lines to the data for damselflies and dragonflies. We’ll now use an interaction to fit these in a single model. This kind of linear model — with a mixture of continuous variables and factors — is often called an analysis of covariance, or ANCOVA. That is, ANCOVA is a type of linear model that blends ANOVA and regression. ANCOVA evaluates whether population means of a dependent variable are equal across levels of a categorical independent variable, while statistically controlling for the effects of other continuous variables that are not of primary interest, known as covariates.\nThus, ANCOVA is a linear model with one categorical and one or more continuous predictors.\nWe will use the odonates data.\n\\star First load the data:\n\nodonata <- read.csv(\"activities/data/GenomeSize.csv\")\n\n  \\star Now create two new variables in the odonata data set called logGS and logBW containing log genome size and log body weight:\n\nodonata$logGS <- log(odonata$GenomeSize)\nodonata$logBW <- log(odonata$BodyWeight)\n\n The models we fitted before looked like this:\n\nWe can now fit the model of body weight as a function of both genome size and suborder:\n\nodonModel <- lm(logBW ~ logGS * Suborder, data = odonata)\n\n\nsummary(odonModel)\n\n\nCall:\nlm(formula = logBW ~ logGS * Suborder, data = odonata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3243 -0.3225  0.0073  0.3962  1.4976 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -2.39947    0.08475 -28.311  < 2e-16 ***\nlogGS                    1.00522    0.22367   4.494 1.99e-05 ***\nSuborderZygoptera       -2.24895    0.13540 -16.610  < 2e-16 ***\nlogGS:SuborderZygoptera -2.14919    0.46186  -4.653 1.07e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6499 on 94 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7549,    Adjusted R-squared:  0.7471 \nF-statistic:  96.5 on 3 and 94 DF,  p-value: < 2.2e-16\n\n\n Again, we’ll look at the anova table first:\n\nanova(odonModel)\n\nAnalysis of Variance Table\n\nResponse: logBW\n               Df  Sum Sq Mean Sq F value    Pr(>F)    \nlogGS           1   1.144   1.144   2.710    0.1031    \nSuborder        1 111.968 111.968 265.133 < 2.2e-16 ***\nlogGS:Suborder  1   9.145   9.145  21.654 1.068e-05 ***\nResiduals      94  39.697   0.422                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n Interpreting this:\n\nThere is no significant main effect of log genome size. The main effect is the important thing here — genome size is hugely important but does very different things for the two different suborders. If we ignored Suborder, there isn’t an overall relationship: the average of those two lines is pretty much flat.\nThere is a very strong main effect of Suborder: the mean body weight in the two groups are very different.\nThere is a strong interaction between suborder and genome size. This is an interaction between a factor and a continuous variable and shows that the slopes are different for the different factor levels.\n\nNow for the summary table:\n\nsummary(odonModel)\n\n\nCall:\nlm(formula = logBW ~ logGS * Suborder, data = odonata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3243 -0.3225  0.0073  0.3962  1.4976 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -2.39947    0.08475 -28.311  < 2e-16 ***\nlogGS                    1.00522    0.22367   4.494 1.99e-05 ***\nSuborderZygoptera       -2.24895    0.13540 -16.610  < 2e-16 ***\nlogGS:SuborderZygoptera -2.14919    0.46186  -4.653 1.07e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6499 on 94 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7549,    Adjusted R-squared:  0.7471 \nF-statistic:  96.5 on 3 and 94 DF,  p-value: < 2.2e-16\n\n\n - The first thing to note is that the r^2 value is really high. The model explains three quarters (0.752) of the variation in the data.\n\nNext, there are four coefficients:\n\nThe intercept is for the first level of Suborder, which is Anisoptera (dragonflies).\nThe next line, for log genome size, is the slope for Anisoptera.\nWe then have a coefficient for the second level of Suborder, which is Zygoptera (damselflies). As with the first model, this difference in factor levels is a difference in mean values and shows the difference in the intercept for Zygoptera.\nThe last line is the interaction between Suborder and logGS. This shows how the slope for Zygoptera differs from the slope for Anisoptera.\n\n\nHow do these hang together to give the two lines shown in the model? We can calculate these by hand:\n\\begin{align*}\n\\textrm{Body Weight} &= -2.40 + 1.01 \\times \\textrm{logGS} & \\textrm{[Anisoptera]}\\\\\n\\textrm{Body Weight} &= (-2.40 -2.25) + (1.01 - 2.15) \\times \\textrm{logGS} & \\textrm{[Zygoptera]}\\\\\n        &= -4.65 - 1.14 \\times \\textrm{logGS}\n\\end{align*}\n\\star Add the above code into your script and check that you understand the outputs.\nWe’ll use the predict function again to get the predicted values from the model and add lines to the plot above.\nFirst, we’ll create a set of numbers spanning the range of genome size:\n\n#get the range of the data:\nrng <- range(odonata$logGS)\n#get a sequence from the min to the max with 100 equally spaced values:\nLogGSForFitting <- seq(rng[1], rng[2], length = 100)\n\n Have a look at these numbers:\n\nprint(LogGSForFitting)\n\n  [1] -0.891598119 -0.873918728 -0.856239337 -0.838559945 -0.820880554\n  [6] -0.803201163 -0.785521772 -0.767842380 -0.750162989 -0.732483598\n [11] -0.714804206 -0.697124815 -0.679445424 -0.661766032 -0.644086641\n [16] -0.626407250 -0.608727859 -0.591048467 -0.573369076 -0.555689685\n [21] -0.538010293 -0.520330902 -0.502651511 -0.484972119 -0.467292728\n [26] -0.449613337 -0.431933946 -0.414254554 -0.396575163 -0.378895772\n [31] -0.361216380 -0.343536989 -0.325857598 -0.308178207 -0.290498815\n [36] -0.272819424 -0.255140033 -0.237460641 -0.219781250 -0.202101859\n [41] -0.184422467 -0.166743076 -0.149063685 -0.131384294 -0.113704902\n [46] -0.096025511 -0.078346120 -0.060666728 -0.042987337 -0.025307946\n [51] -0.007628554  0.010050837  0.027730228  0.045409619  0.063089011\n [56]  0.080768402  0.098447793  0.116127185  0.133806576  0.151485967\n [61]  0.169165358  0.186844750  0.204524141  0.222203532  0.239882924\n [66]  0.257562315  0.275241706  0.292921098  0.310600489  0.328279880\n [71]  0.345959271  0.363638663  0.381318054  0.398997445  0.416676837\n [76]  0.434356228  0.452035619  0.469715011  0.487394402  0.505073793\n [81]  0.522753184  0.540432576  0.558111967  0.575791358  0.593470750\n [86]  0.611150141  0.628829532  0.646508923  0.664188315  0.681867706\n [91]  0.699547097  0.717226489  0.734905880  0.752585271  0.770264663\n [96]  0.787944054  0.805623445  0.823302836  0.840982228  0.858661619\n\n\n We can now use the model to predict the values of body weight at each of those points for each of the two suborders:\n\n#get a data frame of new data for the order\nZygoVals <- data.frame(logGS = LogGSForFitting, Suborder = \"Zygoptera\")\n\n#get the predictions and standard error\nZygoPred <- predict(odonModel, newdata = ZygoVals, se.fit = TRUE)\n\n#repeat for anisoptera\nAnisoVals <- data.frame(logGS = LogGSForFitting, Suborder = \"Anisoptera\")\nAnisoPred <- predict(odonModel, newdata = AnisoVals, se.fit = TRUE)\n\n We’ve added se.fit=TRUE to the function to get the standard error around the regression lines. Both AnisoPred and ZygoPred contain predicted values (called fit) and standard error values (called se.fit) for each of the values in our generated values in LogGSForFitting for each of the two suborders.\nWe can add the predictions onto a plot like this:\n\n# plot the scatterplot of the data\n\n\nodonata$Suborder <- as.factor(odonata$Suborder)\nplot(logBW ~ logGS, data = odonata, col = Suborder)\n# add the predicted lines\nlines(AnisoPred$fit ~ LogGSForFitting, col = \"black\")\nlines(AnisoPred$fit + AnisoPred$se.fit ~ LogGSForFitting, col = \"black\", lty = 2)\nlines(AnisoPred$fit - AnisoPred$se.fit ~ LogGSForFitting, col = \"black\", lty = 2)\n\n\n\n\n\\star Copy the prediction code into your script and run the plot above.\nCopy and modify the last three lines to add the lines for the Zygoptera. Your final plot should look like this:"
  },
  {
    "objectID": "linear_mod_activity.html#introduction-4",
    "href": "linear_mod_activity.html#introduction-4",
    "title": "Linear Models",
    "section": "Introduction",
    "text": "Introduction\nIn biology, we often use statistics to compare competing hypotheses in order to work out the simplest explanation for some data. This often involves collecting several explanatory variables that describe different hypotheses and then fitting them together in a single model, and often including interactions between those variables.\nIn all likelihood, not all of these model terms will be important. If we remove unimportant terms, then the explanatory power of the model will get worse, but might not get significantly worse.\n\n“It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience.”\n– Albert Einstein\n\nOr to paraphrase:\n\n“Everything should be made as simple as possible, but no simpler.”\n\nThe approach we will look at is to start with a maximal model — the model that contains everything that might be important — and simplify it towards the null model — the model that says that none of your variables are important. Hopefully, there is a point somewhere in between where you can’t remove any further terms without making the model significantly worse: this is called the minimum adequate model.\n\n\nSection aims\nThe main aim of this section^{[1]} is to learn how to build and then simplify complex models by removing non-explanatory terms, to arrive at the Minimum Adequate Model.\n\n\nThe process of model simplification\nModel simplification is an iterative process. The flow diagram below shows how it works: at each stage you try and find an acceptable simplification. If successful, then you start again with the new simpler model and try and find a way to simplify this, until eventually, you can’t find anything more to remove.\n\nAs always, we can use an F-test to compare two models and see if they have significantly different explanatory power (there are also other ways to do this, such as using the Akaike Information Criterion, but we will not cover this here). In this context, the main thing to remember is that significance of the F-test used to compare a model and its simplified counterpart is a bad thing — it means that we’ve removed a term from the fitted model that makes it significantly worse."
  },
  {
    "objectID": "linear_mod_activity.html#an-example-1",
    "href": "linear_mod_activity.html#an-example-1",
    "title": "Linear Models",
    "section": "An example",
    "text": "An example\nWe’ll be using the wing length dataset for this practical, so once again:\n\\star Make sure you have changed the working directory to your stats module code folder.\n\\star Create a new blank script called MyModelSimp.R.\n\\star Load the wing length data into a data frame called wings:\n\nwings <- read.csv('activities/data/traitdata_Huxleyetal_2021.csv',stringsAsFactors = TRUE)\n\n In previous sections, we looked at how the categorical variables temp and food_level predicted wing length in in Aedes aegypti. In this section, we will add in another continuous variable: adult lifespan. The first thing we will do is to log both variables and reduce the dataset to the rows for which all of these data are available:\n\nwings$temp       <- as_factor(wings$temp)\nwings$food_level <- as_factor(wings$food_level)\n\nwings$loglength   <- log(wings$length_mm)\nwings$loglifespan <- log(wings$adult_lifespan)\n\n# reduce dataset to four key variables\nwings <- subset(wings, select = c(loglength, loglifespan, temp, food_level)) \n\n# remove the row with missing data\nwings <- na.omit(wings)\n\n \\star Copy the code above into your script and run it\nCheck that the data you end up with has this structure:\n\nstr(wings)\n\n'data.frame':   149 obs. of  4 variables:\n $ loglength  : num  1.075 0.952 0.88 0.963 0.824 ...\n $ loglifespan: num  2.08 1.61 1.61 1.79 1.39 ...\n $ temp       : Factor w/ 3 levels \"22\",\"26\",\"32\": 1 1 1 1 1 1 1 1 1 1 ...\n $ food_level : Factor w/ 2 levels \"0.1\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:121] 38 42 60 103 136 142 143 157 158 159 ...\n  ..- attr(*, \"names\")= chr [1:121] \"38\" \"42\" \"60\" \"103\" ..."
  },
  {
    "objectID": "linear_mod_activity.html#a-maximal-model",
    "href": "linear_mod_activity.html#a-maximal-model",
    "title": "Linear Models",
    "section": "A Maximal model",
    "text": "A Maximal model\nFirst let’s fit a model including all of these variables and all of the interactions:\n\nmodel <- lm(formula = loglifespan ~ loglength * temp * food_level, data = wings)\n\n\\star Add this model-fitting step in your script.\n\\star Look at the output of anova(model) and summary(model).\nScared? Don’t be! There are a number of points to this exercise:\n\nThese tables show exactly the kind of output you’ve seen before. Sure, there are lots of rows but each row is just asking whether a model term (anova) or a model coefficient (summary) is significant.\nSome of the rows are significant, others aren’t: some of the model terms are not explanatory.\nThe two tables show similar things - only a few stars for the anova table and the summary table.\nThat last line in the anova table: loglength:temp:food_level. This is an interaction of three variables capturing how the slope for lifespan changes for different wing lengths for individuals in different temperature-food levels. Does this seem easy to understand?\n\nThe real lesson here is that it is easy to fit complicated models in R.\nUnderstanding and explaining them is a different matter.\nThe temptation is always to start with the most complex possible model but this is rarely a good idea."
  },
  {
    "objectID": "linear_mod_activity.html#a-better-maximal-model",
    "href": "linear_mod_activity.html#a-better-maximal-model",
    "title": "Linear Models",
    "section": "A better maximal model",
    "text": "A better maximal model\nInstead of all possible interactions, we’ll consider two-way interactions: how do pairs of variables affect each other?\nThere is a shortcut for this: y ~ (a + b + c)^2 gets all two way combinations of the variables in the brackets, so is a quicker way of getting this model:\ny ~ a + b + c + a:b + a:c + b:c\nSo let’s use this to fit a simpler maximal model:\n\nmodel <- lm(loglifespan ~ (loglength + temp + food_level)^2, data = wings)\n\nThe anova table for this model looks like this:\n\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: loglifespan\n                      Df  Sum Sq Mean Sq  F value    Pr(>F)    \nloglength              1 29.9465 29.9465 299.2418 < 2.2e-16 ***\ntemp                   2 17.5835  8.7918  87.8522 < 2.2e-16 ***\nfood_level             1  0.0488  0.0488   0.4875  0.486215    \nloglength:temp         2  1.3925  0.6963   6.9573  0.001319 ** \nloglength:food_level   1  0.1193  0.1193   1.1921  0.276787    \ntemp:food_level        2  0.5558  0.2779   2.7768  0.065693 .  \nResiduals            139 13.9104  0.1001                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n The first lines are the main effects: two are significant and one isn’t. Then there are the three interactions. One of these is very significant: loglength:temp, which suggests that the slope of log lifespan value with wing length differs between temperatures. The other interactions are non-significant although some are close.\n\\star Run this model in your script.\n\\star Look at the output of anova(model) and summary(model).\n\\star Generate and inspect the model diagnostic plots."
  },
  {
    "objectID": "linear_mod_activity.html#model-simplification-1",
    "href": "linear_mod_activity.html#model-simplification-1",
    "title": "Linear Models",
    "section": "Model simplification",
    "text": "Model simplification\nNow let’s simplify the model we fitted above. Model simplification is not as straightforward as just dropping terms. Each time you remove a term from a model, the model will change: the model will get worse, since some of the sums of squares are no longer explained, but the remaining variables may partly compensate for this loss of explanatory power. The main point is that if it gets only a little worse, its OK, as the tiny amount of additional variation explained by the term you removed is not really worth it.\nBut how much is “tiny amount”? This is what we will learn now by using the F-test. Again, remember: significance of the F-test used to compare a model and its simplified counterpart is a bad thing — it means that we’ve removed a term from the fitted model that makes the it significantly worse.\nThe first question is: what terms can you remove from a model? Obviously, you only want to remove non-significant terms, but there is another rule – you cannot remove a main effect or an interaction while those main effects or interactions are present in a more complex interaction. For example, in the model y ~ a + b + c + a:b + a:c + b:c, you cannot drop c without dropping both a:c and b:c.\nThe R function drop.scope tells you what you can drop from a model. Some examples:\n\ndrop.scope(model)\n\n[1] \"loglength:temp\"       \"loglength:food_level\" \"temp:food_level\"     \n\n\n\ndrop.scope(y ~ a + b + c + a:b)\n\n[1] \"c\"   \"a:b\"\n\n\n\ndrop.scope(y ~ a + b + c + a:b + b:c + a:b:c)\n\n[1] \"a:b:c\"\n\n\n The last thing we need to do is work out how to remove a term from a model. We could type out the model again, but there is a shortcut using the function update:\n\n# a simple model\nf <- y ~ a + b + c + b:c\n\n# remove b:c from the current model\nupdate(f, . ~ . - b:c)\n\ny ~ a + b + c\n\n\n\n# model g as a response using the same explanatory variables.\nupdate(f, g ~ .)\n\ng ~ a + b + c + b:c\n\n\n Yes, the syntax is weird. The function uses a model or a formula and then allows you to alter the current formula. The dots in the code . ~ . mean ‘use whatever is currently in the response or explanatory variables’. It gives a simple way of changing a model.\nNow that you have learned the syntax, let’s try model simplification with the mammals dataset.\nFrom the above anova and drop.scope output, we know that the interaction loglength:temp:food_level is not significant and a valid term. So, let’s remove this term:\n\nmodel2 <- update(model, . ~ . - loglength:temp:food_level)\n\n And now use ANOVA to compare the two models:\n\nanova(model, model2)\n\nAnalysis of Variance Table\n\nModel 1: loglifespan ~ (loglength + temp + food_level)^2\nModel 2: loglifespan ~ loglength + temp + food_level + loglength:temp + \n    loglength:food_level + temp:food_level\n  Res.Df   RSS Df Sum of Sq F Pr(>F)\n1    139 13.91                      \n2    139 13.91  0         0         \n\n\n This tells us that model2 is not significantly worse than model. That is, dropping that one interaction term did not result in much of a loss of predictability.\nNow let’s look at this simplified model and see what else can be removed:\n\nanova(model2)\n\nAnalysis of Variance Table\n\nResponse: loglifespan\n                      Df  Sum Sq Mean Sq  F value    Pr(>F)    \nloglength              1 29.9465 29.9465 299.2418 < 2.2e-16 ***\ntemp                   2 17.5835  8.7918  87.8522 < 2.2e-16 ***\nfood_level             1  0.0488  0.0488   0.4875  0.486215    \nloglength:temp         2  1.3925  0.6963   6.9573  0.001319 ** \nloglength:food_level   1  0.1193  0.1193   1.1921  0.276787    \ntemp:food_level        2  0.5558  0.2779   2.7768  0.065693 .  \nResiduals            139 13.9104  0.1001                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ndrop.scope(model2)\n\n[1] \"loglength:temp\"       \"loglength:food_level\" \"temp:food_level\"     \n\n\n \\star Add this first simplification to your script and re-run it.\n\\star Look at the output above and decide what is the next possible term to delete\n\\star Using the code above as a model, create model3 as the next simplification. (remember to use model2 in your update call and not model)."
  },
  {
    "objectID": "linear_mod_activity.html#exercise-1",
    "href": "linear_mod_activity.html#exercise-1",
    "title": "Linear Models",
    "section": "Exercise",
    "text": "Exercise\nNow for a more difficult exercise:\n\\star Using the code above to guide you, try and find a minimal adequate model that you are happy with. In each step, the output of anova(model, modelN) should be non-significant (where N is the current step).\nIt can be important to consider both anova and summary tables. It can be worth trying to remove things that look significant in one table but not the other — some terms can explain significant variation on the anova table but the coefficients are not significant.\nRemember to remove terms: with categorical variables, several coefficients in the summary table may come from one term in the model and have to be removed together.\nWhen you have got your final model, save the model as an R data file: save(modelN, file='myFinalModel.Rda').\n\n [1]: Here you work with the script file ModelSimp.R"
  },
  {
    "objectID": "Intro_to_API.html",
    "href": "Intro_to_API.html",
    "title": "VectorByte Dataset Access Script",
    "section": "",
    "text": "This section will cover: Retrieving data from the VecTraits database with the API\nLoad some packages and run some functions …\n\nrequire(httr)\nrequire(jsonlite)\nrequire(tidyverse)\n\n\ngetWebData <- function(dataURL) {\n  if (!\"httr\" %in% installed.packages()) {\n    cat(\"Installing necessary httr library...\\n\")\n    install.packages(\"httr\")\n  }\n  if (!\"jsonlite\" %in% installed.packages()) {\n    cat(\"Installing necessary jsonlite library...\\n\")\n    install.packages(\"jsonlite\")\n  }\n  if (!exists(\"webDataLibrariesOpen\")) {\n    library(httr)\n    library(jsonlite)\n    webDataLibrariesOpen <- TRUE\n  }\n  webData <- GET(url = dataURL)\n  if (status_code(webData) >= 300 || status_code(webData) < 200) {\n    returnValue <- data.frame(\n      message = \"Data fetch failed.\",\n      HTTPcode = status_code(webData)\n    )\n    return(returnValue)\n  }\n  returnValue <- fromJSON(\n    content(webData, \"text\", encoding = \"UTF-8\"),\n    flatten = TRUE\n  )\n  return(returnValue)\n}\n\n\ngetDataset <- function(ID = -1) {\n  totalDatasets <<- as.integer(getWebData(\"https://vectorbyte-qa.crc.nd.edu/portal/api/vectraits-explorer/?format=json\")$data$count)\n  if (as.integer(ID) <= totalDatasets && as.integer(ID) > 0) {\n    datasetID <- ID\n  } else {\n    while (TRUE) {\n      Sys.sleep(0.2)\n      datasetID <- readline(prompt = \"Enter a dataset ID: \")\n      if (as.integer(datasetID) <= totalDatasets && as.integer(datasetID) > 0) {\n        break\n      } else {\n        cat(paste(\"The dataset ID\", datasetID, \"is invalid or is out of range.\\n\"))\n        cat(paste(\"Please choose a number between 1 and\", totalDatasets, \"\\b.\\n\"))\n      }\n    }\n  }\n  dataset <- getWebData(\n    paste(\n      c(\n        \"https://vectorbyte-qa.crc.nd.edu/portal/api/vectraits-dataset/\",\n        datasetID,\n        \"/?format=json\"\n      ),\n      collapse = \"\"\n    )\n  )\n  if (as.character(dataset)[1] == \"Data fetch failed.\") {\n    cat(\"Uh Oh!\\nAn HTTP Error Occured and dataset\", datasetID, \"could not be retrieved.\\n\")\n    cat(\"HTTP Error Code:\", dataset$HTTPcode, \"\\b\\n\\n\")\n  }\n  return(dataset)\n}\n\n\ngetDatasets <- function(IDS, safety = TRUE, l = 50L) {\n  Sys.sleep(0.2)\n  IDs <- IDS\n  if (length(IDs) > l) {\n    cat(\"You may not retrieve more than\", l, \"datasets at a time.\\n\")\n    cat(\"Would you like to retrieve only the first 50 datasets?\\n\")\n    answer <- tolower(readline())\n    if (grepl(\"y\", answer) && !grepl(\"n\", answer)) {\n      IDs <- IDs[1:50]\n    } else {\n      return()\n    }\n  }\n  if (safety && length(IDs) > 50) {\n    cat(\"Are you sure you want to retrieve all\", length(IDs), \"datasets?\\n\")\n    cat(paste(\"This would take about\", ceiling(length(IDs) / 50), \"min.\\n\"))\n    answer <- tolower(readline())\n  } else {\n    cat(paste(\"This will take about\", ceiling(60 * length(IDs) / 50), \"seconds.\\n\"))\n    answer <- \"y\"\n  }\n  if (grepl(\"y\", answer) && !grepl(\"n\", answer)) {\n    total <- length(IDs)\n    setNumber <- 0\n    failedDatasets <<- c()\n    datasets <- list()\n    cat(\"Retrieving datasets....\\n\")\n    for (datasetID in IDs) {\n      setNumber <- setNumber + 1\n      flush.console()\n      datasets[[setNumber]] <- getWebData(\n        paste(\n          c(\n            \"https://vectorbyte-qa.crc.nd.edu/portal/api/vectraits-dataset/\",\n            as.character(datasetID),\n            \"/?format=json\"\n          ),\n          collapse = \"\"\n        )\n      )\n      # Remove previously displayed percentage:\n      cat(paste(\n        rep(\"\\b\", nchar(\n          as.character(\n            (floor(10000 * (setNumber - 1) / total) / 100)\n          )\n        ) + 3),\n        collapse = \"\"\n      ))\n      # Extend loading bar:\n      if (as.character(datasets[[setNumber]])[1] == \"Data fetch failed.\") {\n        cat(\"X> \")\n        failedDatasets <- c(failedDatasets, datasetID)\n      } else {\n        cat(\"=> \")\n      }\n      # Display new percentage:\n      cat(as.character(floor(10000 * setNumber / total) / 100), \"\\b%\")\n    }\n    cat(\"\\b\\b\\b\\b\\b\\b 100%\\nData retrieval complete!\\n\")\n    if (length(failedDatasets) > 0) {\n      cat(\"The following\", length(failedDatasets), \"datasets contained HTTP errors and could not be retrieved:\\n\")\n      print(failedDatasets)\n    }\n    return(datasets)\n  }\n}\n\n\nsearchDatasets <- function(KEYWORD = \"\", safety = TRUE) {\n  keyword <- KEYWORD\n  while (nchar(keyword) < 3) {\n    Sys.sleep(0.2)\n    keyword <- readline(prompt = \"Enter a keyword to search for in all datasets: \")\n    if (nchar(keyword) < 3) {\n      cat(\"Please enter a more descriptive keyword.\\n\")\n    }\n  }\n  cat(\"Searching Datasets....\\n\")\n  flush.console()\n  setSearch <- getWebData(\n    paste(\n      c(\n        \"https://vectorbyte.crc.nd.edu/portal/api/vectraits-explorer/?format=json&keywords=\",\n        gsub(\" \", \"%20\", keyword)\n      ),\n      collapse = \"\"\n    )\n  )\n  if (as.character(setSearch)[1] == \"Data fetch failed.\") {\n    cat(\"Uh Oh!\\nAn HTTP error has occurred:\", setSearch$HTTPcode, \"\\n\")\n    cat(\"This could be because the search term you entered was too general (too many results).\\n\")\n    cat(\"Please try again:\\n\")\n    searchDatasets()\n  } else {\n    cat(length(setSearch$ids), \"relevant datasets found.\\n\")\n    return(getDatasets(setSearch$ids, safety))\n  }\n}\n\n\nsearchDatasetsMulti <- function(KEYWORDS = c(), safety = TRUE) {\n  if (length(KEYWORDS) == 0) {\n    Sys.sleep(0.2)\n    cat(\"Please enter a list of keywords to search for in the datasets:\\n\")\n    keywords <- c()\n    while (length(keywords) == 0) {\n      keywords <- scan(what = \"\")\n    }\n  } else {\n    keywords <- KEYWORDS\n  }\n  cat(\"Searching Datasets....\\n\")\n  flush.console()\n  setSearch <- getWebData(\n    paste(\n      c(\n        \"https://vectorbyte.crc.nd.edu/portal/api/vectraits-explorer/?format=json&keywords=\",\n        gsub(\" \", \"%20\", paste(keywords, collapse = \"%20\"))\n      ),\n      collapse = \"\"\n    )\n  )\n  if (as.character(setSearch)[1] == \"Data fetch failed.\") {\n    cat(\"Uh Oh!\\nAn HTTP error has occurred:\", setSearch$HTTPcode, \"\\n\")\n    cat(\"This could be because the search term you entered had too many results.\\n\")\n    cat(\"This could also be because the search term you entered had no results.\\n\")\n    cat(\"Please try again:\\n\")\n    searchDatasetsMulti()\n  } else {\n    cat(length(setSearch$ids), \"relevant datasets found.\\n\")\n    return(getDatasets(setSearch$ids, safety))\n  }\n}\n\n\nsmartSearch <- function(VARIABLE_NAME, VARIABLE_VALUE, OPERATOR = \"contains\", safety = TRUE) {\n  operator <- tolower(OPERATOR)\n  if (operator != \"contains\") {\n    if (operator == \"contain\" || operator == \"has\") { operator <- \"contains\" }\n    if (operator == \"!contain\" || operator == \"!contains\" || operator == \"!has\" || operator == \"!have\" || operator == \"does not contain\") { operator <- \"ncontains\" }\n    if (operator == \"=\" || operator == \"==\" || operator == \"equal\" || operator == \"equals\") { operator <- \"eq\" }\n    if (operator == \"!=\" || operator == \"not\" || operator == \"!equal\" || operator == \"!equals\") { operator <- \"neq\" }\n    if (operator == \"starts with\" || operator == \"start with\" || operator == \"starts\" || operator == \"start\") { operator <- \"sw\" }\n    if (operator == \"not start with\" || operator == \"!start\" || operator == \"!starts\") { operator <- \"nsw\" }\n  }\n  variable_name <- VARIABLE_NAME\n  if (tolower(variable_name) == \"genus\") { variable_name <- \"Interactor1Genus\" }\n  if (tolower(variable_name) == \"species\") { variable_name <- \"Interactor1Species\" }\n  if (tolower(variable_name) == \"gender\") { variable_name <- \"Interactor1Sex\" }\n  if (tolower(variable_name) == \"who\") { variable_name <- \"SubmittedBy\" }\n  if (tolower(variable_name) == \"stage\") { variable_name <- \"Interactor1Stage\" }\n  cat(\"Searching Datasets....\\n\")\n  flush.console()\n  setSearch <- getWebData(\n    paste(\n      c(\n        \"https://vectorbyte.crc.nd.edu/portal/api/vectraits-explorer/?format=json&field=\",\n        gsub(\" \", \"%20\", variable_name),\n        \"&operator=\",\n        operator,\n        \"&term=\",\n        gsub(\" \", \"%20\", VARIABLE_VALUE)\n      ),\n      collapse = \"\"\n    )\n  )\n  if (as.character(setSearch)[1] == \"Data fetch failed.\") {\n    if (setSearch$HTTPcode == 400) {\n      cat(\"Uh Oh!\\nThe server does not wish to fulfill your request.\\n\")\n      cat(\"This could be because you included unsupported/reserved URL characters, such as:\\n\")\n      cat(\", ! @ # $ % ^ & : ; \\\\ \\\" ' ? / < > emojis etc.\\n\")\n      cat(\"This could also be because too many results matched your search.\\n\")\n      cat(\"(Don't search for \\\"Animalia\\\" in \\\"Interactor1Kingdom\\\", for example.)\\n\")\n      cat(\"This could also be because you used the incorrect capitalization in the variable name entry.\\n\")\n      if (OPERATOR != \"contains\") {\n        cat(\"This could also very likely be because the operator you entered is not supported.\\n\")\n        cat(\"The following operators are supported by the server:\\n\")\n        cat(\"contains, ncontains, eq (equals), neq (not equal to), sw (starts with),\\nnsw (doesn't start with), in, nin\\n\")\n      }\n    } else {\n      cat(\"Uh Oh!\\nAn HTTP error has occurred:\", setSearch$HTTPcode, \"\\n\")\n      if (setSearch$HTTPcode == 404) {\n        cat(\"The most likely reason for this is that no results matched your search.\")\n      }\n    }\n  } else {\n    cat(length(setSearch$ids), \"relevant datasets found.\\n\")\n    return(getDatasets(setSearch$ids, safety))\n  }\n}\n\n\nsciName <- function(DATASET) {\n  return(paste(\n    DATASET$results$Interactor1Genus[1],\n    DATASET$results$Interactor1Species[1]\n  ))\n}\n\n\ndatasetRows <- function(DATASET) {\n  return(length(DATASET$results$DatasetID))\n}\n\n\ndatasetColumns <- function(DATASET) {\n  return(length(DATASET$results[1,]))\n}\n\n\ndatasetSummary <- function(DATASET) {\n  cat(paste(\n    toupper(DATASET$results$OriginalTraitDef[1]),\n    \"in\",\n    toupper(DATASET$results$OriginalTraitUnit[1]),\n    \"\\b:\\n\"\n  ))\n  print(summary(DATASET$results$OriginalTraitValue))\n  if (!is.na(DATASET$results$Interactor1Common[1])) {\n    cat(\"Experiments done on\", DATASET$results$Interactor1Common[1])\n    if (is.na(DATASET$results$Interactor2Common[1])) {\n      cat(\"s.\\n\")\n    } else {\n      cat(paste(\n        c(\n          \"s and \",\n          DATASET$results$Interactor2Common[1],\n          \"s.\\n\"\n        ),\n        collapse = \"\"\n      ))\n    }\n  }\n  cat(datasetRows(DATASET), \"samples total.\\n\")\n}\n\n\npick <- function(SELECTION = 0) {\n  if (SELECTION == 0) {\n    cat(\"MENU:\\n [1] Retrieve dataset by ID\\n [2] Retrieve datasets by IDs\\n [3] Search for datasets by keyword\\n [4] Search for datasets by list of keywords\\n [5] Search for datasets by variable and value\\n\")\n    Sys.sleep(0.25)\n    answer <- as.integer(readline(prompt = \"Enter a number from the menu above to select it. \"))\n  } else {\n    answer <- SELECTION\n  }\n  Sys.sleep(0.25)\n  if (answer == 1) {\n    return(getDataset())\n  }\n  if (answer == 2) {\n    cat(\"Enter a list of dataset IDs below.\\n\")\n    datasetIDs <- scan(what = integer())\n    return(getDatasets(datasetIDs))\n  }\n  if (answer == 3) {\n    return(searchDatasets())\n  }\n  if (answer == 4) {\n    return(searchDatasetsMulti())\n  }\n  if (answer == 5) {\n    vname <- readline(prompt = \"Variable Name: \")\n    Sys.sleep(0.2)\n    vval <- readline(prompt = \"Variable Value: \")\n    return(smartSearch(vname, vval))\n  }\n}\n\n\n\n\ndataset50 <- getDataset(50) # Get dataset with ID 50\nhead(dataset50)\n\n$results\n     Id DatasetID IndividualID OriginalID OriginalTraitName\n1 81848        50                  PHX261    fecundity rate\n2 81849        50                  PHX261    fecundity rate\n3 81850        50                  PHX261    fecundity rate\n4 81851        50                  PHX261    fecundity rate\n5 81852        50                  PHX261    fecundity rate\n              OriginalTraitDef StandardisedTraitName StandardisedTraitDef\n1 mean eggs individual-1 day-1                    NA                   NA\n2 mean eggs individual-1 day-1                    NA                   NA\n3 mean eggs individual-1 day-1                    NA                   NA\n4 mean eggs individual-1 day-1                    NA                   NA\n5 mean eggs individual-1 day-1                    NA                   NA\n  OriginalTraitValue       OriginalTraitUnit OriginalErrorPos OriginalErrorNeg\n1                3.0 eggs individual-1 day-1             51.7             51.7\n2                6.3 eggs individual-1 day-1             33.4             33.4\n3                9.6 eggs individual-1 day-1             36.3             36.3\n4               12.5 eggs individual-1 day-1             45.7             45.7\n5                5.8 eggs individual-1 day-1             75.2             75.2\n        OriginalErrorUnit StandardisedTraitValue StandardisedTraitUnit\n1 coefficient of variance                     NA                    NA\n2 coefficient of variance                     NA                    NA\n3 coefficient of variance                     NA                    NA\n4 coefficient of variance                     NA                    NA\n5 coefficient of variance                     NA                    NA\n  StandardisedErrorPos StandardisedErrorNeg StandardisedErrorUnit Replicates\n1                   NA                   NA                    NA         NA\n2                   NA                   NA                    NA         NA\n3                   NA                   NA                    NA         NA\n4                   NA                   NA                    NA         NA\n5                   NA                   NA                    NA         NA\n      Habitat   LabField ArenaValue ArenaUnit ArenaValueSI ArenaUnitSI\n1 terrestrial laboratory         NA        NA           NA          NA\n2 terrestrial laboratory         NA        NA           NA          NA\n3 terrestrial laboratory         NA        NA           NA          NA\n4 terrestrial laboratory         NA        NA           NA          NA\n5 terrestrial laboratory         NA        NA           NA          NA\n  AmbientTemp AmbientTempMethod AmbientTempUnit AmbientLight AmbientLightUnit\n1          NA                NA              NA           NA               NA\n2          NA                NA              NA           NA               NA\n3          NA                NA              NA           NA               NA\n4          NA                NA              NA           NA               NA\n5          NA                NA              NA           NA               NA\n  SecondStressor SecondStressorDef SecondStressorValue SecondStressorUnit\n1             NA                NA                  NA                 NA\n2             NA                NA                  NA                 NA\n3             NA                NA                  NA                 NA\n4             NA                NA                  NA                 NA\n5             NA                NA                  NA                 NA\n  TimeStart TimeEnd TotalObsTimeValue TotalObsTimeUnit TotalObsTimeValueSI\n1        NA      NA                NA               NA                  NA\n2        NA      NA                NA               NA                  NA\n3        NA      NA                NA               NA                  NA\n4        NA      NA                NA               NA                  NA\n5        NA      NA                NA               NA                  NA\n  TotalObsTimeUnitSI TotalObsTimeNotes ResRepValue ResRepUnit ResRepValueSI\n1                 NA                NA          NA         NA            NA\n2                 NA                NA          NA         NA            NA\n3                 NA                NA          NA         NA            NA\n4                 NA                NA          NA         NA            NA\n5                 NA                NA          NA         NA            NA\n  ResRepUnitSI    LocationText LocationType OriginalLocationDate LocationDate\n1           NA Dryden New York           NA                   NA   1974-01-06\n2           NA Dryden New York           NA                   NA   1974-01-06\n3           NA Dryden New York           NA                   NA   1974-01-06\n4           NA Dryden New York           NA                   NA   1974-01-06\n5           NA Dryden New York           NA                   NA   1974-01-06\n  LocationDatePrecision CoordinateType Latitude Longitude         Interactor1\n1                     3        decimal 42.48917 -76.35972 Sepedon fuscipennis\n2                     3        decimal 42.48917 -76.35972 Sepedon fuscipennis\n3                     3        decimal 42.48917 -76.35972 Sepedon fuscipennis\n4                     3        decimal 42.48917 -76.35972 Sepedon fuscipennis\n5                     3        decimal 42.48917 -76.35972 Sepedon fuscipennis\n  Interactor1Common Interactor1Wholepart Interactor1WholePartType\n1         Marsh fly                   NA                       NA\n2         Marsh fly                   NA                       NA\n3         Marsh fly                   NA                       NA\n4         Marsh fly                   NA                       NA\n5         Marsh fly                   NA                       NA\n  Interactor1Number Interactor1Kingdom Interactor1Phylum Interactor1Class\n1                17           Animalia        Arthropoda          Insecta\n2                59           Animalia        Arthropoda          Insecta\n3                54           Animalia        Arthropoda          Insecta\n4                52           Animalia        Arthropoda          Insecta\n5                24           Animalia        Arthropoda          Insecta\n  Interactor1Order Interactor1Family Interactor1Genus Interactor1Species\n1          Diptera       Sciomyzidae          Sepedon        fuscipennis\n2          Diptera       Sciomyzidae          Sepedon        fuscipennis\n3          Diptera       Sciomyzidae          Sepedon        fuscipennis\n4          Diptera       Sciomyzidae          Sepedon        fuscipennis\n5          Diptera       Sciomyzidae          Sepedon        fuscipennis\n  Interactor1Stage Interactor1Sex Interactor1Temp Interactor1TempUnit\n1            adult         female              15             Celsius\n2            adult         female              21             Celsius\n3            adult         female              26             Celsius\n4            adult         female              30             Celsius\n5            adult         female              33             Celsius\n  Interactor1TempMethod Interactor1GrowthTemp Interactor1GrowthTempUnit\n1                    NA                    NA                        NA\n2                    NA                    NA                        NA\n3                    NA                    NA                        NA\n4                    NA                    NA                        NA\n5                    NA                    NA                        NA\n  Interactor1GrowthDur Interactor1GrowthdDurUnit Interactor1GrowthType\n1                   NA                        NA                    NA\n2                   NA                        NA                    NA\n3                   NA                        NA                    NA\n4                   NA                        NA                    NA\n5                   NA                        NA                    NA\n  Interactor1Acc Interactor1AccTemp Interactor1AccTempNotes Interactor1AccTime\n1             NA                 NA                      NA                 NA\n2             NA                 NA                      NA                 NA\n3             NA                 NA                      NA                 NA\n4             NA                 NA                      NA                 NA\n5             NA                 NA                      NA                 NA\n  Interactor1AccTimeNotes Interactor1AccTimeUnit Interactor1OrigTemp\n1                      NA                     NA                  NA\n2                      NA                     NA                  NA\n3                      NA                     NA                  NA\n4                      NA                     NA                  NA\n5                      NA                     NA                  NA\n  Interactor1OrigTempNotes Interactor1OrigTime Interactor1OrigTimeNotes\n1                       NA                  NA                       NA\n2                       NA                  NA                       NA\n3                       NA                  NA                       NA\n4                       NA                  NA                       NA\n5                       NA                  NA                       NA\n  Interactor1OrigTimeUnit Interactor1EquilibTimeValue\n1                      NA                          NA\n2                      NA                          NA\n3                      NA                          NA\n4                      NA                          NA\n5                      NA                          NA\n  Interactor1EquilibTimeUnit Interactor1Size Interactor1SizeUnit\n1                         NA              NA                  NA\n2                         NA              NA                  NA\n3                         NA              NA                  NA\n4                         NA              NA                  NA\n5                         NA              NA                  NA\n  Interactor1SizeType Interactor1SizeSI Interactor1SizeUnitSI\n1                  NA                NA                    NA\n2                  NA                NA                    NA\n3                  NA                NA                    NA\n4                  NA                NA                    NA\n5                  NA                NA                    NA\n  Interactor1DenValue Interactor1DenUnit Interactor1DenTypeSI\n1                  NA                 NA                   NA\n2                  NA                 NA                   NA\n3                  NA                 NA                   NA\n4                  NA                 NA                   NA\n5                  NA                 NA                   NA\n  Interactor1DenValueSI Interactor1DenUnitSI Interactor1MassValueSI\n1                    NA                   NA                     NA\n2                    NA                   NA                     NA\n3                    NA                   NA                     NA\n4                    NA                   NA                     NA\n5                    NA                   NA                     NA\n  Interactor1MassUnitSI Interactor2 Interactor2Common Interactor2Kingdom\n1                    NA   None None                NA                 NA\n2                    NA   None None                NA                 NA\n3                    NA   None None                NA                 NA\n4                    NA   None None                NA                 NA\n5                    NA   None None                NA                 NA\n  Interactor2Phylum Interactor2Class Interactor2Order Interactor2Family\n1                NA               NA               NA                NA\n2                NA               NA               NA                NA\n3                NA               NA               NA                NA\n4                NA               NA               NA                NA\n5                NA               NA               NA                NA\n  Interactor2Genus Interactor2Species Interactor2Stage Interactor2Sex\n1               NA                 NA               NA             NA\n2               NA                 NA               NA             NA\n3               NA                 NA               NA             NA\n4               NA                 NA               NA             NA\n5               NA                 NA               NA             NA\n  Interactor2Temp Interactor2TempUnit Interactor2TempMethod\n1              NA                  NA                    NA\n2              NA                  NA                    NA\n3              NA                  NA                    NA\n4              NA                  NA                    NA\n5              NA                  NA                    NA\n  Interactor2GrowthTemp Interactor2GrowthTempUnit Interactor2GrowthDur\n1                    NA                        NA                   NA\n2                    NA                        NA                   NA\n3                    NA                        NA                   NA\n4                    NA                        NA                   NA\n5                    NA                        NA                   NA\n  Interactor2GrowthDurUnit Interactor2GrowthType Interactor2Acc\n1                       NA                    NA             NA\n2                       NA                    NA             NA\n3                       NA                    NA             NA\n4                       NA                    NA             NA\n5                       NA                    NA             NA\n  Interactor2AccTemp Interactor2AccTempNotes Interactor2AccTime\n1                 NA                      NA                 NA\n2                 NA                      NA                 NA\n3                 NA                      NA                 NA\n4                 NA                      NA                 NA\n5                 NA                      NA                 NA\n  Interactor2AccTimeNotes Interactor2AccTimeUnit Interactor2OrigTemp\n1                      NA                     NA                  NA\n2                      NA                     NA                  NA\n3                      NA                     NA                  NA\n4                      NA                     NA                  NA\n5                      NA                     NA                  NA\n  Interactor2OrigTempNotes Interactor2OrigTime Interactor2OrigTimeNotes\n1                       NA                  NA                       NA\n2                       NA                  NA                       NA\n3                       NA                  NA                       NA\n4                       NA                  NA                       NA\n5                       NA                  NA                       NA\n  Interactor2OrigTimeUnit Interactor2EquilibTimeValue\n1                      NA                          NA\n2                      NA                          NA\n3                      NA                          NA\n4                      NA                          NA\n5                      NA                          NA\n  Interactor2EquilibTimeUnit Interactor2Size Interactor2SizeUnit\n1                         NA              NA                  NA\n2                         NA              NA                  NA\n3                         NA              NA                  NA\n4                         NA              NA                  NA\n5                         NA              NA                  NA\n  Interactor2SizeType Interactor2SizeSI Interactor2SizeUnitSI\n1                  NA                NA                    NA\n2                  NA                NA                    NA\n3                  NA                NA                    NA\n4                  NA                NA                    NA\n5                  NA                NA                    NA\n  Interactor2DenValue Interactor2DenUnit Interactor2DenTypeSI\n1                  NA                 NA                   NA\n2                  NA                 NA                   NA\n3                  NA                 NA                   NA\n4                  NA                 NA                   NA\n5                  NA                 NA                   NA\n  Interactor2DenValueSI Interactor2DenUnitSI Interactor2MassValueSI\n1                    NA                   NA                     NA\n2                    NA                   NA                     NA\n3                    NA                   NA                     NA\n4                    NA                   NA                     NA\n5                    NA                   NA                     NA\n  Interactor2MassUnitSI PhysicalProcess PhysicalProcess_1 PhysicalProcess_2\n1                    NA              NA                NA                NA\n2                    NA              NA                NA                NA\n3                    NA              NA                NA                NA\n4                    NA              NA                NA                NA\n5                    NA              NA                NA                NA\n  FigureTable\n1     table 1\n2     table 1\n3     table 1\n4     table 1\n5     table 1\n                                                                                                                                                                                     Citation\n1 Barnes 1976. Effect of temperature on development survival oviposition and diapause in laboratory populations of Sepedon fuscipennis (Diptera: Sciomyzidae). Environ. Entomol. 5: 1089-1098\n2 Barnes 1976. Effect of temperature on development survival oviposition and diapause in laboratory populations of Sepedon fuscipennis (Diptera: Sciomyzidae). Environ. Entomol. 5: 1089-1098\n3 Barnes 1976. Effect of temperature on development survival oviposition and diapause in laboratory populations of Sepedon fuscipennis (Diptera: Sciomyzidae). Environ. Entomol. 5: 1089-1098\n4 Barnes 1976. Effect of temperature on development survival oviposition and diapause in laboratory populations of Sepedon fuscipennis (Diptera: Sciomyzidae). Environ. Entomol. 5: 1089-1098\n5 Barnes 1976. Effect of temperature on development survival oviposition and diapause in laboratory populations of Sepedon fuscipennis (Diptera: Sciomyzidae). Environ. Entomol. 5: 1089-1098\n  CuratedByCitation CuratedByDOI                 DOI SubmittedBy\n1                NA           NA 10.1093/ee/5.6.1089 Paul Huxley\n2                NA           NA 10.1093/ee/5.6.1089 Paul Huxley\n3                NA           NA 10.1093/ee/5.6.1089 Paul Huxley\n4                NA           NA 10.1093/ee/5.6.1089 Paul Huxley\n5                NA           NA 10.1093/ee/5.6.1089 Paul Huxley\n  ContributorEmail Notes DefaultChartXaxis DefaultChartCategory\n1 phuxly@gmail.com    NA   Interactor1Temp         LocationText\n2 phuxly@gmail.com    NA   Interactor1Temp         LocationText\n3 phuxly@gmail.com    NA   Interactor1Temp         LocationText\n4 phuxly@gmail.com    NA   Interactor1Temp         LocationText\n5 phuxly@gmail.com    NA   Interactor1Temp         LocationText\n\n\n\nGet a list of datasets by their ID numbers:\n\nlist_of_dataframes <- getDatasets(c(1:10))\n\nThis will take about 12 seconds.\nRetrieving datasets....\n\b\b\b\b=> 10 \b%\b\b\b\b\b=> 20 \b%\b\b\b\b\b=> 30 \b%\b\b\b\b\b=> 40 \b%\b\b\b\b\b=> 50 \b%\b\b\b\b\b=> 60 \b%\b\b\b\b\b=> 70 \b%\b\b\b\b\b=> 80 \b%\b\b\b\b\b=> 90 \b%\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\n\n Use do.call and lapply to create a single dataframe containing all of the datasets you pulled in the previous step:\n\ndatasets1to10 <- do.call(rbind,lapply(list_of_dataframes, data.frame, stringsAsFactors=FALSE))\nhead(datasets1to10)\n\n  results.Id results.DatasetID results.IndividualID results.OriginalID\n1      81518                 1                                  PHX801\n2      81519                 1                                  PHX801\n3      81520                 1                                  PHX801\n4      81521                 1                                  PHX801\n5      81522                 1                                  PHX801\n6      81523                 2                                  PHX802\n  results.OriginalTraitName    results.OriginalTraitDef\n1          development time mean duration of life stage\n2          development time mean duration of life stage\n3          development time mean duration of life stage\n4          development time mean duration of life stage\n5          development time mean duration of life stage\n6                 fecundity     mean lifetime offspring\n  results.StandardisedTraitName results.StandardisedTraitDef\n1                            NA                           NA\n2                            NA                           NA\n3                            NA                           NA\n4                            NA                           NA\n5                            NA                           NA\n6                            NA                           NA\n  results.OriginalTraitValue results.OriginalTraitUnit results.OriginalErrorPos\n1                       21.3                      days                     0.31\n2                       12.2                      days                     0.23\n3                        8.3                      days                     0.16\n4                        6.7                      days                     0.16\n5                        6.5                      days                     0.26\n6                       51.5    offspring individual-1                     3.93\n  results.OriginalErrorNeg results.OriginalErrorUnit\n1                     0.31                        se\n2                     0.23                        se\n3                     0.16                        se\n4                     0.16                        se\n5                     0.26                        se\n6                     3.93                        se\n  results.StandardisedTraitValue results.StandardisedTraitUnit\n1                             NA                            NA\n2                             NA                            NA\n3                             NA                            NA\n4                             NA                            NA\n5                             NA                            NA\n6                             NA                            NA\n  results.StandardisedErrorPos results.StandardisedErrorNeg\n1                           NA                           NA\n2                           NA                           NA\n3                           NA                           NA\n4                           NA                           NA\n5                           NA                           NA\n6                           NA                           NA\n  results.StandardisedErrorUnit results.Replicates results.Habitat\n1                            NA                 NA     terrestrial\n2                            NA                 NA     terrestrial\n3                            NA                 NA     terrestrial\n4                            NA                 NA     terrestrial\n5                            NA                 NA     terrestrial\n6                            NA                 NA     terrestrial\n  results.LabField results.ArenaValue results.ArenaUnit results.ArenaValueSI\n1       laboratory                 NA                NA                   NA\n2       laboratory                 NA                NA                   NA\n3       laboratory                 NA                NA                   NA\n4       laboratory                 NA                NA                   NA\n5       laboratory                 NA                NA                   NA\n6       laboratory                 NA                NA                   NA\n  results.ArenaUnitSI results.AmbientTemp results.AmbientTempMethod\n1                  NA                  NA                        NA\n2                  NA                  NA                        NA\n3                  NA                  NA                        NA\n4                  NA                  NA                        NA\n5                  NA                  NA                        NA\n6                  NA                  NA                        NA\n  results.AmbientTempUnit results.AmbientLight results.AmbientLightUnit\n1                      NA                   NA                       NA\n2                      NA                   NA                       NA\n3                      NA                   NA                       NA\n4                      NA                   NA                       NA\n5                      NA                   NA                       NA\n6                      NA                   NA                       NA\n  results.SecondStressor results.SecondStressorDef results.SecondStressorValue\n1                     NA                        NA                          NA\n2                     NA                        NA                          NA\n3                     NA                        NA                          NA\n4                     NA                        NA                          NA\n5                     NA                        NA                          NA\n6                     NA                        NA                          NA\n  results.SecondStressorUnit results.TimeStart results.TimeEnd\n1                         NA                NA              NA\n2                         NA                NA              NA\n3                         NA                NA              NA\n4                         NA                NA              NA\n5                         NA                NA              NA\n6                         NA                NA              NA\n  results.TotalObsTimeValue results.TotalObsTimeUnit\n1                        NA                       NA\n2                        NA                       NA\n3                        NA                       NA\n4                        NA                       NA\n5                        NA                       NA\n6                        NA                       NA\n  results.TotalObsTimeValueSI results.TotalObsTimeUnitSI\n1                          NA                         NA\n2                          NA                         NA\n3                          NA                         NA\n4                          NA                         NA\n5                          NA                         NA\n6                          NA                         NA\n  results.TotalObsTimeNotes results.ResRepValue results.ResRepUnit\n1                        NA                  NA                 NA\n2                        NA                  NA                 NA\n3                        NA                  NA                 NA\n4                        NA                  NA                 NA\n5                        NA                  NA                 NA\n6                        NA                  NA                 NA\n  results.ResRepValueSI results.ResRepUnitSI        results.LocationText\n1                    NA                   NA Nonsan-si Republic of Korea\n2                    NA                   NA Nonsan-si Republic of Korea\n3                    NA                   NA Nonsan-si Republic of Korea\n4                    NA                   NA Nonsan-si Republic of Korea\n5                    NA                   NA Nonsan-si Republic of Korea\n6                    NA                   NA Nonsan-si Republic of Korea\n  results.LocationType results.OriginalLocationDate results.LocationDate\n1                field                           NA           2009-01-01\n2                field                           NA           2009-01-01\n3                field                           NA           2009-01-01\n4                field                           NA           2009-01-01\n5                field                           NA           2009-01-01\n6                field                           NA           2009-01-01\n  results.LocationDatePrecision results.CoordinateType results.Latitude\n1                             1                decimal             36.5\n2                             1                decimal             36.5\n3                             1                decimal             36.5\n4                             1                decimal             36.5\n5                             1                decimal             36.5\n6                             1                decimal             36.5\n  results.Longitude results.Interactor1 results.Interactor1Common\n1            126.75 Acyrthosiphon pisum                 Pea aphid\n2            126.75 Acyrthosiphon pisum                 Pea aphid\n3            126.75 Acyrthosiphon pisum                 Pea aphid\n4            126.75 Acyrthosiphon pisum                 Pea aphid\n5            126.75 Acyrthosiphon pisum                 Pea aphid\n6            126.75 Acyrthosiphon pisum                 Pea aphid\n  results.Interactor1Wholepart results.Interactor1WholePartType\n1                           NA                               NA\n2                           NA                               NA\n3                           NA                               NA\n4                           NA                               NA\n5                           NA                               NA\n6                           NA                               NA\n  results.Interactor1Number results.Interactor1Kingdom\n1                        39                   Animalia\n2                        53                   Animalia\n3                        45                   Animalia\n4                        42                   Animalia\n5                         8                   Animalia\n6                        50                   Animalia\n  results.Interactor1Phylum results.Interactor1Class results.Interactor1Order\n1                Arthropoda                  Insecta                Hemiptera\n2                Arthropoda                  Insecta                Hemiptera\n3                Arthropoda                  Insecta                Hemiptera\n4                Arthropoda                  Insecta                Hemiptera\n5                Arthropoda                  Insecta                Hemiptera\n6                Arthropoda                  Insecta                Hemiptera\n  results.Interactor1Family results.Interactor1Genus results.Interactor1Species\n1                 Aphididae            Acyrthosiphon                      pisum\n2                 Aphididae            Acyrthosiphon                      pisum\n3                 Aphididae            Acyrthosiphon                      pisum\n4                 Aphididae            Acyrthosiphon                      pisum\n5                 Aphididae            Acyrthosiphon                      pisum\n6                 Aphididae            Acyrthosiphon                      pisum\n  results.Interactor1Stage results.Interactor1Sex results.Interactor1Temp\n1                 juvenile                   <NA>                      10\n2                 juvenile                   <NA>                      15\n3                 juvenile                   <NA>                      20\n4                 juvenile                   <NA>                      25\n5                 juvenile                   <NA>                      30\n6                    adult                 female                      10\n  results.Interactor1TempUnit results.Interactor1TempMethod\n1                     Celsius                            NA\n2                     Celsius                            NA\n3                     Celsius                            NA\n4                     Celsius                            NA\n5                     Celsius                            NA\n6                     Celsius                            NA\n  results.Interactor1GrowthTemp results.Interactor1GrowthTempUnit\n1                            NA                              <NA>\n2                            NA                              <NA>\n3                            NA                              <NA>\n4                            NA                              <NA>\n5                            NA                              <NA>\n6                            NA                              <NA>\n  results.Interactor1GrowthDur results.Interactor1GrowthdDurUnit\n1                           NA                                NA\n2                           NA                                NA\n3                           NA                                NA\n4                           NA                                NA\n5                           NA                                NA\n6                           NA                                NA\n  results.Interactor1GrowthType results.Interactor1Acc\n1                            NA                     NA\n2                            NA                     NA\n3                            NA                     NA\n4                            NA                     NA\n5                            NA                     NA\n6                            NA                     NA\n  results.Interactor1AccTemp results.Interactor1AccTempNotes\n1                         NA                              NA\n2                         NA                              NA\n3                         NA                              NA\n4                         NA                              NA\n5                         NA                              NA\n6                         NA                              NA\n  results.Interactor1AccTime results.Interactor1AccTimeNotes\n1                         NA                              NA\n2                         NA                              NA\n3                         NA                              NA\n4                         NA                              NA\n5                         NA                              NA\n6                         NA                              NA\n  results.Interactor1AccTimeUnit results.Interactor1OrigTemp\n1                             NA                          NA\n2                             NA                          NA\n3                             NA                          NA\n4                             NA                          NA\n5                             NA                          NA\n6                             NA                          NA\n  results.Interactor1OrigTempNotes results.Interactor1OrigTime\n1                               NA                          NA\n2                               NA                          NA\n3                               NA                          NA\n4                               NA                          NA\n5                               NA                          NA\n6                               NA                          NA\n  results.Interactor1OrigTimeNotes results.Interactor1OrigTimeUnit\n1                               NA                              NA\n2                               NA                              NA\n3                               NA                              NA\n4                               NA                              NA\n5                               NA                              NA\n6                               NA                              NA\n  results.Interactor1EquilibTimeValue results.Interactor1EquilibTimeUnit\n1                                  NA                                 NA\n2                                  NA                                 NA\n3                                  NA                                 NA\n4                                  NA                                 NA\n5                                  NA                                 NA\n6                                  NA                                 NA\n  results.Interactor1Size results.Interactor1SizeUnit\n1                      NA                          NA\n2                      NA                          NA\n3                      NA                          NA\n4                      NA                          NA\n5                      NA                          NA\n6                      NA                          NA\n  results.Interactor1SizeType results.Interactor1SizeSI\n1                          NA                        NA\n2                          NA                        NA\n3                          NA                        NA\n4                          NA                        NA\n5                          NA                        NA\n6                          NA                        NA\n  results.Interactor1SizeUnitSI results.Interactor1DenValue\n1                            NA                          NA\n2                            NA                          NA\n3                            NA                          NA\n4                            NA                          NA\n5                            NA                          NA\n6                            NA                          NA\n  results.Interactor1DenUnit results.Interactor1DenTypeSI\n1                         NA                           NA\n2                         NA                           NA\n3                         NA                           NA\n4                         NA                           NA\n5                         NA                           NA\n6                         NA                           NA\n  results.Interactor1DenValueSI results.Interactor1DenUnitSI\n1                            NA                           NA\n2                            NA                           NA\n3                            NA                           NA\n4                            NA                           NA\n5                            NA                           NA\n6                            NA                           NA\n  results.Interactor1MassValueSI results.Interactor1MassUnitSI\n1                             NA                            NA\n2                             NA                            NA\n3                             NA                            NA\n4                             NA                            NA\n5                             NA                            NA\n6                             NA                            NA\n  results.Interactor2 results.Interactor2Common results.Interactor2Kingdom\n1           None None                        NA                         NA\n2           None None                        NA                         NA\n3           None None                        NA                         NA\n4           None None                        NA                         NA\n5           None None                        NA                         NA\n6           None None                        NA                         NA\n  results.Interactor2Phylum results.Interactor2Class results.Interactor2Order\n1                        NA                       NA                       NA\n2                        NA                       NA                       NA\n3                        NA                       NA                       NA\n4                        NA                       NA                       NA\n5                        NA                       NA                       NA\n6                        NA                       NA                       NA\n  results.Interactor2Family results.Interactor2Genus results.Interactor2Species\n1                        NA                       NA                         NA\n2                        NA                       NA                         NA\n3                        NA                       NA                         NA\n4                        NA                       NA                         NA\n5                        NA                       NA                         NA\n6                        NA                       NA                         NA\n  results.Interactor2Stage results.Interactor2Sex results.Interactor2Temp\n1                       NA                     NA                      NA\n2                       NA                     NA                      NA\n3                       NA                     NA                      NA\n4                       NA                     NA                      NA\n5                       NA                     NA                      NA\n6                       NA                     NA                      NA\n  results.Interactor2TempUnit results.Interactor2TempMethod\n1                          NA                            NA\n2                          NA                            NA\n3                          NA                            NA\n4                          NA                            NA\n5                          NA                            NA\n6                          NA                            NA\n  results.Interactor2GrowthTemp results.Interactor2GrowthTempUnit\n1                            NA                                NA\n2                            NA                                NA\n3                            NA                                NA\n4                            NA                                NA\n5                            NA                                NA\n6                            NA                                NA\n  results.Interactor2GrowthDur results.Interactor2GrowthDurUnit\n1                           NA                               NA\n2                           NA                               NA\n3                           NA                               NA\n4                           NA                               NA\n5                           NA                               NA\n6                           NA                               NA\n  results.Interactor2GrowthType results.Interactor2Acc\n1                            NA                     NA\n2                            NA                     NA\n3                            NA                     NA\n4                            NA                     NA\n5                            NA                     NA\n6                            NA                     NA\n  results.Interactor2AccTemp results.Interactor2AccTempNotes\n1                         NA                              NA\n2                         NA                              NA\n3                         NA                              NA\n4                         NA                              NA\n5                         NA                              NA\n6                         NA                              NA\n  results.Interactor2AccTime results.Interactor2AccTimeNotes\n1                         NA                              NA\n2                         NA                              NA\n3                         NA                              NA\n4                         NA                              NA\n5                         NA                              NA\n6                         NA                              NA\n  results.Interactor2AccTimeUnit results.Interactor2OrigTemp\n1                             NA                          NA\n2                             NA                          NA\n3                             NA                          NA\n4                             NA                          NA\n5                             NA                          NA\n6                             NA                          NA\n  results.Interactor2OrigTempNotes results.Interactor2OrigTime\n1                               NA                          NA\n2                               NA                          NA\n3                               NA                          NA\n4                               NA                          NA\n5                               NA                          NA\n6                               NA                          NA\n  results.Interactor2OrigTimeNotes results.Interactor2OrigTimeUnit\n1                               NA                              NA\n2                               NA                              NA\n3                               NA                              NA\n4                               NA                              NA\n5                               NA                              NA\n6                               NA                              NA\n  results.Interactor2EquilibTimeValue results.Interactor2EquilibTimeUnit\n1                                  NA                                 NA\n2                                  NA                                 NA\n3                                  NA                                 NA\n4                                  NA                                 NA\n5                                  NA                                 NA\n6                                  NA                                 NA\n  results.Interactor2Size results.Interactor2SizeUnit\n1                      NA                          NA\n2                      NA                          NA\n3                      NA                          NA\n4                      NA                          NA\n5                      NA                          NA\n6                      NA                          NA\n  results.Interactor2SizeType results.Interactor2SizeSI\n1                          NA                        NA\n2                          NA                        NA\n3                          NA                        NA\n4                          NA                        NA\n5                          NA                        NA\n6                          NA                        NA\n  results.Interactor2SizeUnitSI results.Interactor2DenValue\n1                            NA                          NA\n2                            NA                          NA\n3                            NA                          NA\n4                            NA                          NA\n5                            NA                          NA\n6                            NA                          NA\n  results.Interactor2DenUnit results.Interactor2DenTypeSI\n1                         NA                           NA\n2                         NA                           NA\n3                         NA                           NA\n4                         NA                           NA\n5                         NA                           NA\n6                         NA                           NA\n  results.Interactor2DenValueSI results.Interactor2DenUnitSI\n1                            NA                           NA\n2                            NA                           NA\n3                            NA                           NA\n4                            NA                           NA\n5                            NA                           NA\n6                            NA                           NA\n  results.Interactor2MassValueSI results.Interactor2MassUnitSI\n1                             NA                            NA\n2                             NA                            NA\n3                             NA                            NA\n4                             NA                            NA\n5                             NA                            NA\n6                             NA                            NA\n  results.PhysicalProcess results.PhysicalProcess_1 results.PhysicalProcess_2\n1                      NA                        NA                        NA\n2                      NA                        NA                        NA\n3                      NA                        NA                        NA\n4                      NA                        NA                        NA\n5                      NA                        NA                        NA\n6                      NA                        NA                        NA\n  results.FigureTable\n1             table 1\n2             table 1\n3             table 1\n4             table 1\n5             table 1\n6             table 1\n                                                                                                                               results.Citation\n1 Ahn et al. 2020. Thermal effects on the population parameters and growth of Acyrthosiphon pisum (Harris) (Hemiptera: Aphididae). Insects 11:8\n2 Ahn et al. 2020. Thermal effects on the population parameters and growth of Acyrthosiphon pisum (Harris) (Hemiptera: Aphididae). Insects 11:8\n3 Ahn et al. 2020. Thermal effects on the population parameters and growth of Acyrthosiphon pisum (Harris) (Hemiptera: Aphididae). Insects 11:8\n4 Ahn et al. 2020. Thermal effects on the population parameters and growth of Acyrthosiphon pisum (Harris) (Hemiptera: Aphididae). Insects 11:8\n5 Ahn et al. 2020. Thermal effects on the population parameters and growth of Acyrthosiphon pisum (Harris) (Hemiptera: Aphididae). Insects 11:8\n6 Ahn et al. 2020. Thermal effects on the population parameters and growth of Acyrthosiphon pisum (Harris) (Hemiptera: Aphididae). Insects 11:8\n  results.CuratedByCitation results.CuratedByDOI             results.DOI\n1                        NA                   NA 10.3390/insects11080481\n2                        NA                   NA 10.3390/insects11080481\n3                        NA                   NA 10.3390/insects11080481\n4                        NA                   NA 10.3390/insects11080481\n5                        NA                   NA 10.3390/insects11080481\n6                        NA                   NA 10.3390/insects11080481\n  results.SubmittedBy results.ContributorEmail              results.Notes\n1         Paul Huxley         phuxly@gmail.com reared on faba bean leaves\n2         Paul Huxley         phuxly@gmail.com reared on faba bean leaves\n3         Paul Huxley         phuxly@gmail.com reared on faba bean leaves\n4         Paul Huxley         phuxly@gmail.com reared on faba bean leaves\n5         Paul Huxley         phuxly@gmail.com reared on faba bean leaves\n6         Paul Huxley         phuxly@gmail.com reared on faba bean leaves\n  results.DefaultChartXaxis results.DefaultChartCategory\n1           Interactor1Temp                 LocationText\n2           Interactor1Temp                 LocationText\n3           Interactor1Temp                 LocationText\n4           Interactor1Temp                 LocationText\n5           Interactor1Temp                 LocationText\n6           Interactor1Temp                 LocationText\n\n\n\n\n\n\ndatasets1to10 <- datasets1to10 %>%\n  `colnames<-`(str_to_lower(colnames(.))) %>%\n  `colnames<-`(str_remove(colnames(.), \"(results)\")) %>%\n  `colnames<-`(str_remove(colnames(.), \".\"))"
  },
  {
    "objectID": "Intro_to_API.html#search-for-and-list-datasets-on-a-specific-genus",
    "href": "Intro_to_API.html#search-for-and-list-datasets-on-a-specific-genus",
    "title": "VectorByte Dataset Access Script",
    "section": "Search for and list datasets on a specific genus",
    "text": "Search for and list datasets on a specific genus\n\nlist_of_AedesDatasets <- searchDatasets(\"Aedes\")\n\nSearching Datasets....\n80 relevant datasets found.\nYou may not retrieve more than 50 datasets at a time.\nWould you like to retrieve only the first 50 datasets?\n\n\n This will prompt you with something like the following:\n “Searching Datasets…. 80 relevant datasets found. You may not retrieve more than 50 datasets at a time. Would you like to retrieve only the first 50 datasets?” \nTo which you can respond with “Y” and run the following code to …\n\nPrint a list of datasets on Aedes mosquitoes\n\nfor (i in 1:length(list_of_AedesDatasets)) {\n  print(list_of_AedesDatasets[[i]]$results$DatasetID[1])\n}\n\n You can then use the code above to pull all of the data that’s currently in VecTraits on Aedes\n\n\nOther options\n\nRetrieve Multiple Datasets by Their IDs\nFormat:\nlist_of_dataframes <- getDatasets(<Vector, List, or Range of IDs>)\nExamples:\n\nlist_of_dataframes <- getDatasets(c(10, 20, 35)) # Get datasets 10, 20, and 35\n\nThis will take about 4 seconds.\nRetrieving datasets....\n\b\b\b\b=> 33.33 \b%\b\b\b\b\b\b\b\b=> 66.66 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- getDatasets(seq(200, 240, 10)) # Get datasets 200, 210, 220, 230, and 240\n\nThis will take about 6 seconds.\nRetrieving datasets....\n\b\b\b\b=> 20 \b%\b\b\b\b\b=> 40 \b%\b\b\b\b\b=> 60 \b%\b\b\b\b\b=> 80 \b%\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- getDatasets(list(50, 1, 580, 20)) # Get datasets 1, 20, 50, and 580\n\nThis will take about 5 seconds.\nRetrieving datasets....\n\b\b\b\b=> 25 \b%\b\b\b\b\b=> 50 \b%\b\b\b\b\b=> 75 \b%\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- getDatasets(320:324) # Get datasets 320, 321, 322, 323, and 324\n\nThis will take about 6 seconds.\nRetrieving datasets....\n\b\b\b\b=> 20 \b%\b\b\b\b\b=> 40 \b%\b\b\b\b\b=> 60 \b%\b\b\b\b\b=> 80 \b%\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\n\n\n\n\nRetrieve List of Datasets Related To Keyword\nFormat: list_of_dataframes <- searchDatasets(<Keyword>)\nExamples:\n\nlist_of_dataframes <- searchDatasets(\"tiger mosquito\") # Get datasets related to tiger mosquitos\n\nSearching Datasets....\n17 relevant datasets found.\nThis will take about 21 seconds.\nRetrieving datasets....\n\b\b\b\b=> 5.88 \b%\b\b\b\b\b\b\b=> 11.76 \b%\b\b\b\b\b\b\b\b=> 17.64 \b%\b\b\b\b\b\b\b\b=> 23.52 \b%\b\b\b\b\b\b\b\b=> 29.41 \b%\b\b\b\b\b\b\b\b=> 35.29 \b%\b\b\b\b\b\b\b\b=> 41.17 \b%\b\b\b\b\b\b\b\b=> 47.05 \b%\b\b\b\b\b\b\b\b=> 52.94 \b%\b\b\b\b\b\b\b\b=> 58.82 \b%\b\b\b\b\b\b\b\b=> 64.7 \b%\b\b\b\b\b\b\b=> 70.58 \b%\b\b\b\b\b\b\b\b=> 76.47 \b%\b\b\b\b\b\b\b\b=> 82.35 \b%\b\b\b\b\b\b\b\b=> 88.23 \b%\b\b\b\b\b\b\b\b=> 94.11 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\n\n\n\nRetrieve List of Datasets Related To Multiple Keywords\nFormat: list_of_dataframes <- searchDatasetsMulti(<Vector or List of Keywords>)\nExamples:\n(All the following retrieve datasets related to yellow fever & terrestrial areas in North Carolina)\n\nlist_of_dataframes <- searchDatasetsMulti(c(\"terrestrial\", \"north carolina\", \"yellow fever\"))\n\nSearching Datasets....\n3 relevant datasets found.\nThis will take about 4 seconds.\nRetrieving datasets....\n\b\b\b\b=> 33.33 \b%\b\b\b\b\b\b\b\b=> 66.66 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- searchDatasetsMulti(list(\"terrestrial\", \"north carolina\", \"yellow fever\"))\n\nSearching Datasets....\n3 relevant datasets found.\nThis will take about 4 seconds.\nRetrieving datasets....\n\b\b\b\b=> 33.33 \b%\b\b\b\b\b\b\b\b=> 66.66 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- searchDatasetsMulti(c(\"TERRESTRIAL\", \"NORTH CAROLINA\", \"YELLOW FEVER\"))\n\nSearching Datasets....\n3 relevant datasets found.\nThis will take about 4 seconds.\nRetrieving datasets....\n\b\b\b\b=> 33.33 \b%\b\b\b\b\b\b\b\b=> 66.66 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- searchDatasetsMulti(list(\"tERRestRiAL\", \"NoRTh CARoLina\", \"YELLow feVer\"))\n\nSearching Datasets....\n3 relevant datasets found.\nThis will take about 4 seconds.\nRetrieving datasets....\n\b\b\b\b=> 33.33 \b%\b\b\b\b\b\b\b\b=> 66.66 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\n\n\n\nRetrieve List of Datasets By Column Values\nFormat:\nlist_of_dataframes <- smartSearch(<Column Name>, <Column Value>, <Optional Operator>)\nExamples:\n\nlist_of_dataframes <- smartSearch(\"Interactor1Genus\", \"culex\") # Datasets with genus Culex\n\nSearching Datasets....\n17 relevant datasets found.\nThis will take about 21 seconds.\nRetrieving datasets....\n\b\b\b\b=> 5.88 \b%\b\b\b\b\b\b\b=> 11.76 \b%\b\b\b\b\b\b\b\b=> 17.64 \b%\b\b\b\b\b\b\b\b=> 23.52 \b%\b\b\b\b\b\b\b\b=> 29.41 \b%\b\b\b\b\b\b\b\b=> 35.29 \b%\b\b\b\b\b\b\b\b=> 41.17 \b%\b\b\b\b\b\b\b\b=> 47.05 \b%\b\b\b\b\b\b\b\b=> 52.94 \b%\b\b\b\b\b\b\b\b=> 58.82 \b%\b\b\b\b\b\b\b\b=> 64.7 \b%\b\b\b\b\b\b\b=> 70.58 \b%\b\b\b\b\b\b\b\b=> 76.47 \b%\b\b\b\b\b\b\b\b=> 82.35 \b%\b\b\b\b\b\b\b\b=> 88.23 \b%\b\b\b\b\b\b\b\b=> 94.11 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- smartSearch(\"SubmittedBy\", \"Lachance\") # Datasets submitted by Lachance\n\nSearching Datasets....\n22 relevant datasets found.\nThis will take about 27 seconds.\nRetrieving datasets....\n\b\b\b\b=> 4.54 \b%\b\b\b\b\b\b\b=> 9.09 \b%\b\b\b\b\b\b\b=> 13.63 \b%\b\b\b\b\b\b\b\b=> 18.18 \b%\b\b\b\b\b\b\b\b=> 22.72 \b%\b\b\b\b\b\b\b\b=> 27.27 \b%\b\b\b\b\b\b\b\b=> 31.81 \b%\b\b\b\b\b\b\b\b=> 36.36 \b%\b\b\b\b\b\b\b\b=> 40.9 \b%\b\b\b\b\b\b\b=> 45.45 \b%\b\b\b\b\b\b\b\b=> 50 \b%\b\b\b\b\b=> 54.54 \b%\b\b\b\b\b\b\b\b=> 59.09 \b%\b\b\b\b\b\b\b\b=> 63.63 \b%\b\b\b\b\b\b\b\b=> 68.18 \b%\b\b\b\b\b\b\b\b=> 72.72 \b%\b\b\b\b\b\b\b\b=> 77.27 \b%\b\b\b\b\b\b\b\b=> 81.81 \b%\b\b\b\b\b\b\b\b=> 86.36 \b%\b\b\b\b\b\b\b\b=> 90.9 \b%\b\b\b\b\b\b\b=> 95.45 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nlist_of_dataframes <- smartSearch(\"Interactor2Species\", \"null\", \"neq\") # Datasets with second interactor\n\nSearching Datasets....\n53 relevant datasets found.\nYou may not retrieve more than 50 datasets at a time.\nWould you like to retrieve only the first 50 datasets?\n\nlist_of_dataframes <- smartSearch(\"OriginalTraitName\", \"survival\", \"eq\") # Datasets strictly about survival\n\nSearching Datasets....\n75 relevant datasets found.\nYou may not retrieve more than 50 datasets at a time.\nWould you like to retrieve only the first 50 datasets?\n\n\n\n\nThe pick() Function\nIf all the information above was too much and there’s no way you’re going to remember it, all you need to know is the pick() function. The pick() function displays a small menu and allows you to choose an option in order for you to find and retrieve whatever dataset(s) you may be looking for.\nFormat:\nx <- pick()\n\n\nAccess Dataframes In Lists\nWhen multiple datasets are retrieved and stored in a list of dataframes, you can access individual datasets with the following format:\nfirst_dataframe <- list_of_dataframes[[1]]\nsecond_dataframe <- list_of_dataframes[[2]]\n…and so on for every dataframe in the list.\nExample:\nTo print a list of the names of those who contributed to the datasets related to tiger mosquitos:\n\nlist_of_dataframes <- searchDatasets(\"tiger mosquito\")\n\nSearching Datasets....\n17 relevant datasets found.\nThis will take about 21 seconds.\nRetrieving datasets....\n\b\b\b\b=> 5.88 \b%\b\b\b\b\b\b\b=> 11.76 \b%\b\b\b\b\b\b\b\b=> 17.64 \b%\b\b\b\b\b\b\b\b=> 23.52 \b%\b\b\b\b\b\b\b\b=> 29.41 \b%\b\b\b\b\b\b\b\b=> 35.29 \b%\b\b\b\b\b\b\b\b=> 41.17 \b%\b\b\b\b\b\b\b\b=> 47.05 \b%\b\b\b\b\b\b\b\b=> 52.94 \b%\b\b\b\b\b\b\b\b=> 58.82 \b%\b\b\b\b\b\b\b\b=> 64.7 \b%\b\b\b\b\b\b\b=> 70.58 \b%\b\b\b\b\b\b\b\b=> 76.47 \b%\b\b\b\b\b\b\b\b=> 82.35 \b%\b\b\b\b\b\b\b\b=> 88.23 \b%\b\b\b\b\b\b\b\b=> 94.11 \b%\b\b\b\b\b\b\b\b=> 100 \b%\b\b\b\b\b\b 100%\nData retrieval complete!\n\nfor (i in 1:length(list_of_dataframes)) {\n  print(list_of_dataframes[[i]]$results$SubmittedBy[1])\n}\n\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Lauren Chapman\"\n[1] \"Paul Huxley\"\n[1] \"Paul Huxley\"\n[1] \"Lauren Chapman\"\n[1] \"Lauren Chapman\"\n[1] \"Lauren Chapman\"\n\n\n\n\nAccess Data In Dataframes\nTo access variable x in row y of a dataset stored as a dataframe, use the $ symbol and square brackets []:\nvariable_value <- dataframe$results$x[y]\nFor example, if you had a dataset stored as a dataframe and you wanted to know the Genus of Interactor 1 in the fifth row, you could get that with the following code:\ndataframe$results$Interactor1Genus[5]\nThe same format also works for dataframes in lists. For example, you could run the following to get the original trait value in row 3 of the fourth dataset in the list:\nlist_of_dataframes[[4]]$results$OriginalTraitValue[3]\nThe same format also works on raw functions, although it is recommended that retrieved datasets are immediately stored in an R object:\ngetDataset(5)$results$SubmittedBy[1] # Name of person who submitted the dataset with ID 5\n\n\nMinor Additional Functions\ndatasetSummary(DATASET)\nSummary: Prints summary of data within dataset Parameters: DATASET = A dataset as a dataframe, such as getDataset(1) Returns: Nothing\nsciName(DATASET)\nSummary: Finds the scientific name of the main interactor within the dataset Parameters: DATASET = A dataset as a dataframe, such as getDataset(1) Returns: Scientific Name of main interactor within dataset, as string\ndatasetRows(DATASET)\nSummary: Finds the number of rows/samples in a dataset Parameters: DATASET = A dataset as a dataframe, such as getDataset(1) Returns: Number of rows, as integer\ndatasetColumns(DATASET)\nSummary: Finds the number of columns/variables in a dataset Parameters: DATASET = A dataset as a dataframe, such as getDataset(1) Returns: Number of columns, as integer"
  },
  {
    "objectID": "Get_API_Data.html",
    "href": "Get_API_Data.html",
    "title": "",
    "section": "",
    "text": "VectorByte Dataset Access Script\n\n\nUsage Instructions at: https://docs.google.com/document/d/1e0WLCyGJZfdzbLneyqfm4yKiTBzBugCvoA4Lu5MnIec/preview\ngetWebData <- function(dataURL) { if (!“httr” %in% installed.packages()) { cat(“Installing necessary httr library…”) install.packages(“httr”) } if (!“jsonlite” %in% installed.packages()) { cat(“Installing necessary jsonlite library…”) install.packages(“jsonlite”) } if (!exists(“webDataLibrariesOpen”)) { library(httr) library(jsonlite) webDataLibrariesOpen <- TRUE } webData <- GET(url = dataURL) if (status_code(webData) >= 300 || status_code(webData) < 200) { returnValue <- data.frame( message = “Data fetch failed.”, HTTPcode = status_code(webData) ) return(returnValue) } returnValue <- fromJSON( content(webData, “text”, encoding = “UTF-8”), flatten = TRUE ) return(returnValue) }\ngetDataset <- function(ID = -1) { totalDatasets <<- as.integer(getWebData(“https://vectorbyte-qa.crc.nd.edu/portal/api/vectraits-explorer/?format=json”)\\(data\\)count) if (as.integer(ID) <= totalDatasets && as.integer(ID) > 0) { datasetID <- ID } else { while (TRUE) { Sys.sleep(0.2) datasetID <- readline(prompt = “Enter a dataset ID:”) if (as.integer(datasetID) <= totalDatasets && as.integer(datasetID) > 0) { break } else { cat(paste(“The dataset ID”, datasetID, “is invalid or is out of range.”)) cat(paste(“Please choose a number between 1 and”, totalDatasets, “”)) } } } dataset <- getWebData( paste( c( “https://vectorbyte-qa.crc.nd.edu/portal/api/vectraits-dataset/”, datasetID, “/?format=json” ), collapse = “” ) ) if (as.character(dataset)[1] == “Data fetch failed.”) { cat(“Uh Oh!HTTP Error Occured and dataset”, datasetID, “could not be retrieved.”) cat(“HTTP Error Code:”, dataset$HTTPcode, “”) } return(dataset) }\ngetDatasets <- function(IDS, safety = TRUE, l = 50L) { Sys.sleep(0.2) IDs <- IDS if (length(IDs) > l) { cat(“You may not retrieve more than”, l, “datasets at a time.”) cat(“Would you like to retrieve only the first 50 datasets?”) answer <- tolower(readline()) if (grepl(“y”, answer) && !grepl(“n”, answer)) { IDs <- IDs[1:50] } else { return() } } if (safety && length(IDs) > 50) { cat(“Are you sure you want to retrieve all”, length(IDs), “datasets?”) cat(paste(“This would take about”, ceiling(length(IDs) / 50), “min.”)) answer <- tolower(readline()) } else { cat(paste(“This will take about”, ceiling(60 * length(IDs) / 50), “seconds.”)) answer <- “y” } if (grepl(“y”, answer) && !grepl(“n”, answer)) { total <- length(IDs) setNumber <- 0 failedDatasets <<- c() datasets <- list() cat(“Retrieving datasets….”) for (datasetID in IDs) { setNumber <- setNumber + 1 flush.console() datasets[[setNumber]] <- getWebData( paste( c( “https://vectorbyte-qa.crc.nd.edu/portal/api/vectraits-dataset/”, as.character(datasetID), “/?format=json” ), collapse = “” ) ) # Remove previously displayed percentage: cat(paste( rep(“, nchar( as.character( (floor(10000 * (setNumber - 1) / total) / 100) ) ) + 3), collapse =”” )) # Extend loading bar: if (as.character(datasets[[setNumber]])[1] == “Data fetch failed.”) { cat(“X>”) failedDatasets <- c(failedDatasets, datasetID) } else { cat(“=>”) } # Display new percentage: cat(as.character(floor(10000 * setNumber / total) / 100), “%”) } cat(“00%retrieval complete!”) if (length(failedDatasets) > 0) { cat(“The following”, length(failedDatasets), “datasets contained HTTP errors and could not be retrieved:”) print(failedDatasets) } return(datasets) } }\nsearchDatasets <- function(KEYWORD = ““, safety = TRUE) { keyword <- KEYWORD while (nchar(keyword) < 3) { Sys.sleep(0.2) keyword <- readline(prompt =”Enter a keyword to search for in all datasets: “) if (nchar(keyword) < 3) { cat(”Please enter a more descriptive keyword.“) } } cat(”Searching Datasets….“) flush.console() setSearch <- getWebData( paste( c(”https://vectorbyte.crc.nd.edu/portal/api/vectraits-explorer/?format=json&keywords=“, gsub(” “,”%20”, keyword) ), collapse = “” ) ) if (as.character(setSearch)[1] == “Data fetch failed.”) { cat(“Uh Oh!HTTP error has occurred:”, setSearch\\(HTTPcode, \"\\n\")  cat(\"This could be because the search term you entered was too general (too many results).\\n\")  cat(\"Please try again:\\n\")  searchDatasets()  } else {  cat(length(setSearch\\)ids), “relevant datasets found.”) return(getDatasets(setSearch$ids, safety)) } }\nsearchDatasetsMulti <- function(KEYWORDS = c(), safety = TRUE) { if (length(KEYWORDS) == 0) { Sys.sleep(0.2) cat(“Please enter a list of keywords to search for in the datasets:”) keywords <- c() while (length(keywords) == 0) { keywords <- scan(what = ““) } } else { keywords <- KEYWORDS } cat(”Searching Datasets….“) flush.console() setSearch <- getWebData( paste( c(”https://vectorbyte.crc.nd.edu/portal/api/vectraits-explorer/?format=json&keywords=“, gsub(” “,”%20”, paste(keywords, collapse = “%20”)) ), collapse = “” ) ) if (as.character(setSearch)[1] == “Data fetch failed.”) { cat(“Uh Oh!HTTP error has occurred:”, setSearch\\(HTTPcode, \"\\n\")  cat(\"This could be because the search term you entered had too many results.\\n\")  cat(\"This could also be because the search term you entered had no results.\\n\")  cat(\"Please try again:\\n\")  searchDatasetsMulti()  } else {  cat(length(setSearch\\)ids), “relevant datasets found.”) return(getDatasets(setSearch$ids, safety)) } }\nsmartSearch <- function(VARIABLE_NAME, VARIABLE_VALUE, OPERATOR = “contains”, safety = TRUE) { operator <- tolower(OPERATOR) if (operator != “contains”) { if (operator == “contain” || operator == “has”) { operator <- “contains” } if (operator == “!contain” || operator == “!contains” || operator == “!has” || operator == “!have” || operator == “does not contain”) { operator <- “ncontains” } if (operator == “=” || operator == “==” || operator == “equal” || operator == “equals”) { operator <- “eq” } if (operator == “!=” || operator == “not” || operator == “!equal” || operator == “!equals”) { operator <- “neq” } if (operator == “starts with” || operator == “start with” || operator == “starts” || operator == “start”) { operator <- “sw” } if (operator == “not start with” || operator == “!start” || operator == “!starts”) { operator <- “nsw” } } variable_name <- VARIABLE_NAME if (tolower(variable_name) == “genus”) { variable_name <- “Interactor1Genus” } if (tolower(variable_name) == “species”) { variable_name <- “Interactor1Species” } if (tolower(variable_name) == “gender”) { variable_name <- “Interactor1Sex” } if (tolower(variable_name) == “who”) { variable_name <- “SubmittedBy” } if (tolower(variable_name) == “stage”) { variable_name <- “Interactor1Stage” } cat(“Searching Datasets….”) flush.console() setSearch <- getWebData( paste( c( “https://vectorbyte.crc.nd.edu/portal/api/vectraits-explorer/?format=json&field=”, gsub(” “,”%20”, variable_name), “&operator=”, operator, “&term=”, gsub(” “,”%20”, VARIABLE_VALUE) ), collapse = “” ) ) if (as.character(setSearch)[1] == “Data fetch failed.”) { if (setSearch$HTTPcode == 400) { cat(“Uh Oh!server does not wish to fulfill your request.”) cat(“This could be because you included unsupported/reserved URL characters, such as:”) cat(“, ! @ # $ % ^ & : ; \\ \" ’ ? / < > emojis etc.”) cat(“This could also be because too many results matched your search.”) cat(“(Don’t search for \"Animalia\" in \"Interactor1Kingdom\", for example.)”) cat(“This could also be because you used the incorrect capitalization in the variable name entry.”) if (OPERATOR != “contains”) { cat(“This could also very likely be because the operator you entered is not supported.”) cat(“The following operators are supported by the server:”) cat(“contains, ncontains, eq (equals), neq (not equal to), sw (starts with),(doesn’t start with), in, nin”) } } else { cat(“Uh Oh!HTTP error has occurred:”, setSearch\\(HTTPcode, \"\\n\")  if (setSearch\\)HTTPcode == 404) { cat(“The most likely reason for this is that no results matched your search.”) } } } else { cat(length(setSearch\\(ids), \"relevant datasets found.\\n\")  return(getDatasets(setSearch\\)ids, safety)) } }\nsciName <- function(DATASET) { return(paste( DATASET\\(results\\)Interactor1Genus[1], DATASET\\(results\\)Interactor1Species[1] )) }\ndatasetRows <- function(DATASET) { return(length(DATASET\\(results\\)DatasetID)) }\ndatasetColumns <- function(DATASET) { return(length(DATASET$results[1,])) }\ndatasetSummary <- function(DATASET) { cat(paste( toupper(DATASET\\(results\\)OriginalTraitDef[1]), “in”, toupper(DATASET\\(results\\)OriginalTraitUnit[1]), “” )) print(summary(DATASET\\(results\\)OriginalTraitValue)) if (!is.na(DATASET\\(results\\)Interactor1Common[1])) { cat(“Experiments done on”, DATASET\\(results\\)Interactor1Common[1]) if (is.na(DATASET\\(results\\)Interactor2Common[1])) { cat(“s.”) } else { cat(paste( c( “s and”, DATASET\\(results\\)Interactor2Common[1], “s.” ), collapse = “” )) } } cat(datasetRows(DATASET), “samples total.”) }\npick <- function(SELECTION = 0) { if (SELECTION == 0) { cat(“MENU:Retrieve dataset by IDRetrieve datasets by IDsSearch for datasets by keywordSearch for datasets by list of keywordsSearch for datasets by variable and value”) Sys.sleep(0.25) answer <- as.integer(readline(prompt = “Enter a number from the menu above to select it.”)) } else { answer <- SELECTION } Sys.sleep(0.25) if (answer == 1) { return(getDataset()) } if (answer == 2) { cat(“Enter a list of dataset IDs below.”) datasetIDs <- scan(what = integer()) return(getDatasets(datasetIDs)) } if (answer == 3) { return(searchDatasets()) } if (answer == 4) { return(searchDatasetsMulti()) } if (answer == 5) { vname <- readline(prompt = “Variable Name:”) Sys.sleep(0.2) vval <- readline(prompt = “Variable Value:”) return(smartSearch(vname, vval)) } }"
  },
  {
    "objectID": "schedule2023.html",
    "href": "schedule2023.html",
    "title": "2023 Training Schedule",
    "section": "",
    "text": "Main materials\n\nPre-workshop\nWe will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We are assuming familiarity with R basics. In addition, we recommend that you do the following:\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter up to the section on Writing R code. Of course, keep going if you want (although we will cover some similar materials here).\nReview background on introductory probability and statistics (solutions to exercises)\n\nOnce you have completed the pre-work and set-up tasks, you should get yourself familiar with the VectorByte Training materials.\nAlthough there will be input at the start of most sessions, much of the synchronous time is planned to be dedicated to helping you work through exercises and activities\n  \n\n\n17th July 2023\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nArrival\n\n\n\n\n  \n\n\n18th July 2023 (08:30 - 17:00)\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n08:30\nCourse overview\n\n\n\n09:00\nIntro to traits\nSlides; Paper\n\n\n10:30\nBreak\n\n\n\n11:00\nIntro to the VecTraits database\nVecTraits; API\n\n\n12:00\nLunch\n\n\n\n13:00\nTutorial: Data wrangling and visualizing data\nData wrangling\n\n\n15:00\nBreak\n\n\n\n15:30\nStatistical Analyses I: Linear models\nLinear models\n\n\n\n  \n\n\n19th July 2023 (08:30 - 17:00)\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n08:30\nQ & A\n\n\n\n09:00\nAllometric analysis using LMs\nLinear models\n\n\n10:30\nBreak\n\n\n\n11:00\nLinear vs. Nonlinear models\nPractical\n\n\n12:00\nLunch\n\n\n\n13:00\nIntro to Bayes\nLecture; Practical 1\n\n\n14:00\nBreak\n\n\n\n14:15\nBayesian computation and MCMC\nLecture; Practical 2\n\n\n15:15\nBreak\n\n\n\n15:30\nFitting TPCs with bayesTPC\nPractical; Data\n\n\n16:30\nChoose projects\n\n\n\n\n\n\n20th July 2023 (10:00 - 1:00)\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n8:30\nQ & A\n\n\n\n9:00\nWork on analysis\n\n\n\n10:30\nBreak\n\n\n\n11:00\nWork on analysis\n\n\n\n12:00\nLunch\n\n\n\n13:00\nContinue analysis/present project\n\n\n\n15:00\nBreak\n\n\n\n15:30\nPresentations\n\n\n\n16:30\nDiscussion & Wrap-up\n\n\n\n\n\n\n21st July 2023\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nTravel\n\n\n\n\n\n\nPost-workshop\nEnjoy using these new techniques and databases!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Welcome to the VectorByte 2023 training workshop!\n\nCheck out our about and schedule pages to continue."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the VectorByte Training Materials 2023",
    "section": "",
    "text": "As the VectorByte team has developed these materials, we’ve aimed to provide resources for both guided (during the workshop) and self-led learning. We assume basic familiarity with:\n\nThe R Programming Language\nBasic calculus (especially the mathematical idea of functions)\nBasic probability and statistics (e.g., what is a probability distribution, normal and binomial distributions, means, variances)\n\nWe’ve divided the materials into subject matter modules. Each module is designed to build on the previous one, and expects at least knowledge of all of the preceding modules in the sequence. Based on your experience, we encourage students to “choose your own adventure” and start from where you feel comfortable.\nEach module consists of four kinds of materials:\n\nslides with presentation of materials\nvideos with recorded lectures based on slides\nlabs/hands-on materials to allow you to practice material in a practical way\nsolutions to exercises, when necessary\n\nWe also include links to additional resources/materials/references.\nFor more information about the goals and approach of the VectorByte RCN are available at vectorbyte.org."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "VectorByte Training Materials 2023",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting.\nWe will also be using Slack for additional support during the training. Please have these installed in advance.\n\n\n\nWe are assuming familiarity with R basics. In addition, we recommend that you do the following:\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter up to the section on Writing R code. Of course, keep going if you want (although we will cover some similar materials here).\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. But there are many more resources online (e.g., this and this ) – pick something that suits your learning style.\nReview background on introductory probability and statistics (solutions to exercises)\nInculcate the coding Jedi inside of you - or the Sith - whatever works.\n\n \n\n\n\n\n\nThis component will be delivered live & synchronously. The VecTraits website can be found here. It might be an idea to explore this prior to the workshop.\n\nIntro to the VecTraits API\n \n\n\n\n\nLecture slides\nCator et al. 2020. The Role of Vector Trait Variation in Vector-Borne Disease Dynamics\n\n \n\n\n\n\nPractical\nDatasets:\n\nPoundhill Data\nPoundhill Meta Data\nHuxley et al Trait Data\nGenome Size\nWrangling Practical Data\n\n\n \n\n\n\n\nLecture Slides, Lecture Video 1, Lecture Video 2, Practical\nDatasets:\n\nGenome Size\nHuxley et al. Trait Data\nLinear Models Practical Data\n\n\n \n\n\n\n\nLecture Slides, Lecture Video, Practical\nDatasets:\n\ncsm7I Data\nAedes juvenile mortality data\nNLLS Practical Data\n\n\n \n\n\n\n\nLecture Slides, Practical 1\nDatasets:\n\nMidge data\n\n\n \n\n\n\n\nLecture Slides, Practical 2A\n\n \n\n\n\n\nPractical\nDatasets:\n\nAedes data"
  },
  {
    "objectID": "Stats_review_soln.html",
    "href": "Stats_review_soln.html",
    "title": "VectorByte Methods Training 2023",
    "section": "",
    "text": "Main materials\nBack to stats review\n\nQuestion 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\n\nAnswer: both of these are zero, because the die cannot take these values.\n    \n\n\nQuestion 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\nAnswer: The CDF total probability of having a value less than or equal to its argument. Thus F(3)= 1/2, F(7)=1, and F(1.5)=1/6\n    \n\n\nQuestion 3: For a normal distribution with mean 0, what is F(0)?\n\nAnswer: The normal distribution is symmetric around its mean, with half of its probability on each side. Thus, F(0)=1/2\n    \n\n\nQuestion 4: Summation Notation Practice\n\n\n\ni\n1\n2\n3\n4\n\n\n\n\nZ_i\n2.0\n-2.0\n3.0\n-3.0\n\n\n\n\nCompute \\sum_{i=1}^{4}{z_i} = 0 \nCompute \\sum_{i=1}^4{(z_i - \\bar{z})^2} = 26 \nWhat is the sample variance? Assume that the z_i are i.i.d.. Note that i.i.d.~stands for “independent and identically distributed”. \n\nSolution: \ns^2= \\frac{\\sum_{i=1}^N(Y_i - \\bar{Y})^2}{N-1} = \\frac{26}{3}\n= 8\\times \\frac{2}{3}\n \n\nFor a general set of N numbers, \\{X_1, X_2, \\dots, X_N \\} and \\{Y_1, Y_2, \\dots, Y_N \\} show that \n\\sum_{i=1}^N{(X_i - \\bar{X})(Y_i - \\bar{Y})} = \\sum_{i=1}^N{(X_i-\\bar{X})Y_i}\n\n\n Solution: First, we multiply through and distribute: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\sum_{i=1}^N(X_i-\\bar{X})\\bar{Y}\n Next note that \\bar{Y} (the mean of the Y_is) doesn’t depend on i so we can pull it out of the summation: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\bar{Y} \\sum_{i=1}^N(X_i-\\bar{X}).\n Finally, the last sum must be zero because \n\\sum_{i=1}^N(X_i-\\bar{X}) = \\sum_{i=1}^N X_i- \\sum_{i=1}^N \\bar{X} = N\\bar{X} - N\\bar{X}=0.\n Thus \\begin{align*}\n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) &= \\sum_{i=1}^N(X_i-\\bar{X})Y_i - \\bar{Y}\\times 0\\\\\n& = \\sum_{i=1}^N(X_i-\\bar{X})Y_i.\n\\end{align*}\n    \n\n\nQuestion 5: Properites of Expected Values\nUsing the definition of an expected value above and with X and Y having the same probability distribution, show that:\n\\begin{align*}\n\\text{E}[X+Y]  & = \\text{E}[X] + \\text{E}[Y]\\\\  \n& \\text{and} \\\\\n\\text{E}[cX]  & = c\\text{E}[X]. \\\\\n\\end{align*}\nGiven these, and the fact that \\mu=\\text{E}[X], show that:\n\\begin{align*}\n\\text{E}[(X-\\mu)^2]  = \\text{E}[X^2] - (\\text{E}[X])^2\n\\end{align*}\nThis gives a formula for calculating variances (since \\text{Var}(X)= \\text{E}[(X-\\mu)^2]).\nSolution: Assuming X and Y are both i.i.d. with distribution f(x). The expectation of X+Y is defined as \\begin{align*}\n\\text{E}[X+Y]  & =  \\int (X+Y) f(x)dx \\\\\n              & =  \\int (X f(x) +Y f(x))dx  \\\\\n              & =  \\int X f(x)dx  +\\int Y f(x)dx  \\\\\n               & = \\text{E}[X] + \\text{E}[Y]  \n\\end{align*} Similarly \\begin{align*}\n\\text{E}[cX]   & =  \\int cXf(x)dx \\\\\n              & =  c \\int Xf(x) dx  \\\\\n              & = c\\text{E}[X]. \\\\\n\\end{align*} Thus we can re-write: \\begin{align*}\n\\text{E}[(X-\\mu)^2]  & = \\text{E}[ X^2 - 2X\\mu + \\mu^2] \\\\\n                        & = \\text{E}[X^2] - 2\\mu\\text{E}[X] + \\mu^2 \\\\\n                        & = \\text{E}[X^2] -2\\mu^2 + \\mu^2 \\\\\n                        & = \\text{E}[X^2] - \\mu^2 \\\\\n& = \\text{E}[X^2] - (\\text{E}[X])^2.\n\\end{align*}\n   \n\n\nQuestion 6: Functions of Random Variables\nSuppose that \\mathrm{E}[X]=\\mathrm{E}[Y]=0, \\mathrm{var}(X)=\\mathrm{var}(Y)=1, and \\mathrm{corr}(X,Y)=0.5.\n\nCompute \\mathrm{E}[3X-2Y]; and\n\\mathrm{var}(3X-2Y).\nCompute \\mathrm{E}[X^2].\n\nSolution:\n\nUsing the properties of expectations, we can re-write this as: \\begin{align*}\n\\mathrm{E}[3X-2Y] & = \\mathrm{E}[3X] + \\mathrm{E}[-2Y]\\\\\n& = 3 \\mathrm{E}[X] -2 \\mathrm{E}[Y]\\\\\n& = 3 \\times 0 -2 \\times 0\\\\\n&=0\n\\end{align*}\n\n\nUsing the properties of variances, we can re-write this as: \\begin{align*}\n\\mathrm{var}(3X-2Y) & = 3^2\\text{Var}(X) + (-2)^2\\text{Var}(Y) + 2(3)(-2)\\text{Cov}(XY)\\\\\n& =  9 \\times 1 + 4 \\times 1 -12 \\text{Corr}(XY)\\sqrt{\\text{Var}(X)\\text{Var}(Y)}\\\\\n& = 9+4 -12 \\times 0.5\\times1\\\\\n&=7\n\\end{align*}\n\n\nRecalling from Question 5 that the variance is \\mathrm{var}(X) = \\text{E}[X^2] - (\\text{E}[X])^2, we can re-arrange to obtain: \\begin{align*}\n\\mathrm{E}[X^2] & = \\mathrm{var}(X) + (\\mathrm{E}[X])^2\\\\\n& = 1+(0)^2 \\\\\n& =1\n\\end{align*}\n\n\n\nThe Sampling Distribution\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,4) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\n\n\\displaystyle \\mathrm{Var}(\\bar{Y}) =\n\\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N Y_i\\right) =\n\\frac{N}{N^2}\\mathrm{Var}(Y) =\\frac{4}{N}.\nThis is the derivation for the variance of the sampling distribution.\n \n\nWhat is the expectation of the sample mean?\n\n\\displaystyle\\mathrm{E}[\\bar{Y}] = \\frac{N}{N}\\mathrm{E}(Y) = \\mu. This is the mean of the sampling distribution.\n\n\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\n\n\\displaystyle \\mathrm{Var}(Y) = 4, because this is a sample directly from the population distribution.\n \n\nWhat is the standard error of \\bar{Y}?\n\nHere, again, we are looking at the distribution of the sample mean, so we must consider the sampling distribution, and the standard error (aka the standard distribution) is just the square root of the variance from part i.\n\\displaystyle \\mathrm{se}(\\bar{Y}) = \\sqrt{\\mathrm{Var}(\\bar{Y})} =\\frac{2}{\\sqrt{N}}.\n\n\nHypothesis Testing and Confidence Intervals\n\n\n\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq ` r m`, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?\n\n  \nThis question is asking you think about the hypothesis that the mean of your distribution is equal to 12. I give you the distribution of the data themselves (i.e., that they’re normal). To test the hypothesis, you work with the sampling distribution (i.e., the distribution of the sample mean) which is: \\bar{Y}\\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\n\nIf we knew \\sigma, we could use as our test statistic z=\\displaystyle \\frac{\\bar{y} - 12}{\\sigma/\\sqrt{n}}. However, here we need to estimate \\sigma so we use z=\\displaystyle \\frac{\\bar{y} - 12}{s_y/\\sqrt{n}} where \\displaystyle s_{y} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\bar{y})^2}{n-1}}.\n\n\nIf the null is true, the z \\sim t_{n-1}(0,1). Since we estimate the mean frm the data, the degrees of freedom is n-1.\n\n\nAs n approaches infinity, t_{n-1}(0,1) \\rightarrow N(0,1).\n\n\nYou reject the null for \\{z: |z| > t_{n-1,\\alpha/2}\\}.\n\n\nThe p-value is 2\\Pr(Z_{n-1} >|z|). \n\n\nThe 95% CI is \\bar{Y} \\pm \\frac{s_{y}}{\\sqrt{n}} t_{n-1,\\alpha/2}.\n\nFor 19 out of 20 different samples, an interval constructed in this way will include the true value of the mean, \\mu. \n\nz = (11-12)/(1/3) = -3 and 2\\Pr(Z_{8} >|z|) = .017, so we do reject the null.  The 95% CI for \\mu is 11 \\pm \\frac{1}{3}2.3 = (10.23, 11.77)."
  },
  {
    "objectID": "Stats_review.html",
    "href": "Stats_review.html",
    "title": "VectorByte Methods Training 2023",
    "section": "",
    "text": "Main materials\nSolutions to exercises"
  },
  {
    "objectID": "Stats_review.html#some-probability-notation",
    "href": "Stats_review.html#some-probability-notation",
    "title": "VectorByte Methods Training 2023",
    "section": "Some probability notation",
    "text": "Some probability notation\nWe have a set, S of all possible events. Let \\text{Pr}(A) (or alternatively \\text{Prob}(A)) be the probability of event A. Then:\n\nA^c is the complement to A (all events that are not A).\nA \\cup B is the union of events A and B (“A or B”).\nA \\cap B is the intersection of events A and B (“A and B”).\n\\text{Pr}(A|B) is the conditional probability of A given that B occurs."
  },
  {
    "objectID": "Stats_review.html#axioms-of-probability",
    "href": "Stats_review.html#axioms-of-probability",
    "title": "VectorByte Methods Training 2023",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\nThese are the basic definitions that we use when we talk about probabilities. You’ve probably seen these before, but maybe not in mathematical notation. If the notation is new to you, I suggest that you use the notation above to translate these statements into words and confirm that you understand what they mean. I give you an example for the first statement.\n\n\\sum_{i \\in S} \\text{Pr}(A_i)=1, where 0 \\leq \\text{Pr}(A_i) \\leq 1 (the probabilities of all the events that can happen must sum to one, and all of the individual probabilities must be less than one)\n\\text{Pr}(A)=1-\\text{Pr}(A^c)\n\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) -\\text{Pr}(A \\cap B)\n\\text{Pr}(A \\cap B) = \\text{Pr}(A|B)\\text{Pr}(B)\nIf A and B are independent, then \\text{Pr}(A|B) = \\text{Pr}(A)"
  },
  {
    "objectID": "Stats_review.html#bayes-theorem",
    "href": "Stats_review.html#bayes-theorem",
    "title": "VectorByte Methods Training 2023",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nBayes Theorem allows us to related the conditional probabilities of two events A and B:\n\\begin{align*}\n\\text{Pr}(A|B) & = \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\\\\\n&\\\\\n& =  \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B|A)\\text{Pr}(A) + \\text{Pr}(B|A^c)\\text{Pr}(A^c)}\n\\end{align*}"
  },
  {
    "objectID": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training 2023",
    "section": "Discrete RVs and their Probability Distributions",
    "text": "Discrete RVs and their Probability Distributions\nMany things that we observe are naturally discrete. For instance, whole numbers of chairs or win/loss outcomes for games. Discrete probability distributions are used to describe these kinds of events.\nFor discrete RVs, the distribution of probabilities is described by the probability mass function (pmf), f_k such that:\n\\begin{align*}\nf_k  \\equiv \\text{Pr}(X & = k) \\\\\n\\text{where } 0\\leq f_k \\leq 1 & \\text{ and } \\sum_k f_k = 1\n\\end{align*}\nFor example, for a fair 6-sided die:\nf_k = 1/6 for k= \\{1,2,3,4,5,6\\}.\n\\star Question 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\nRelated to the pmf is the cumulative distribution function (cdf), F(x). F(x) \\equiv \\text{Pr}(X \\leq x)\nFor the 6-sided die F(x)= \\displaystyle\\sum_{k=1}^{x} f_k\nwhere x \\in 1\\dots 6.\n\\star Question 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\n\nVisualizing distributions of discrete RVs in R\nExample: Imagine a RV can take values 1 through 10, each with probability 0.1:\n \n\nvals<-seq(1,10, by=1)\npmf<-rep(0.1, 10)\ncdf<-pmf[1]\nfor(i in 2:10) cdf<-c(cdf, cdf[i-1]+pmf[i])\npar(mfrow=c(1,2), bty=\"n\")\nbarplot(height=pmf, names.arg=vals, ylim=c(0, 1), main=\"pmf\", col=\"blue\")\nbarplot(height=cdf, names.arg=vals, ylim=c(0, 1), main=\"cdf\", col=\"red\")"
  },
  {
    "objectID": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training 2023",
    "section": "Continuous RVs and their Probability Distributions",
    "text": "Continuous RVs and their Probability Distributions\nThings are just a little different for continuous RVs. Instead we use the probability density function (pdf) of the RV, and denote it by f(x). It still describes how relatively likely are alternative values of an RV – that is, if the pdf his higher around one value than around another, then the first is more likely to happen. However, the pdf does not return a probability, it is a function that describes the probability density.\nAn analogy:\nProbabilities are like weights of objects. The PMF tells you how much weight each possible value or outcome contributes to a whole. The PDF tells you how dense it is around a value. To calculate the weight of a real object, you need to also know the size of the area that you’re interested in and the density there The probability that your RV takes exactly any value is zero, just like the probability that any atom in a very thin wire is lined up at exactly that position is zero (and to the amount of mass at that location is zero). However, you can take a very thin slice around that location to see how much material is there.\nRelated to the pdf is the cumulative distribution function (cdf), F(x). \nF(x) \\equiv \\text{Pr}(X \\leq x)\n For a continuous distribution: \nF(x)= \\int_{-\\infty}^x f(x')dx'\n\n \n For a normal distribution with mean 0, what is F(0)?\n \n\nVisualizing distributions of continuous RVs in R\nExample: exponential RV, where f(x) = re^{-rx}:\n\n\nvals<-seq(0,10, length=1000)\nr<-0.5\npar(mfrow=c(1,2), bty=\"n\")\nplot(vals, dexp(vals, rate=r), main=\"pdf\", col=\"blue\", type=\"l\", lwd=3, ylab=\"\", xlab=\"\")\nplot(vals, pexp(vals, rate=r), main=\"cdf\", ylim=c(0,1), col=\"red\",\n     type=\"l\", lwd=3, ylab=\"\", xlab=\"\")"
  },
  {
    "objectID": "Stats_review.html#confidence-intervals",
    "href": "Stats_review.html#confidence-intervals",
    "title": "VectorByte Methods Training 2023",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nSuppose Z_{n-p} \\sim t_{n-p}(0,1). A centered interval is on this t distribution can be written as: \\text{Pr}(-t_{n-p,\\alpha/2} \\< Z\\_{n-p} \\< t_{n-p,\\alpha/2}) = 1-\\alpha. That is, between these values of the t distribution (1-\\alpha)\\times 100 percent of the probability is contained in that symmetric interval. We can visually indicate these location on a plot of the t distribution (here with df=5 and \\alpha=0.05):\n\nx<-seq(-4.5, 4.5, length=1000)\nalpha=0.05\n\n## draw a line showing the normal pdf on the histogram\nplot(x, dt(x, df=5), col=\"black\", lwd=2, type=\"l\", xlab=\"x\", ylab=\"\")\nabline(v=qt(alpha/2, df=5), col=3, lty=2, lwd=2)\nabline(v=qt(1-alpha/2, df=5), col=2, lty=2, lwd=2)\n\nlegend(\"topright\", \n       legend=c(\"t, df=5\", \"lower a/2\", \"upper a/2\"),\n       col=c(1,3,2), lwd=2, lty=c(1, 2,2))\n\n\n\n\n\n\n\n\nIn the R code here, {\\tt qt} is the Student-t “quantile function”. The function {\\tt qt(alpha, df)} returns a value z such that \\alpha = P(Z_{\\mathrm{df}} < z), i.e., t_{\\mathrm{df},\\alpha}.\nHow can we use this to determine the confidence interval for \\theta? Since \\theta \\sim t_{n-p}(\\mu, s^2), we can replace the Z_{n-p} in the interval above with the definition in terms of \\theta, \\mu and s and rearrange: \\begin{align*}\n1-\\alpha& = \\text{Pr}\\left(-t_{n-p,\\alpha/2} < \\frac{\\mu - \\bar{\\theta}}{s} <\nt_{n-p,\\alpha/2}\\right) \\\\\n&=\n\\text{Pr}(\\bar{\\theta}-t_{n-p,\\alpha/2}s < \\mu <\n\\bar{\\theta} + t_{n-p,\\alpha/2}s)\n\\end{align*}\nThus (1-\\alpha)*100% of the time, \\mu is within the confidence interval (written in two equivalent ways):\n\\bar{\\theta} \\pm t_{n-p,\\alpha/2} \\times s \\;\\;\\; \\Leftrightarrow \\;\\;\\; \\bar{\\theta}-t_{n-p,\\alpha/2} \\times s, \\bar{\\theta} + t_{n-p,\\alpha/2}\\times s\nWhy should we care about confidence intervals?\n\nThe confidence interval captures the amount of information in the data about the parameter.\nThe center of the interval tells you what your estimate is.\nThe length of the interval tells you how sure you are about your estimate."
  },
  {
    "objectID": "Stats_review.html#p-values",
    "href": "Stats_review.html#p-values",
    "title": "VectorByte Methods Training 2023",
    "section": "p-Values",
    "text": "p-Values\nWhat is a p-value? The American Statistical Association issued a statement where they defined it in the following way:\n“Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” (ASA Statement on Statistical Significance and P-Values.)\nMore formally, we formulate a p-value in terms of a null hypothesis/model and test whether or not our observed data are more extreme than we would expect under that specific null model. In your previous courses you’ve probably seen very specific null models, corresponding to, for instance the null hypothesis that the mean of your data is normally distributed with mean m (often m=0). We often denote the null model as H_0 and the alternative as H_a or H_1. For instance, for our example above with \\theta we might want to test the following:\nH_0: \\bar{\\theta}=0 \\;\\;\\; \\text{vs.} \\;\\;\\; H_a: \\bar{\\theta}\\neq 0\nTo perform the hypothesis test we would FIRST choose our rejection level, \\alpha. Although convention is to use \\alpha =0.05 corresponding to a 95% confidence region, one could choose based on how sure one needs to be for a particular application. Next we build our test statistic. There are two cases, first if we know \\sigma and second if we don’t.\nIf we knew the variance \\sigma^2, our test statistic would be Z=\\frac{\\bar{\\theta}-0}{\\sigma}, and we expect that this should have a standard normal distribution, i.e., Z\\sim\\mathcal{N}(0,1). If we don’t know \\sigma and instead estimate is as s (which is most of the time), our test statistic would be Z_{df}=\\frac{\\bar{\\theta}-0}{s} (i.e., it would have a t-distribution).\nWe calculate the value of the appropriate statistic (either Z or Z_{df}) for our data, and then we compare it to the values of the standard distribution (normal or t, respectively) corresponding to the \\alpha level that we chose, i.e., we see if the number that we got for our statistic is inside the horizontal lines that we drew on the standard distribution above. If it is, then the data are consistent with the null hypothesis and we cannot reject the null. If the statistic is outside the region the data are NOT consistent with the null, and instead we reject the null and use the alternative as our new working hypothesis.\nNotice that this process is focused on the null hypothesis. We cannot tell if the alternative hypothesis is true, or, really, if it’s actually better than the null. We can only say that the null is not consistent with our data (i.e., we can falsify the null) at a given level of certainty.\nAlso, the hypothesis testing process is the same as building a confidence interval, as above, and then seeing if the null hypothesis is within your confidence interval. If the null is outside of your confidence interval then you can reject your null at the level of certainty corresponding to the \\alpha that you used to build your CI. If the value for the null is within your CI, you cannot reject at that level."
  },
  {
    "objectID": "Stats_review.html#the-sampling-distribution-1",
    "href": "Stats_review.html#the-sampling-distribution-1",
    "title": "VectorByte Methods Training 2023",
    "section": "The Sampling Distribution",
    "text": "The Sampling Distribution\n\n\n\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,9) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\nWhat is the expectation of the sample mean?\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\nWhat is the standard error of \\bar{Y}?"
  },
  {
    "objectID": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "href": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "title": "VectorByte Methods Training 2023",
    "section": "Hypothesis Testing and Confidence Intervals",
    "text": "Hypothesis Testing and Confidence Intervals\n\n\n\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq 12, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?"
  },
  {
    "objectID": "VB_Bayes2.html#learning-objectives",
    "href": "VB_Bayes2.html#learning-objectives",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntroduce computation tools to perform inference for simple models in R (how to turn the Bayesian crank)\nAppreciate the need for sensitivity analysis, model checking and comparison, and the potential dangers of Bayesian methods."
  },
  {
    "objectID": "VB_Bayes2.html#what-if-we-cant-calculate-an-analytic-posterior",
    "href": "VB_Bayes2.html#what-if-we-cant-calculate-an-analytic-posterior",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "What if we can’t calculate an analytic posterior?",
    "text": "What if we can’t calculate an analytic posterior?\nIf we go back to the full Bayes theorem: \\[\n\\text{Pr}(\\theta|Y) = \\frac{\\mathcal{L}(\\theta; Y)f(\\theta)}{\\text{Pr}(Y)}\n\\] We are usually specifying the likelihood and the prior but we often don’t know the normalizing constant in the denominator. Without this, the probabilities don’t properly integrate to 1 and we can’t make probability statements.\nWe can use Monte Carlo methods to approximate the posterior."
  },
  {
    "objectID": "VB_Bayes2.html#stochastic-simulation-monte-carlo",
    "href": "VB_Bayes2.html#stochastic-simulation-monte-carlo",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Stochastic Simulation & Monte Carlo",
    "text": "Stochastic Simulation & Monte Carlo\nStochastic simulation is a way to understand variability in a system and for calculating quantities that may be difficult or impossible to obtain directly.\n\nMonte Carlo (MC) methods are “a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results.” - Wikipedia"
  },
  {
    "objectID": "VB_Bayes2.html#mc-for-bayesian-statistics",
    "href": "VB_Bayes2.html#mc-for-bayesian-statistics",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "MC for Bayesian Statistics",
    "text": "MC for Bayesian Statistics\nWe use Monte Carlo (MC) methods to generate random deviates in the right ratios from the target posterior called draws or samples.\n\nWe use these draws to approximate/summarize our distribution and make inference statements (point estimates, CIs, etc). We can also use the draws to calculate the posterior distribution of any function of our estimated parameters.\n\nAs the number of draws/samples gets large we can approximate these quantities arbitrarily high precision."
  },
  {
    "objectID": "VB_Bayes2.html#the-plug-in-principle",
    "href": "VB_Bayes2.html#the-plug-in-principle",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "The “plug-in principle”",
    "text": "The “plug-in principle”\nUsing MC to perform these calculations (and to propagate the uncertainty) rests on the idea of the plug-in principle:\n\nA summary statistic or other feature of a distribution (e.g. expected value) can be approximated by the same summary/feature of an empirical sample from that distribution (e.g., sample mean)."
  },
  {
    "objectID": "VB_Bayes2.html#markov-chain-mc-mcmc",
    "href": "VB_Bayes2.html#markov-chain-mc-mcmc",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Markov Chain MC (MCMC)",
    "text": "Markov Chain MC (MCMC)\nMCMC is the most commonly used numerical algorithm for generating posterior samples.\n A Markov Chain is a sequence of randomly generated numbers where each draw depends on the one immediately preceding it.\n\nPlot – Ian Murray (http://mlg.eng.cam.ac.uk/zoubin/tut06/mcmc.pdf)"
  },
  {
    "objectID": "VB_Bayes2.html#gibbs-sampling",
    "href": "VB_Bayes2.html#gibbs-sampling",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Gibbs Sampling",
    "text": "Gibbs Sampling\nGibbs sampling is a type of MCMC that leverages the conditional distributions of parameters to generate samples by proposing them one at a time. This is the algorithm implemented in the popular Bayesian packages BUGS, WinBUGS, \\({\\tt nimble}\\), and JAGS/\\({\\tt rjags}\\), and that we use for \\({\\tt bayesTPC}\\).\n We will treat Gibbs sampling and other of the numerical methods as mostly “black boxes”. We’ll learn to diagnose output from these later on in the practical component."
  },
  {
    "objectID": "VB_Bayes2.html#what-do-we-do-with-posterior-samples",
    "href": "VB_Bayes2.html#what-do-we-do-with-posterior-samples",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "What do we do with Posterior Samples?",
    "text": "What do we do with Posterior Samples?\nWe can treat the draws much like we would data:\n\nCalculate posterior summaries (mean, median, mode, etc) just like we would a data sample\nCalculate precision of the summaries (e.g., sample variance)\nCIs via quantiles (order statistics of the data) or HPD intervals (using \\({\\tt CODA}\\) package in \\({\\tt R}\\))\n\n\nIf the samples are parameters in a complex model, we can plug them all in, one at a time, to get a range of possible predictions from the model (we’ll see this in the practical bit, later on)."
  },
  {
    "objectID": "VB_Bayes2.html#models-comparison-via-dic",
    "href": "VB_Bayes2.html#models-comparison-via-dic",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Models Comparison via (DIC)",
    "text": "Models Comparison via (DIC)\nThe  Deviance Information Criterion (DIC) seeks to judge a model on how well it fits, penalized by the complexity of the model: \\[\nDIC = D(\\bar{\\theta}) + 2p_D\n\\] where:\n\nDeviance: \\(D(\\theta)=-2\\log(\\mathcal{L}(\\theta; y)) + C\\)\nPenalty: \\(p_D = \\bar{D} -D(\\bar{\\theta})\\)\n\\(D(\\bar{\\theta})\\): deviance at the posterior mean of \\(\\theta\\)\n\\(\\bar{D}\\): average deviance across the posterior samples.\n\n\\(\\rightarrow\\) Already implemented in nimble!"
  },
  {
    "objectID": "VB_Bayes2.html#bayesian-using-nimblejags",
    "href": "VB_Bayes2.html#bayesian-using-nimblejags",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Bayesian using nimble/JAGS",
    "text": "Bayesian using nimble/JAGS\nBoth nimble and JAGS implement Gibbs sampling/MCMC in a fairly easy to use package that you can call from R. Models are encoded using the BUGS language.\n\nThat is, once you specify the appropriate sampling distribution/likelihood and any priors for the parameters, it will use MCMC to obtain samples from the posterior in the right ratios so that we can calculate whatever we want."
  },
  {
    "objectID": "VB_Bayes2.html#specifying-a-bugs-model",
    "href": "VB_Bayes2.html#specifying-a-bugs-model",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Specifying a BUGS model",
    "text": "Specifying a BUGS model\nThe trickiest and most important part of each analysis is properly specifying the model for all of the data that you want to fit. Before you begin to code, you need to decide:\n\n\nWhat is the relationship between your predictors and your response?\nWhat kind of probability distribution should you use to describe your response variable?\nAre there any constraints on your parameters or responses that you need to encode in your prior or likelihood, respectively?"
  },
  {
    "objectID": "VB_Bayes2.html#next-steps",
    "href": "VB_Bayes2.html#next-steps",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Computation and MCMC",
    "section": "Next Steps",
    "text": "Next Steps\nThere are two practicals focusing on using \\({\\tt nimble}\\) and \\({\\tt bayesTPC}\\) to conduct analyses. It has two main chunks:\n\nComparing your conjugate Bayesian analysis on the midge data to the approximate results with \\({\\tt nimble}\\).\nFitting a TPC to trait data using \\({\\tt bayesTPC}\\) (easier than having to code it yourself in \\({\\tt nimble}\\)!).\n\nFor both you’ll be led through visualizing your MCMC chains and your posterior distributions of parameters and predictions. There are also advanced practice suggestions for those who want to go further."
  },
  {
    "objectID": "VB_Bayes_activity2B.html",
    "href": "VB_Bayes_activity2B.html",
    "title": "Introduction to Bayesian Methods",
    "section": "",
    "text": "This section is focused on using the bayesTPC package to fit TPCs to data using the methods we’ve explored in the Bayesian lectures and the first two activities. Here we won’t be talking much about the implementation, but instead will rely on the bayesTPC package and it’s functions to allow us to specify, fit, and analyze the data.\n\n\nFor this practical you will need to first install nimble, then be sure to install the following packages:\n\n# Load libraries\nrequire(nimble)\nrequire(HDInterval)\nlibrary(MCMCvis)\nrequire(coda) # makes diagnostic plots\nrequire(IDPmisc) # makes nice colored pairs plots to look at joint posteriors\nrequire(\"matrixStats\")\nrequire(\"truncnorm\")\n##require(mcmcplots) # another option for diagnostic plots, currently unused\n\n We are also introducing our new, in development, package bayesTPC. It is currently available through github.\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"johnwilliamsmithjr/bayesTPC\")\nlibrary(bayesTPC)"
  },
  {
    "objectID": "VB_Bayes_activity2B.html#the-data",
    "href": "VB_Bayes_activity2B.html#the-data",
    "title": "Introduction to Bayesian Methods",
    "section": "The Data",
    "text": "The Data\nThese data are traits from Aedes aegypti mosquitoes measured across temperature in lab experiments. The traits we have data on thermal performance are:   - pEA: proportion surviving from egg to adulthood\n- MDR: mosquito development rate\n- PDR: parasite development rate (= 1/EIP the extrinsic incubation period)\n- \\mu (mu): death rate (here = 1/longevity)\nNote that some of the traits come in multiple forms (e.g., \\mu and 1/\\mu, PDR and EIP, if we’re assuming lifespan and development time are exponentially distributed – a common modeling assumption).\nAs always, first we have a look at the data:\n\nhead(Aaeg.data)\n\n  trait.name Temp   trait                   ref trait2 trait2.name\n1        pEA   22 0.90812 Westbrook_Thesis_2010   <NA>        <NA>\n2        pEA   27 0.93590 Westbrook_Thesis_2010   <NA>        <NA>\n3        pEA   32 0.81944 Westbrook_Thesis_2010   <NA>        <NA>\n4        MDR   22 0.09174 Westbrook_Thesis_2010   <NA>        <NA>\n5        MDR   27 0.13587 Westbrook_Thesis_2010   <NA>        <NA>\n6        MDR   32 0.15823 Westbrook_Thesis_2010   <NA>        <NA>\n\n\nNow let’s pull a subset of the data related to mortality/survival:\n\nmu.data <- subset(Aaeg.data, trait.name == \"mu\")\nlf.data <- subset(Aaeg.data, trait.name == \"1/mu\")\npar(mfrow=c(1,2), bty=\"l\") \nplot(trait ~ Temp, data = mu.data, ylab=\"mu\")\nplot(trait ~ Temp, data = lf.data, ylab=\"1/mu\")\n\n\n\n\n\n\n\n\nNote that the \\mu data is u-shaped and the lifespan data is hump-shaped.\nWe could choose to fit this either way. Since thermal performance metrics are often assumed to be unimodal thermal responses, we will fit lifespan instead of \\mu as our example. Thus, we’ll need to convert the \\mu data to lifespan by taking the inverse. We will combine the data, by assuming that lifespan is 1/\\mu (not usually a good idea, but we’re going to do it here so we have more data for the example).\n\nmu.data.inv <- mu.data # make a copy of the mu data\nmu.data.inv$trait <- 1/mu.data$trait # take the inverse of the trait values to convert mu to lifespan\nlf.data.comb <- rbind(mu.data.inv, lf.data) # combine both lifespan data sets together \npar(mfrow=c(1,1), bty=\"l\") \nplot(trait ~ Temp, data = lf.data.comb, ylab=\"1/mu\",\n     ylim=c(0,40))"
  },
  {
    "objectID": "VB_Bayes_activity2B.html#two-thermal-performance-curve-models",
    "href": "VB_Bayes_activity2B.html#two-thermal-performance-curve-models",
    "title": "Introduction to Bayesian Methods",
    "section": "Two thermal performance curve models",
    "text": "Two thermal performance curve models\nAlthough there are many functional forms that can be used to describe TPCs, we’ll focus on two of the more common (and easy to fit) functions. Traits that respond unimodally but symmetrically to temperature (often the case for compound traits) can be fit with a quadratic function: f_1(T) = \\begin{cases} 0 &\\text {if } T \\leq T_0 \\\\\n-q (T-T_0) (T-T_m) & \\text {if } T_0 < T <T_m \\\\\n0 &\\text{if } T \\geq T_m \\end{cases}\nTraits that respond unimodally but asymetrically can be fited with a Briere function: \nf_2(T) = \\begin{cases} 0 &\\text {if } T \\leq T_0 \\\\\nq T (T-T_0) \\sqrt{T_m-T} & \\text {if } T_0 < T <T_m \\\\\n0 &\\text{if } T \\geq T_m \\end{cases}\nIn both models, T_0 is the lower thermal limit, T_m is the upper thermal limit (i.e., where the trait value goes to zero on either end), and q>0 scales the height of the curve, (and so also the value of the trait at the optimum temperature). Note that above we’re assuming that the quadratic must be concave down (hence the negative sign), and that the performance goes to zero outside of the thermal limits."
  },
  {
    "objectID": "VB_Bayes_activity2B.html#model-and-data-specification",
    "href": "VB_Bayes_activity2B.html#model-and-data-specification",
    "title": "Introduction to Bayesian Methods",
    "section": "Model and data specification",
    "text": "Model and data specification\nUnlike the previous Bayesian example, bayesTPC has a number of TPCs already implemented. We can view which TPC models are currently implemented:\n\nget_models()\n\n [1] \"binomial_glm_lin\"  \"binomial_glm_quad\" \"briere\"           \n [4] \"gaussian\"          \"kamykowski\"        \"pawar_shsch\"      \n [7] \"quadratic\"         \"ratkowsky\"         \"stinner\"          \n[10] \"weibull\"          \n\n\nWe can view the form of the implemented TPC using the get_formula function:\n\nget_formula(\"quadratic\")\n\nexpression(-1 * q * (Temp - T_min) * (Temp - T_max) * (T_max > \n    Temp) * (Temp > T_min))\n\n\nCurrently, the likelihood for all TPCs is by default is a normal distribution with a lower truncation at zero, and where the mean of the normal distribution is set to be the TPC (here a quadratic). The last piece of the Bayesian puzzle is the prior. You can see the default parameter names and their default priors using “get_default_priors”:\n\nget_default_priors(\"quadratic\")\n\n              q           T_max           T_min \n  \"dunif(0, 1)\" \"dunif(25, 60)\"  \"dunif(0, 24)\" \n\n\nAs you can see, for the quadratic function, the default priors are specified via uniform distributions (the two arguments specific the lower and upper bounds, respectively). For the quadratic (and the Briere), the curvature parameter must be positive, and the priors need to be specified to ensure that T_{min}<T_{max}. Note that if you want to set a prior to a normal distribution, unlike in R and most other programs, in nimble (and thus bayesTPC) the inverse of the variance of the normal distribution is used, denoted by \\tau = \\frac{1}{\\sigma^2}.\nbayesTPC expects data to be in a named list with the “Trait” as the response and “Temp” as the predictor, that is:\n\nlf.data.bTPC<-list(Trait = lf.data.comb$trait, Temp=lf.data.comb$Temp)\n\nThe workhorse of the bayesTPC package is the b_TPC function. If you are happy to use the default priors, etc, the usage is simply:\n\nAedTestFit<- b_TPC(data = lf.data.bTPC, model = 'quadratic')\n\nCreating NIMBLE model:\n\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n===== Monitors ===== \nthin = 1: q, sigma.sq, T_max, T_min\n===== Samplers ===== \nRW sampler (4)\n  - q\n  - T_max\n  - T_min\n  - sigma.sq\n\nRunning MCMC:\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\n We can examine the object that is saved. It includes the data, information on the priors, numbers of samples, and the samples themselves.\n\nnames(AedTestFit)\n\n[1] \"samples\"      \"mcmc\"         \"data\"         \"model_type\"   \"priors\"      \n[6] \"constants\"    \"uncomp_model\"\n\n\n We’ll mostly be using the samples:\n\ndim(AedTestFit$samples) # number of samples then number of params\n\n[1] 10000     4\n\nhead(AedTestFit$samples) # show first few sets of samples\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1 \nEnd = 7 \nThinning interval = 1 \n        T_max    T_min         q sigma.sq\n[1,] 44.43478 14.62259 0.1137034 4.819677\n[2,] 44.43478 13.73256 0.1137034 4.819677\n[3,] 44.43478 13.73256 0.1137034 4.819677\n[4,] 43.59761 13.73256 0.1137034 5.195313\n[5,] 43.59761 13.73256 0.1137034 5.770068\n[6,] 43.58247 12.79661 0.1137034 6.872366\n[7,] 42.87303 12.29535 0.1137034 6.872366\n\n\n Notice that the samples are of type MCMC, which means they’ve been formatted with the coda package (which bayesTPC uses for some of the plotting and diagnostics). Further, by default we take 10000 samples, no burnin, using a random walk sampler.\nBut we may also want to check what model we fit and the priors that were set:\n\nAedTestFit$model_type\n\n[1] \"quadratic\"\n\nAedTestFit$priors\n\n                                       q \n                           \"dunif(0, 1)\" \n                                   T_max \n                         \"dunif(25, 60)\" \n                                   T_min \n                          \"dunif(0, 24)\" \n                                sigma.sq \n\"T(dt(mu = 0, tau = 1/10, df = 1), 0, )\""
  },
  {
    "objectID": "VB_Bayes_activity2B.html#mcmc-diagnostics",
    "href": "VB_Bayes_activity2B.html#mcmc-diagnostics",
    "title": "Introduction to Bayesian Methods",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nWe’ll show you a few different ways to examine the output. View the summary of parameters (only the first 5 lines, or it will also show you all of your derived quantities):\n\nsummary(AedTestFit$samples)\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean       SD  Naive SE Time-series SE\nT_max    39.0788  1.70623 0.0170623        0.16478\nT_min     6.9664  2.26052 0.0226052        0.23750\nq         0.1152  0.03007 0.0003007        0.00413\nsigma.sq 43.5310 14.51913 0.1451913        0.66528\n\n2. Quantiles for each variable:\n\n             2.5%      25%    50%     75%   97.5%\nT_max    36.25542 37.95629 38.912 39.9542 43.4823\nT_min     1.87988  5.66072  7.134  8.3285 11.5630\nq         0.06522  0.09488  0.112  0.1305  0.1899\nsigma.sq 23.55891 33.88284 40.889 50.0216 78.8200\n\n\n We can also assess this visually by plotting the chains of the three main TPC parameters and the standard devation of the normal observation model:\n\npar(mfrow=c(2,2))\ntraceplot(AedTestFit)\n\n\n\n\n\n\n\n\nThese all seem to be mixing alright, although we can see that we need to drop a bit of the burn-in.\nWe can examine the ACF of the chains as well (one for each parameter), similarly to a time series, to again check for autocorrelation within the chain (we want the autocorrelation to be fairly low):\n\ns1<-as.data.frame(AedTestFit$samples)\npar(mfrow=c(2,2))\nfor(i in 1:4) {\n  acf(s1[,i], lag.max=50, main=\"\",\n      ylab = paste(\"ACF: \", names(s1)[i], sep=\"\"))\n}\n\n\n\n\n\n\n\n\nThere is still a bit of autocorrelation, especially for the 3 quadratic parameters, but it isn’t too bad. The chain for \\sigma is mixing best (the ACF falls off the most quickly). We could reduce the autocorrelation even further by thinning the chain (i.e., change the nt parameter to 5 or 10).\nThe last important diagnostic is to compare the prior and posterior distributions. Various packages in R have bespoke functions to do this. bayesTPC includes a built in function that creates posterior/prior overlap plots for all model parameters (note that the priors are smoothed because the algorithm uses kernel smoothing instead of the exact distribution).\n\nppo_plot(AedTestFit)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior distribution here is very different from the posterior. These data are highly informative for the parameters of interest and are very unlikely to be influenced much by the prior distribution (although you can always change the priors to check this). However, notice that the posteriors of T_m and T_0 are slightly truncated by their priors."
  },
  {
    "objectID": "VB_Bayes_activity2B.html#visualizing-the-joint-posterior-of-parameters",
    "href": "VB_Bayes_activity2B.html#visualizing-the-joint-posterior-of-parameters",
    "title": "Introduction to Bayesian Methods",
    "section": "Visualizing the joint posterior of parameters",
    "text": "Visualizing the joint posterior of parameters\nNow that we’ve confirmed that things are working well, it’s often useful to also look at the joint distribution of all of your parameters together. Of course, if you have a high dimensional posterior, rendering a 2-D representation can be difficult. Instead, the standard is to examine the pair-wise posterior distribution, for instance as follows:\n\nbayesTPC_ipairs(AedTestFit)\n\n\n\n\n\n\n\n\nAs you can see, estimates of T_0 and T_m are highly correlated with q– not surprising given the interplay between them in the quadratic function. This correlation is an important feature of the system, and we use the full posterior distribution that includes this correlation when we want to build the corresponding posterior distribution of the behavior of the quadratic function that we’ve fit."
  },
  {
    "objectID": "VB_Bayes_activity2B.html#modifying-the-fitting-routines",
    "href": "VB_Bayes_activity2B.html#modifying-the-fitting-routines",
    "title": "Introduction to Bayesian Methods",
    "section": "Modifying the fitting routines",
    "text": "Modifying the fitting routines\nAs we noted above, the bTPC function has a set of default specifications for multiple components for every type of implemented TPC. Many of these we can change. For example, above we noted that we can see that we need to drop samples for the burn-in. We might also want to change priors, or use an alternative sampler.\n\nAedQuadFit <- b_TPC(data = lf.data.bTPC, ## data\n                    model = 'quadratic', ## model to fit\n                    niter = 11000, ## total iterations\n                    burn = 1000, ## number of burn in samples\n                    samplerType = 'AF_slice', ## slice sampler\n                    priors = list(q = 'dunif(0, .5)', \n                                  sigma.sq = 'dexp(1)') ## priors\n                    ) \n\nCreating NIMBLE model:\n\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n===== Monitors ===== \nthin = 1: q, sigma.sq, T_max, T_min\n===== Samplers ===== \nAF_slice sampler (1)\n  - q, T_max, T_min, sigma.sq \n\nRunning MCMC:\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\n Let’s take a look at the output in this case:\n\nsummary(AedQuadFit$samples)\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean      SD  Naive SE Time-series SE\nT_max    38.8628 1.03113 0.0103113      0.0173415\nT_min     6.9304 1.29244 0.0129244      0.0218539\nq         0.1138 0.01714 0.0001714      0.0002824\nsigma.sq 18.3480 2.65298 0.0265298      0.0312130\n\n2. Quantiles for each variable:\n\n            2.5%     25%     50%     75%   97.5%\nT_max    37.1495 38.1422 38.7711 39.4617 41.1361\nT_min     4.0666  6.1638  7.0503  7.8229  9.1220\nq         0.0807  0.1022  0.1136  0.1254  0.1478\nsigma.sq 13.6941 16.5075 18.1267 19.9897 24.0896\n\n\n We again plot the chains of the three main TPC parameters and the standard deviation of the normal observation model:\n\n## plot(lf.fit.mcmc[,c(1,3,4)]) ## default coda plot\npar(mfrow=c(2,2))\ntraceplot(AedQuadFit)\n\n\n\n\n\n\n\n\nThese all seem to be mixing well, better that the first time, although we can see that we need to drop a bit of the burn-in.\nWe again look at ACF of the chains as well (one for each parameter):\n\ns1<-as.data.frame(AedQuadFit$samples)\npar(mfrow=c(2,2))\nfor(i in 1:4) acf(s1[,i], lag.max=50, main=\"\", ylab = paste(\"ACF: \", names(s1)[i], sep=\"\"))\n\n\n\n\n\n\n\n\nNotice this falls off much more quickly – the samples from the slice filter in this case are less autocorrelated than the default random walk (“RW”) filter.\nAnd comparing the new priors to the posteriors:\n\nppo_plot(AedQuadFit, burn = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s also fit the Briere function to the data, just to see how it does:\n\nAedBriFit <- b_TPC(data = lf.data.bTPC, ## data\n                    model = 'briere', ## model to fit\n                    niter = 11000, ## total iterations\n                    burn = 1000, ## number of burn in samples\n                    samplerType = 'AF_slice', ## slice sampler\n                    priors = list(T_min = \"dunif(5,10)\",\n                                  T_max = \"dunif(18,35)\",\n                                  sigma.sq = 'dexp(1)') ## priors\n                    ) \n\nCreating NIMBLE model:\n\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n===== Monitors ===== \nthin = 1: q, sigma.sq, T_max, T_min\n===== Samplers ===== \nAF_slice sampler (1)\n  - q, T_max, T_min, sigma.sq \n\nRunning MCMC:\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\n\nsummary(AedBriFit$samples)\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean      SD  Naive SE Time-series SE\nT_max    34.6785 0.19000 0.0019000      0.0021598\nT_min     5.9884 0.71908 0.0071908      0.0099586\nq         0.4878 0.02878 0.0002878      0.0003643\nsigma.sq 18.0176 2.64058 0.0264058      0.0333677\n\n2. Quantiles for each variable:\n\n            2.5%     25%     50%    75%   97.5%\nT_max    34.2885 34.5421 34.6896 34.833 34.9801\nT_min     5.0396  5.4075  5.8581  6.433  7.6763\nq         0.4401  0.4672  0.4849  0.505  0.5528\nsigma.sq 13.4731 16.1402 17.7733 19.649 23.8433\n\n\nWe again plot the chains of the three main TPC parameters and the standard deviation of the normal observation model:\n\n## plot(lf.fit.mcmc[,c(1,3,4)]) ## default coda plot\npar(mfrow=c(2,2))\ntraceplot(AedBriFit, burn=1000)\n\n\n\n\n\n\n\n\nOverall very good mixing, but we can see our choice of priors wasn’t ideal.\nWe again look at ACF of the chains as well (one for each parameter):\n\ns2<-as.data.frame(AedBriFit$samples[1000:10000,])\npar(mfrow=c(2,2))\nfor(i in 1:4) {\n  acf(s2[,i], lag.max=50, main=\"\",\n      ylab = paste(\"ACF: \", names(s2)[i], sep=\"\"))\n}\n\n\n\n\n\n\n\n\nThis is great! Finally comparing the new priors to the posteriors:\n\nppo_plot(AedBriFit, burn = 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we look at the joint posterior.\n\nbayesTPC_ipairs(AedBriFit, burn=1000)\n\n\n\n\n\n\n\n\nFinally, what if we wanted to fit a model not included in bayesTPC?\nLet’s look at how the package defines the Briere model.\n\nget_default_model_specification(\"briere\")\n\nbayesTPC Model Specification of Type: briere\nModel Formula:\n  q * (Temp - T_min) * sqrt((T_max > Temp) * abs(T_max - Temp)) * (T_max > Temp) * (Temp > T_min)\nModel Parameters and Priors:\n  q: dunif(0, 1) \n  T_max: dunif(25, 60) \n  T_min: dunif(0, 24)\nPrior for Variance:\n  T(dt(mu = 0, tau = 1/10, df = 1), 0, )\n\n\n For our model, we will choose an alternate Briere formula, changing the square root to a cube root.\n\nf_2(T) = \\begin{cases} 0 &\\text{if } T \\leq T_0 \\\\\nq T (T-T_0) \\sqrt[3]{T_m-T} & \\text{if } T_0 < T <T_m \\\\\n0 &\\text{if } T \\geq T_m \\end{cases}  \n\n\nmy_briere_formula <- expression(q * (Temp - T_min) * ((T_max > Temp) * abs(T_max - Temp))^(1/3) * (T_max > Temp) * (Temp > T_min))\n\n Now, we choose the priors we want to sample from. We’ll include a little more flexibility here to ensure the model fits how we want it to.\n\nmy_briere_priors <- c(\n  q = \"dunif(0,1)\",\n  T_max = \"dunif(20,45)\",\n  T_min = \"dunif(-5,10)\")\n\n Since we have no constants we need to add, that’s all the information we need! We can use specify_normal_model() to create a model object we can train.\n\nmy_briere <- specify_normal_model(\"my_briere\", #model name\n                                  parameters = my_briere_priors, #names are parameters, values are priors\n                                  formula = my_briere_formula\n                                  )\n\nUsing default prior for model variance.\nNormal model type 'my_briere' can now be accessed using other bayesTPC functions. Reload the package to reset back to defaults.\n\n\n Now we can use this model just like any other.\n\nget_formula(\"my_briere\")\n\nexpression(q * (Temp - T_min) * ((T_max > Temp) * abs(T_max - \n    Temp))^(1/3) * (T_max > Temp) * (Temp > T_min))\n\nget_default_priors(\"my_briere\")\n\n             q          T_max          T_min \n  \"dunif(0,1)\" \"dunif(20,45)\" \"dunif(-5,10)\" \n\n\n We can also pass the model object in, instead of just the name. configure_model() returns the BUGS model that will be trained.\n\ncat(configure_model(my_briere))\n\n{\n    for (i in 1:N){\n            Trait[i] ~ T(dnorm(mean = q * (Temp[i] - T_min) * ((T_max > Temp[i]) * abs(T_max - Temp[i]))^(1/3) * (T_max > Temp[i]) * (Temp[i] > T_min), var = sigma.sq), 0, )\n    }\n    q ~ dunif(0,1)\n    T_max ~ dunif(20,45)\n    T_min ~ dunif(-5,10)\n    sigma.sq ~ T(dt(mu = 0, tau = 1/10, df = 1), 0, )\n}\n\n\n Now, let’s train our Briere model.\n\nAedMyBriFit <- b_TPC(data = lf.data.bTPC, ## data\n                    model = 'my_briere', ## model to fit\n                    niter = 11000, ## total iterations\n                    burn = 1000, ## number of burn in samples\n                    samplerType = 'AF_slice', ## slice sampler\n                    priors = list(sigma.sq = 'dexp(1)') ## priors\n                    )  \n\nCreating NIMBLE model:\n\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n===== Monitors ===== \nthin = 1: q, sigma.sq, T_max, T_min\n===== Samplers ===== \nAF_slice sampler (1)\n  - q, T_max, T_min, sigma.sq \n\nRunning MCMC:\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\n Finally, we can run the same diagnostics as before.\n\npar(mfrow=c(2,2))\ntraceplot(AedMyBriFit, burn=1000)\n\n\n\n\nLooks like the mixing went well again. How about ACF?\n\ns3<-as.data.frame(AedMyBriFit$samples[1000:10000,])\npar(mfrow=c(2,2))\nfor(i in 1:4) {\n  acf(s3[,i], lag.max=50, main=\"\",\n      ylab = paste(\"ACF: \", names(s3)[i], sep=\"\"))\n}\n\n\n\n\n\n\n\n\nGood! Finally, let’s look at our posterior distributions and see how well we fit the data.\n\nppo_plot(AedMyBriFit)\n\n\n\n\n\n\n\n\n\n\n\n\nbayesTPC_ipairs(AedMyBriFit, burn = 1000)"
  },
  {
    "objectID": "VB_Bayes_activity2B.html#plot-the-fits",
    "href": "VB_Bayes_activity2B.html#plot-the-fits",
    "title": "Introduction to Bayesian Methods",
    "section": "Plot the fits",
    "text": "Plot the fits\nFirst extract the fits/predictions using the bayesTPC_summary function and use the tidyverse to save the model predictions as a tibble. We can then use ggplot to generate a pretty plot of our TPC with prediction bounds\n\nlibrary(tidyverse)\nbriere_fit <- as_tibble(bayesTPC_summary(AedMyBriFit, plot = F))\nhead(briere_fit)\n\n# A tibble: 6 × 4\n  Temp_interval Upper_bounds Lower_bounds Medians\n          <dbl>        <dbl>        <dbl>   <dbl>\n1          10.6         19.0         12.7    15.9\n2          10.6         19.0         12.7    15.9\n3          10.6         19.0         12.7    15.9\n4          10.7         19.0         12.8    16.0\n5          10.7         19.1         12.8    16.0\n6          10.7         19.1         12.9    16.0\n\n\n\nggplot(briere_fit)+\n  geom_line(aes(Temp_interval, Medians))+\n  theme_bw()+ \n  geom_ribbon(aes(Temp_interval, ymin=Lower_bounds, ymax=Upper_bounds), fill=\"#30694B\",alpha=0.5,\n              inherit.aes = T)+\n  geom_point(aes(Temp, trait), lf.data.comb, shape=21, fill='#C0C0C0',\n              col='#000000', alpha=0.8, stroke=0.5, size=2)+\n  theme(text = element_text(size=12))+\n  scale_y_continuous(expression(plain(paste(\"lifespan (days)\"))))+\n  labs(x=expression(plain(paste(\" Temperature, \",degree,\"C\"))))+\n  theme(legend.position = 'none',\n        axis.title.y = element_text(size=12), axis.title.x = element_text(size=12),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\nWe can also use the built in function, posteriorPredTPC(), to plot the median and lower/upper bounds from samples taken from the posterior distribution.\n\nposteriorPredTPC(AedMyBriFit)\n\nWarning in posteriorPredTPC(AedMyBriFit): Currently using summaryType = \"hdi\".\nDefault credible interval mass is credMass = .9)\n\n\n\n\n\nFinally we can plot the fits/predictions. These are the posterior estimates of the fitted lines to the data. Recall that we can take each accepted sample, and plug it into the quadratic equations. This gives us the same number of possible lines as samples. We can then summarize these with the HPD intervals across each temperature. This is especially easy in this case because we’ve already saved these samples as output in our model file:"
  },
  {
    "objectID": "VB_Bayes_activity2B.html#additional-analyses",
    "href": "VB_Bayes_activity2B.html#additional-analyses",
    "title": "Introduction to Bayesian Methods",
    "section": "Additional analyses",
    "text": "Additional analyses\nOnce you have all of these samples, you can do many other things. For example, you can use the which.max() function to find the peak temperature (T_{pk}) for adult lifespan and its value at T_{pk}:\n\nlifespan_Tpk <- briere_fit %>% slice(which.max(Medians)) \n\n You can then plot T_{pk} and the trait value at T_{pk}:\n\nggplot(briere_fit)+\n  geom_line(aes(Temp_interval, Medians), size=0.4)+\n  theme_bw()+ \n  geom_ribbon(aes(Temp_interval, ymin=Lower_bounds, ymax=Upper_bounds), fill=\"#30694B\",alpha=0.5,\n              inherit.aes = T)+\n  geom_point(aes(Temp, trait), lf.data.comb, shape=21, fill='#C0C0C0',\n             col='#000000', alpha=0.8, stroke=0.5, size=2)+\n  geom_point(aes(Temp_interval, Medians), lifespan_Tpk, shape=23, fill='yellow',\n            col='#000000', alpha=0.8, stroke=0.5, size=3)+\n  theme(text = element_text(size=12))+\n  scale_y_continuous(expression(plain(paste(\"lifespan (days)\"))))+\n  labs(x=expression(plain(paste(\" Temperature, \",degree,\"C\"))))+\n  theme(legend.position = 'none',\n        axis.title.y = element_text(size=12), axis.title.x = element_text(size=12),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nThis suggests that optimal temperature for adult lifespan in Aedes aegypti is 25.8 degrees Celsius! Can you now figure out how to get the credible intervals for T_{pk} and the trait value at T_{pk}?"
  },
  {
    "objectID": "VB_NLLS_activity.html",
    "href": "VB_NLLS_activity.html",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "",
    "text": "Main Materials\nWe will use the following packages:"
  },
  {
    "objectID": "VB_NLLS_activity.html#introduction",
    "href": "VB_NLLS_activity.html#introduction",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Introduction",
    "text": "Introduction\nIn this section, you will learn to fit non-linear mathematical models to data using Non-Linear Least Squares (NLLS).\nSpecifically, you will learn to\n\nVisualize the data and the mathematical model you want to fit to them\nFit a non-linear model\nAssess the quality of the fit, and whether the model is appropriate for your data\n\nThis section assumes that you have at least a conceptual understanding of what Linear vs Non-linear models are, how they are fitted to data, and how the fits can be assessed statistically.\nWe will use R. For starters, clear all variables and graphic devices and load necessary packages:\n\nrm(list = ls())\ngraphics.off()"
  },
  {
    "objectID": "VB_NLLS_activity.html#traits-data-as-an-example",
    "href": "VB_NLLS_activity.html#traits-data-as-an-example",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Traits data as an example",
    "text": "Traits data as an example\nOur first set of examples will focus on traits.\nA trait is any measurable feature of an individual organism. This includes physical traits (e.g., morphology, body mass, wing length), performance traits (e.g., biochemical kinetics, respiration rate, body velocity, fecundity), and behavioral traits (e.g., feeding preference, foraging strategy, mate choice). All natural populations show variation in traits across individuals. A trait is functional when it directly (e.g., mortality rate) or indirectly (e.g., somatic development or growth rate) determines individual fitness. Therefore, variation in (functional) traits can generate variation in the rate of increase and persistence of populations. When measured in the context of life cycles, without considering interactions with other organisms (e.g., predators or prey of the focal population), functional traits are typically called life history traits (such as mortality rate and fecundity). Other traits determine interactions both within the focal population (e.g., intra-specific interference or mating frequency) and between the focal population/species and others, including the species which may act as resources (prey, for example). Thus both life history and interaction traits determine population fitness and therefore abundance, which ultimately influences dynamics and functioning of the wider ecosystem, such as carbon fixation rate or disease transmission rate."
  },
  {
    "objectID": "VB_NLLS_activity.html#biochemical-kinetics",
    "href": "VB_NLLS_activity.html#biochemical-kinetics",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Biochemical Kinetics",
    "text": "Biochemical Kinetics\nThe properties of an organism’s metabolic pathways, and the underlying (enzyme-mediated) biochemical reactions (kinetics) are arguably its most fundamental “traits”, because these drive all “performance” traits, from photosynthesis and respiration, to movement and growth rate.\nThe Michaelis-Menten model is widely used to quantify reaction kinetics data and estimate key biochemical parameters. This model relates biochemical reaction rate (V) (rate of formation of the product of the reaction), to concentration of the substrate (S):\n\nV = \\frac{V_{\\max} S}{K_M + S}\n\nHere,\n\nV_{\\max} is the maximum rate that can be achieved in the reaction system, which happens at saturating substrate concentration, and\nK_M is the Michaelis or half-saturation constant, defined as the substrate concentration at which the reaction rate is half of V_{\\max }.\n\nBiochemical reactions involving a single substrate are often well fitted by the Michaelis-Menten kinetics, suggesting that its assumptions are often valid.\n\n\n\nThe Michaelis-Menten model.\n\n\nLet’s fit the Michaelis-Menten model to some data.\n\nGenerating data\nInstead of using real experimental data, we will actually generate some “data” because that way we know exactly what the errors in the data are. You can also import and use your own dataset for the fitting steps further below.\nWe can generate some data as follows:\n\nS_data <- seq(1,50,1) # Generate a sequence of substrate concentrations\nS_data\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n\n\nV_data <- ((12.5 * S_data)/(7.1 + S_data)) # Generate a Michaelis-Menten response with V_max = 12.5 and K_M = 7.1\nplot(S_data, V_data)\n\n\n\n\nNote that our choice of V_{\\max} = 12.5 and K_M = 7.1 is completely arbitrary. As long as we make sure that V_{\\max} > 0, K_H > 0, and K_M lies well within the lower half of the the range of substrate concentrations (0-50), these “data” will be physically biologically sensible.\nNow let’s add some random (normally-distributed) fluctuations to the data to emulate experimental / measurement error:\n\nset.seed(1456) # To get the same random fluctuations in the \"data\" every time\nV_data <- V_data + rnorm(50,0,1) # Add random fluctuations to emulate error with standard deviation of 0.5\nplot(S_data, V_data)\n\n\n\n\nThat looks real!\n\n\nFitting the model\nNow, fit the model to the data:\n\nMM_model <- nls(V_data ~ V_max * S_data / (K_M + S_data))\n\nWarning in nls(V_data ~ V_max * S_data/(K_M + S_data)): No starting values specified for some parameters.\nInitializing 'V_max', 'K_M' to '1.'.\nConsider specifying 'start' or using a selfStart model\n\n\nThis warning arises because nls requires “starting values” for the parameters (two in this case: V_max and K_M) to start searching for optimal combinations of parameter values (ones that minimize the RSS). Indeed, all NLLS fitting functions / algorithms require this. If you do not provide starting values, nls gives you a warning (as above) and uses a starting value of 1 for every parameter by default. For simple models, despite the warning, this works well enough.\n\nBefore proceeding further, have a look at what `nls()`'s arguments are using `?nls`, or looking at the documentation [online](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/nls).\n\nWe will address the issue of starting values soon enough, but first let’s look at how good the fit that we obtained looks. The first thing to do is to see how well the model fitted the data, for which plotting is the best first option:\n\nplot(S_data,V_data, xlab = \"Substrate Concentration\", ylab = \"Reaction Rate\")  # first plot the data \nlines(S_data,predict(MM_model),lty=1,col=\"blue\",lwd=2) # now overlay the fitted model \n\n\n\n\nThis looks pretty good.\nNote that we used we used the predict() function here just as we did in any of the linear models sections.\n\nIn general, you can use most of the same commands/functions (e.g., `predict()` and `summary()`) on the output of a `nls()` model fitting object as you would on a `lm()` model fitting object.\n\nNow lets get some stats of this NLLS fit. Having obtained the fit object (MM_model), we can use summary() just like we would for a lm() fit object:\n\nsummary(MM_model)\n\n\nFormula: V_data ~ V_max * S_data/(K_M + S_data)\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \nV_max  13.5041     0.5345  25.265  < 2e-16 ***\nK_M     9.4085     1.3198   7.128 4.67e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.001 on 48 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 1.938e-06\n\n\nThis looks a lot like the output of a linear model, and to be specific, of a Linear Regression. For starters, compare the above output with the output of summary(genomeSizeModelDragon) the linear regression section.\nSo here are the main things to note about the output of summary() of an nls() model object:\n\nEstimates are, as in the output of the lm() function for fitting linear models, the estimated values of the coefficients of the model that you fitted (V_(\\max) and K_M). Note that although we generated our data using V_{\\max} = 12.5 and K_M = 7.1, the actual coefficients are quite different from what we are getting with the NLLS fitting ( \\hat{V_{\\max} = 13.5 and \\hat{K}_M = 9.4). This is because we introduced random (normally-distributed) errors. This tell you something about how experimental and/or measurement errors can distort your image of the underlying mechanism or process.\n\nStd. Error, t value, and Pr(>|t|) and Residual standard error have the same interpretation as in the output of lm() (please look back at the Linear Regression section)\nNumber of iterations to convergence tells you how many times the NLLS algorithm had to adjust the parameter values till it managed to find a solution that minimizes the Residual Sum of Squares (RSS)\nAchieved convergence tolerance tells you on what basis the algorithm decided that it was close enough to the a solution; basically if the RSS does not improve more than a certain threshold despite parameter adjustments, the algorithm stops searching. This may or may not be close to an optimal solution (but in this case it is).\n\nThe last two items are specific to the output of an nls() fitting summary(), because unlike Ordinary Least Squares (OLS), which is what we used for Linear regression, NLLS is not an exact procedure, and the fitting requires computer simulations. As such, you do not need to report these last two items when presenting the results of an NLLS fit, but they are useful for problem solving in case the fitting does not work (more on this below).\nAs noted above, you can use the same sort of commands on a nls() fitting result as you can on a lm() object.\nFor example, you can get just the values of the estimated coefficients using:\n\ncoef(MM_model)\n\n    V_max       K_M \n13.504118  9.408462 \n\n\nThus, much of the output of NLLS fitting using nls() is analogous to the output of an lm(). However, further statistical inference here cannot be done using Analysis of Variance (ANOVA), because the model is not a Linear Model. Try anova(MM_model), and see what happens. We will address statistical inference with NLLS model fitting further below (with a different example)."
  },
  {
    "objectID": "VB_NLLS_activity.html#confidence-intervals",
    "href": "VB_NLLS_activity.html#confidence-intervals",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nOne particularly useful thing you can do after NLLS fitting is to calculate/construct the confidence intervals (CI’s) around the estimated parameters in our fitted model, analogous to how we would in the OLS fitting used for Linear Models:\n\nconfint(MM_model)\n\nWaiting for profiling to be done...\n\n\n           2.5%    97.5%\nV_max 12.538677 14.66093\nK_M    7.124685 12.36158\n\n\nThe Waiting for profiling to be done... message reflects the fact that calculating the standard errors from which the CI’s are calculated requires a particular computational procedure (which we will not go into here) when it comes to NLLS fits. Calculating confidence intervals can be useful because you can use a coefficient/parameter estimate’s confidence intervals to test whether it is significantly different from some reference value. Note that the intervals for K_M do not include the original value of K_M = 7.1 that we used to generate the data!.\nAlso, the intervals should not include zero for the coefficient to be statistically significant in itself, that is, different from zero."
  },
  {
    "objectID": "VB_NLLS_activity.html#r-squared-values",
    "href": "VB_NLLS_activity.html#r-squared-values",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "R-squared values",
    "text": "R-squared values\nTo put it simply, unlike an R^2 value obtained by fitting a linear model, that obtained from NLLS fitting is not reliable, and should not be used. The reason for this is somewhat technical (e.g., see this paper) and we won’t go into it here. But basically, NLLS R^2 values do not always accurately reflect the quality of fit, and definitely cannot be used to select between competing models. Indeed R^2 values obtained from NLLS fitting even be negative when the model fits very poorly! We will learn more about model selection with non-linear models later below."
  },
  {
    "objectID": "VB_NLLS_activity.html#the-starting-values-problem",
    "href": "VB_NLLS_activity.html#the-starting-values-problem",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "The starting values problem",
    "text": "The starting values problem\nNow let’s revisit the issue of starting values in NLLS fitting. Previously, we fitted the Michaelis-Menten Model without any starting values, and R gave us a warning but managed to fit the model to our synthetic “data” using default starting values.\nLets try the NLLS fitting again, but with some particular starting values:\n\nMM_model2 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 2, K_M = 2))\n\nNote that unlike before, we got no warning message about starting values.\nLet’s compare the coefficient estimates from our two different model fits to the same dataset:\n\ncoef(MM_model)\n\n    V_max       K_M \n13.504118  9.408462 \n\ncoef(MM_model2)\n\n    V_max       K_M \n13.504105  9.408426 \n\n\nNot too different, but not exactly the same!\nIn contrast, when you fit linear models you will get exactly the same coefficient estimates every single time, because OLS is an exact procedure.\nNow, let’s try even more different start values:\n\nMM_model3 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = .01, K_M = 10))\n\nCompare the coefficients of this model fit to the two previous ones:\n\ncoef(MM_model)\n\n    V_max       K_M \n13.504118  9.408462 \n\ncoef(MM_model2)\n\n    V_max       K_M \n13.504105  9.408426 \n\ncoef(MM_model3)\n\n    V_max       K_M \n 2.956160 -3.474732 \n\n\nThe estimates in our latest model fit are completely different (in fact, K_M is negative)! Let’s plot this model’s and the first model’s fit together:\n\nplot(S_data,V_data)  # first plot the data \nlines(S_data,predict(MM_model),lty=1,col=\"blue\",lwd=2) # overlay the original model fit\nlines(S_data,predict(MM_model3),lty=1,col=\"red\",lwd=2) # overlay the latest model fit\n\n\n\n\nAs you would have guessed from the really funky coefficient estimates that were obtained in MM_model3, this is a pretty poor model fit to the data, with the negative value of K_M causing the fitted version of the Michaelis-Menten model to behave strangely.\nLet’s try with even more different starting values.\n\nnls(V_data ~ V_max * S_data / (K_M + S_data), \n    start = list(V_max = 0, K_M = 0.1))\n\nTry putting a zero for the starting V_max value. You’ll get an error. The singular gradient matrix at initial parameter estimates error arises from the fact that the starting values you provided were so far from the optimal solution, that the parameter searching in nls() failed at the very first step. The algorithm could not figure out where to go from those starting values. In fact, the starting value we gave it is biologically/ physically impossible, because V_max can’t really equal 0.\nLet’s look at some more starting values that can cause the model fitting to fail:\n\nnls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 100))\n\n\nnls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -0.1, K_M = 100))\n\nIn both the above cases, the model fitting was able to start, but eventually failed because the starting values were too far from the (approximately) optimal values (V_{\\max} \\approx 13.5, K_M \\approx 9.4).\nAnd what happens if we start really close to the optimal values? Let’s try:\n\nMM_model4 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 13.5, K_M = 9.4))\n\n\ncoef(MM_model)\n\n    V_max       K_M \n13.504118  9.408462 \n\ncoef(MM_model4)\n\n    V_max       K_M \n13.504102  9.408419 \n\n\nThe results of the first model fit and this last one are still not exactly the same! This drives home the point that NLLS is not an “exact” procedure. However, the differences between these two solutions are minuscule, so the main thing to take away is that if the starting values are reasonable, NLLS is exact enough.\nNote that and even if you started the NLLS fitting with the exact parameter values with which you generated the data before introducing errors (so use start = list(V_max = 12.5, K_M = 7.1) above instead), you would still get the same result for the coefficients (try it). This is because the NLLS fitting will converge back to the parameter estimates based on the actual data, errors and all."
  },
  {
    "objectID": "VB_NLLS_activity.html#a-more-robust-nlls-algorithm",
    "href": "VB_NLLS_activity.html#a-more-robust-nlls-algorithm",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "A more robust NLLS algorithm",
    "text": "A more robust NLLS algorithm\nThe standard NLLS function in R, nls, which we have been using so far, does the NLLS fitting by implementing an algorithm called the Gauss-Newton algorithm. While the Gauss-Newton algorithm works well for most simple non-linear models, it has a tendency to “get lost” or “stuck” while searching for optimal parameter estimates (that minimize the residual sum of squares, or RSS). Therefore, nls will often fail to fit your model to the data if you start off at starting values for the parameters that are too far from what the optimal values would be, as you saw above (e.g., when you got the singular gradient matrix error).\nSome nonlinear models are especially difficult for nls to fit to data because such model have a mathematical form that makes it hard to find parameter combinations that minimize the residual sum of squared (RSS). If this does not makes sense, don’t worry about it.\nOne solution to this is to use a different algorithm than Gauss-Newton. nls() has one other algorithm that can be more robust in some situations, called the “port” algorithm. However, there is a better solution still: the Levenberg-Marqualdt algorithm, which is less likely to get stuck (is more robust than) than Gauss-Newton (or port). If you want to learn more about the technicalities of this, here are here are good places to start (also see the Readings list at the end of this section).\nTo be able to use nlsLM, we will need to switch to a different NLLS function called nlsLM. In order to be able to use nlsLM, you will need the nls.lm R package, which you can install using the method appropriate for your operating system (e.g., linux users will launch R in sudo mode first) and then use:\nNow let’s use the minpack.lm package:\nLet’s try it (using the same starting values as MM_model2 above):\n\nMM_model5 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 2, K_M = 2))\n\nNow compare the nls and nlsLM fitted coefficients:\n\ncoef(MM_model2)\n\n    V_max       K_M \n13.504105  9.408426 \n\ncoef(MM_model5)\n\n    V_max       K_M \n13.504116  9.408456 \n\n\nClose enough.\nNow, let’s try fitting the model using all those starting parameter combinations that failed previously:\n\nMM_model6 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 1, K_M = 10))\n\n\nMM_model7 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0, K_M = 0.1))\n\n\nMM_model8 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 100))\n\n\nMM_model9 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -0.1, K_M = 100))\n\n\ncoef(MM_model2)\n\n    V_max       K_M \n13.504105  9.408426 \n\ncoef(MM_model5)\n\n    V_max       K_M \n13.504116  9.408456 \n\ncoef(MM_model6)\n\n    V_max       K_M \n13.504119  9.408464 \n\ncoef(MM_model7)\n\n    V_max       K_M \n 6.235507 -1.392507 \n\ncoef(MM_model8)\n\n    V_max       K_M \n13.504126  9.408482 \n\ncoef(MM_model9)\n\n    V_max       K_M \n13.504115  9.408453 \n\n\nNice, these all worked with nlsLM even though they had failed with nls! But one of them (model 7) still gives you poor values for the coefficients.\nBut nlsLM also has its limits. Let’s try more absurd starting values:\n\nnlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -10, K_M = -10))\n\nThus, using starting values that are in a sensible range is always a good idea. Here, we know that neither V_{\\max} nor K_M can be negative, so we can use that bit of information to assign reasonable starting values.\n\n*How do you find \"sensible\" starting values for NLLS fitting?* This very much depends on your understanding of the mathematical model that is being fitted to the data, and the mechanistic interpretation of its parameters."
  },
  {
    "objectID": "VB_NLLS_activity.html#bounding-parameter-values",
    "href": "VB_NLLS_activity.html#bounding-parameter-values",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Bounding parameter values",
    "text": "Bounding parameter values\nYou can also bound the starting values, i.e., prevent them from exceeding some minimum and maximum value during the NLLS fitting process:\n\nnlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.5, K_M = 0.5))\n\nNonlinear regression model\n  model: V_data ~ V_max * S_data/(K_M + S_data)\n   data: parent.frame()\n V_max    K_M \n13.504  9.408 \n residual sum-of-squares: 48.06\n\nNumber of iterations to convergence: 9 \nAchieved convergence tolerance: 1.49e-08\n\n\n\nnlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.5, K_M = 0.5), lower=c(0.4,0.4), upper=c(100,100))\n\nNonlinear regression model\n  model: V_data ~ V_max * S_data/(K_M + S_data)\n   data: parent.frame()\n V_max    K_M \n13.504  9.408 \n residual sum-of-squares: 48.06\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 1.49e-08\n\n\nSo the solution was found in one lesser iteration (not a spectacular improvement, but an improvement nevertheless).\nHowever, if you bound the parameters too much (to excessively narrow ranges), the algorithm cannot search sufficient parameter space (combinations of parameters), and will fail to converge on a good solution. For example:\n\nnlsLM(V_data ~ V_max * S_data / (K_M + S_data), start =  list(V_max = 0.5, K_M = 0.5), lower=c(0.4,0.4), upper=c(20,20))\n\nNonlinear regression model\n  model: V_data ~ V_max * S_data/(K_M + S_data)\n   data: parent.frame()\nV_max   K_M \n17.21 20.00 \n residual sum-of-squares: 78.58\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 1.49e-08\n\n\nHere the algorithm converged on a poor solution, and in fact took fewer iterations (3) than before to do so. This is because it could not explore sufficient parameter combinations of V_max and K_M as we have narrowed the range that both these parameters could be allowed to take during the optimization too much."
  },
  {
    "objectID": "VB_NLLS_activity.html#diagnostics-of-an-nlls-fit",
    "href": "VB_NLLS_activity.html#diagnostics-of-an-nlls-fit",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Diagnostics of an NLLS fit",
    "text": "Diagnostics of an NLLS fit\nNLLS regression carries the same three key assumptions as Linear models:\n\nNo (in practice, minimal) measurement error in explanatory/independent/predictor variable (x-axis variable)\nData have constant normal variance — errors in the y-axis are homogeneously distributed over the x-axis range\nThe measurement/observation errors are Normally distributed (Gaussian)\n\nAt the very least, it is a good idea to plot the residuals of a fitted NLLS model. Let’s do that for our Michaelis-Menten Model fit:\n\nhist(residuals(MM_model6))\n\n\n\n\nThe residuals look OK. But this should not come as a surprise because we generated these “data” ourselves using normally-distributed errors!"
  },
  {
    "objectID": "VB_NLLS_activity.html#abundance-data-as-an-example",
    "href": "VB_NLLS_activity.html#abundance-data-as-an-example",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Abundance data as an example",
    "text": "Abundance data as an example\nFluctuations in the abundance (density) of single populations may play a crucial role in ecosystem dynamics and emergent functional characteristics, such as rates of carbon fixation or disease transmission. For example, if vector population densities or their traits change at the same or shorter timescales than the rate of disease transmission, then (vector) abundance fluctuations can cause significant fluctuations in disease transmission rates. Indeed, most disease vectors are small ectotherms with short generation times and greater sensitivity to environmental conditions than their (invariably larger, longer-lived, and often, endothermic) hosts. So understanding how populations vary over time, space, and with respect to environmental variables such as temperature and precipitation is key. We will look at fitting models to the growth of a single population here."
  },
  {
    "objectID": "VB_NLLS_activity.html#population-growth-rates",
    "href": "VB_NLLS_activity.html#population-growth-rates",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Population growth rates",
    "text": "Population growth rates\nA population grows exponentially while its abundance is low and resources are not limiting (the Malthusian principle). This growth then slows and eventually stops as resources become limiting. There may also be a time lag before the population growth really takes off at the start. We will focus on microbial (specifically, bacterial) growth rates. Bacterial growth in batch culture follows a distinct set of phases; lag phase, exponential phase and stationary phase. During the lag phase a suite of transcriptional machinery is activated, including genes involved in nutrient uptake and metabolic changes, as bacteria prepare for growth. During the exponential growth phase, bacteria divide at a constant rate, the population doubling with each generation. When the carrying capacity of the media is reached, growth slows and the number of cells in the culture stabilizes, beginning the stationary phase.\nTraditionally, microbial growth rates were measured by plotting cell numbers or culture density against time on a semi-log graph and fitting a straight line through the exponential growth phase – the slope of the line gives the maximum growth rate (r_{max}). Models have since been developed which we can use to describe the whole sigmoidal bacterial growth curve (e.g., using NLLS). Here we will take a look at these different approaches, from applying linear models to the exponential phase, through to fitting non-linear models to the full growth curve.\nLet’s first generate some “data” on the number of bacterial cells as a function of time that we can play with:\n\nt <- seq(0, 22, 2)\nN <- c(32500, 33000, 38000, 105000, 445000, 1430000, 3020000, 4720000, 5670000, 5870000, 5930000, 5940000)\n\nset.seed(1234) # set seed to ensure you always get the same random sequence \n\ndata <- data.frame(t, N * (1 + rnorm(length(t), sd = 0.1))) # add some random error\n\nnames(data) <- c(\"Time\", \"N\")\n\nhead(data)\n\n  Time          N\n1    0   28577.04\n2    2   33915.52\n3    4   42120.88\n4    6   80370.17\n5    8  464096.05\n6   10 1502365.99\n\n\nNote how we added some random “sampling” error using N * (1 + rnorm(length(t), sd = .1)).\nThis means that we are adding an error at each time point t (let’s call this fluctuation \\epsilon_t) as a * percentage * of the population (N_t) at that time point in a vectorized way. That is, we are performing the operation N_t \\times (1 + \\epsilon_t) at all time points at one go. This is important to note because this is often the way that errors appear – proportional to the value being measured.\nNow let’s plot these data:\n\nggplot(data, aes(x = Time, y = N)) + \n geom_point(size = 3) +\n labs(x = \"Time (Hours)\", y = \"Population size (cells)\")\n\n\n\n\n\nBasic approach\nThe size of an exponentially growing population (N) at any given time (t) is given by:\n$ N(t) = N_0 e^{rt}$\nwhere N_0 is the initial population size and r is the growth rate. We can re-arrange this to give:\nr = \\frac{\\log(N(t)) - \\log(N_0)}{t}\nThat is, in exponential growth at a constant rate, the growth rate can be simply calculated as the difference in the log of two population sizes, over time. We will log-transform the data and estimate by eye where growth looks exponential.\n\ndata$LogN <- log(data$N)\n\n# visualise\nggplot(data, aes(x = t, y = LogN)) + \n geom_point(size = 3) +\n labs(x = \"Time (Hours)\", y = \"log(cell number)\")\n\n\n\n\nBy eye the logged data looks fairly linear (beyond the initial “lag phase” of growth; see below) between hours 5 and 10, so we’ll use that time-period to calculate the growth rate.\n\n(data[data$Time == 10,]$LogN - data[data$Time == 6,]$LogN)/(10-6)\n\n[1] 0.7320383\n\n\nThis is our first, most basic estimate of r.\nOr, we can decide not to eyeball the data, but just pick the maximum observed gradient of the curve. For this, we can use the the diff() function:\n\ndiff(data$LogN)\n\n [1]  0.171269154  0.216670872  0.646099643  1.753448393  1.174704941\n [6]  0.639023868  0.449529740  0.181493482 -0.000450184  0.054490710\n[11] -0.054600924\n\n\nThis gives all the (log) population size differences between successive timepoint pairs. The max of this is what we want, divided by the time-step.\n\nmax(diff(data$LogN))/2 # 2 is the difference in any successive pair of timepoints\n\n[1] 0.8767242\n\n\n\n\nUsing OLS\nBut we can do better than this. To account for some error in measurement, we shouldn’t really take the data points directly, but fit a linear model through them instead, where the slope gives our growth rate. This is pretty much the “traditional” way of calculating microbial growth rates – draw a straight line through the linear part of the log-transformed data.\n\nlm_growth <- lm(LogN ~ Time, data = data[data$Time > 2 & data$Time < 12,])\nsummary(lm_growth)\n\n\nCall:\nlm(formula = LogN ~ Time, data = data[data$Time > 2 & data$Time < \n    12, ])\n\nResiduals:\n       3        4        5        6 \n 0.21646 -0.38507  0.12076  0.04785 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   7.9366     0.5350  14.835  0.00451 **\nTime          0.6238     0.0728   8.569  0.01335 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3256 on 2 degrees of freedom\nMultiple R-squared:  0.9735,    Adjusted R-squared:  0.9602 \nF-statistic: 73.42 on 1 and 2 DF,  p-value: 0.01335\n\n\nNpw we get r \\approx 0.62, which is probably closer to the “truth”.\nBut this is still not ideal because we only guessed the exponential phase by eye. We could do it better by iterating through different windows of points, comparing the slopes and finding which the highest is to give the maximum growth rate, r_{max}. This is called a “rolling regression”.\nOr better still, we can fit a more appropriate mathematical model using NLLS!\n\n\nUsing NLLS\nFor starters, a classical, (somewhat) mechanistic model is the logistic equation:\n\nN_t = \\frac{N_0 K e^{r t}}{K + N_0 (e^{r t} - 1)}\n\nHere N_t is population size at time t, N_0 is initial population size, r is maximum growth rate (AKA r_\\text{max}), and K is carrying capacity (maximum possible abundance of the population). Note that this model is actually the solution to the differential equation that defines the classic logistic population growth model.\nLet’s fit it to the data. First, we need to define it as a function object:\n\nlogistic_model <- function(t, r_max, K, N_0){ # The classic logistic equation\n return(N_0 * K * exp(r_max * t)/(K + N_0 * (exp(r_max * t) - 1)))\n}\n\nNow fit it:\n\n# first we need some starting parameters for the model\nN_0_start <- min(data$N) # lowest population size\nK_start <- max(data$N) # highest population size\nr_max_start <- 0.62 # use our estimate from the OLS fitting from above\n\nfit_logistic <- nlsLM(N ~ logistic_model(t = Time, r_max, K, N_0), data,\n      list(r_max=r_max_start, N_0 = N_0_start, K = K_start))\n\nsummary(fit_logistic)\n\n\nFormula: N ~ logistic_model(t = Time, r_max, K, N_0)\n\nParameters:\n       Estimate Std. Error t value Pr(>|t|)    \nr_max 6.309e-01  3.791e-02  16.641 4.56e-08 ***\nN_0   3.317e+03  1.451e+03   2.286   0.0481 *  \nK     5.538e+06  7.192e+04  76.995 5.32e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 119200 on 9 degrees of freedom\n\nNumber of iterations to convergence: 12 \nAchieved convergence tolerance: 1.49e-08\n\n\nWe did not pay much attention to what starting values we used in the simpler example of fitting the allometric model because the power-law model is easy to fit using NLLS, and starting far from the optimal parameters does not matter too much. Here, we used the actual data to generate more realistic start values for each of the three parameters (r_max, N_0, K) of the Logistic equation.\nNow, plot the fit:\n\ntimepoints <- seq(0, 22, 0.1)\n\nlogistic_points <- logistic_model(t = timepoints, \n         r_max = coef(fit_logistic)[\"r_max\"], \n         K = coef(fit_logistic)[\"K\"], \n         N_0 = coef(fit_logistic)[\"N_0\"])\ndf1 <- data.frame(timepoints, logistic_points)\ndf1$model <- \"Logistic equation\"\nnames(df1) <- c(\"Time\", \"N\", \"model\")\n\nggplot(data, aes(x = Time, y = N)) +\n geom_point(size = 3) +\n geom_line(data = df1, aes(x = Time, y = N, col = model), size = 1) +\n theme(aspect.ratio=1)+ # make the plot square \n labs(x = \"Time\", y = \"Cell number\")\n\n\n\n\nThat looks nice, and the r_{max} estimate we get (0.64) is fairly close to what we got above with OLS fitting.\nNote that we’ve done this fitting to the original non transformed data, whilst the linear regressions earlier were on log transformed data. What would this function look like on a log-transformed axis?\n\nggplot(data, aes(x = Time, y = LogN)) +\n geom_point(size = 3) +\n geom_line(data = df1, aes(x = Time, y = log(N), col = model), size = 1) +\n theme(aspect.ratio=1)+ \n labs(x = \"Time\", y = \"log(Cell number)\")\n\n\n\n\nThe model actually diverges from the data at the lower end! This was not visible in the previous plot where you examined the model in linear scale (without taking a log) because the deviation of the model is small, and only becomes clear in the log scale. This is because of the way logarithms work. Let’s have a look at this in our Cell counts “data”:\n\nggplot(data, aes(x = N, y = LogN)) +\n geom_point(size = 3) +\n theme(aspect.ratio = 1)+ \n labs(x = \"N\", y = \"log(N)\")\n\n\n\n\nAs you can see the logarithm is a strongly nonlinear transformation of any sequence of real numbers, with small numbers close to zero yielding disproportionately large deviations.\n\nYou may play with increasing the error (by increasing the value of `sd` in synthetic data generation step above) and re-evaluating all the subsequent model fitting steps above. However, note that above some values of `sd`, you will start to get negative values of populations, especially at early time points, which will raise issues with taking a logarithm.\n\nThe above seen deviation of the Logistic model from the data is because this model assumes that the population is growing right from the start (Time = 0), while in “reality” (in our synthetic “data”), this is not what’s happening; the population takes a while to grow truly exponentially (i.e., there is a time lag in the population growth). This time lag is seen frequently in the lab, and is also expected in nature, because when bacteria encounter fresh growth media (in the lab) or a new resource/environment (in the field), they take some time to acclimate, activating genes involved in nutrient uptake and metabolic processes, before beginning exponential growth. This is called the lag phase and can be seen in our example data where exponential growth doesn’t properly begin until around the 4th hour.\nTo capture the lag phase, more complicated bacterial growth models have been designed.\nOne of these is the modified Gompertz model (Zwietering et. al., 1990), which has been used frequently in the literature to model bacterial growth:\n\n  \\log(N_t) = N_0 + (N_{max} - N_0) e^{-e^{r_{max} \\exp(1) \\frac{t_{lag} - t}{(N_{max} - N_0) \\log(10)} + 1}}\n\nHere maximum growth rate (r_{max}) is the tangent to the inflection point, t_{lag} is the x-axis intercept to this tangent (duration of the delay before the population starts growing exponentially) and \\log\\left(\\frac{N_{max}}{N_0}\\right) is the asymptote of the log-transformed population growth trajectory, i.e., the log ratio of maximum population density N_{max} (aka “carrying capacity”) and initial cell (Population) N_0 density.\n\nNote that unlike the Logistic growth model above, the Gompertz model is in the log scale. This is because the model is not derived from a differential equation, but was designed * specifically * to be fitted to log-transformed data.\n\nNow let’s fit and compare the two alternative nonlinear growth models: Logistic and Gompertz.\nFirst, specify the function object for the Gompertz model (we already defined the function for the Logistic model above):\n\ngompertz_model <- function(t, r_max, K, N_0, t_lag){ # Modified gompertz growth model (Zwietering 1990)\n return(N_0 + (K - N_0) * exp(-exp(r_max * exp(1) * (t_lag - t)/((K - N_0) * log(10)) + 1)))\n}   \n\nAgain, note that unlike the Logistic growth function above, this function has been written in the log scale.\nNow let’s generate some starting values for the NLLS fitting of the Gompertz model.\nAs we did above for the logistic equation, let’s derive the starting values by using the actual data:\n\nN_0_start <- min(data$LogN) # lowest population size, note log scale\nK_start <- max(data$LogN) # highest population size, note log scale\nr_max_start <- 0.62 # use our previous estimate from the OLS fitting from above\nt_lag_start <- data$Time[which.max(diff(diff(data$LogN)))] # find last timepoint of lag phase\n\n\nSo how did we find a reasonable time lag from the data? *\n\nLet’s break the last command down:\n\ndiff(data$LogN) # same as what we did above - get differentials\n\n [1]  0.171269154  0.216670872  0.646099643  1.753448393  1.174704941\n [6]  0.639023868  0.449529740  0.181493482 -0.000450184  0.054490710\n[11] -0.054600924\n\n\n\ndiff(diff(data$LogN)) # get the differentials of the differentials (approx 2nd order derivatives)\n\n [1]  0.04540172  0.42942877  1.10734875 -0.57874345 -0.53568107 -0.18949413\n [7] -0.26803626 -0.18194367  0.05494089 -0.10909163\n\n\n\nwhich.max(diff(diff(data$LogN))) # find the timepoint where this 2nd order derivative really takes off \n\n[1] 3\n\n\n\ndata$Time[which.max(diff(diff(data$LogN)))] # This then is a good guess for the last timepoint of the lag phase\n\n[1] 4\n\n\nNow fit the model using these start values:\n\nfit_gompertz <- nlsLM(LogN ~ gompertz_model(t = Time, r_max, K, N_0, t_lag), data,\n      list(t_lag=t_lag_start, r_max=r_max_start, N_0 = N_0_start, K = K_start))\n\nYou might one or more warning(s) that the model fitting iterations generated NaNs during the fitting procedure for these data (because at some point the NLLS fitting algorithm “wandered” to a combination of K and N_0 values that yields a NaN for log(K/N_0)).\nYou can ignore these warning in this case. But not always – sometimes these NaNs mean that the equation is wrongly written, or that it generates NaNs across the whole range of the x-values, in which case the model is inappropriate for these data.\nGet the model summary:\n\nsummary(fit_gompertz)\n\n\nFormula: LogN ~ gompertz_model(t = Time, r_max, K, N_0, t_lag)\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \nt_lag  4.80680    0.18433   26.08 5.02e-09 ***\nr_max  1.86616    0.08749   21.33 2.45e-08 ***\nN_0   10.39142    0.05998  173.24 1.38e-15 ***\nK     15.54956    0.05056  307.57  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09418 on 8 degrees of freedom\n\nNumber of iterations to convergence: 10 \nAchieved convergence tolerance: 1.49e-08\n\n\nAnd see how the fits of the two nonlinear models compare:\n\ntimepoints <- seq(0, 24, 0.1)\n\nlogistic_points <- log(logistic_model(t = timepoints, \n          r_max = coef(fit_logistic)[\"r_max\"], \n          K = coef(fit_logistic)[\"K\"], \n          N_0 = coef(fit_logistic)[\"N_0\"]))\n\ngompertz_points <- gompertz_model(t = timepoints, \n         r_max = coef(fit_gompertz)[\"r_max\"], \n         K = coef(fit_gompertz)[\"K\"], \n         N_0 = coef(fit_gompertz)[\"N_0\"], \n         t_lag = coef(fit_gompertz)[\"t_lag\"])\n\ndf1 <- data.frame(timepoints, logistic_points)\ndf1$model <- \"Logistic model\"\nnames(df1) <- c(\"Time\", \"LogN\", \"model\")\n\ndf2 <- data.frame(timepoints, gompertz_points)\ndf2$model <- \"Gompertz model\"\nnames(df2) <- c(\"Time\", \"LogN\", \"model\")\n\nmodel_frame <- rbind(df1, df2)\n\nggplot(data, aes(x = Time, y = LogN)) +\n geom_point(size = 3) +\n geom_line(data = model_frame, aes(x = Time, y = LogN, col = model), size = 1) +\n theme_bw() + # make the background white\n theme(aspect.ratio=1)+ # make the plot square \n labs(x = \"Time\", y = \"log(Abundance)\")\n\n\n\n\nClearly, the Gompertz model fits way better than the logistic growth equation in this case! Note also that there is a big difference in the fitted value of r_{max} from the two models; the value is much lower from the Logistic model because it ignores the lag phase, including it into the exponential growth phase.\nYou can now perform model selection like you did above in the allometric scaling example."
  },
  {
    "objectID": "VB_NLLS_activity.html#some-tips-and-tricks-for-nlls-fitting",
    "href": "VB_NLLS_activity.html#some-tips-and-tricks-for-nlls-fitting",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Some tips and tricks for NLLS fitting",
    "text": "Some tips and tricks for NLLS fitting\n\nStarting values\nThe main challenge for NLLS fitting is finding starting (initial) values for the parameters, which the algorithm needs to proceed with the fitting/optimization. Inappropriate starting values can result in the algorithm finding parameter combinations represent convergence to a local optimum rather than the (globally) optimal solution. Starting parameter estimates can also result in or complete “divergence”, i.e., the search results in a combination of parameters that cause mathematical “singularity” (e.g., log(0) or division by zero).\n\nObtaining them\nFinding the starting values is a bit of an art. There is no method for finding starting values that works universally (across different types of models).\nThe one universal rule though, is that finding starting values requires you to understand the meaning of each of the parameters in your model.\nFurthermore, you will typically need to determine starting values specific to each model and each dataset that that you are wanting to fit that model to. To do so, understanding how each parameter in the model corresponds to features of the actual data is really necessary.\nFor example, in the Gompertz population growth rate model (eqn. {eq}eq:Gompertz), your starting values generator function would, for each dataset,\n* Calculate a starting value for r_{max} by searching for the steepest slope of the growth curve (e.g., with a rolling OLS regression) * Calculate a starting value of t_{lag} by intersecting the fitted line with the x (time)-axis * Calculate a starting value for the asymptote K as the highest data (abundance) value in the dataset.\n\nIdeally, you should write a separate a function that calculates starting values for the model parameters.\n\n\n\nSampling them\nOnce you have worked out how to generate starting values for each non-linear model and dataset, a good next step for optimizing the fitting across multiple datasets (and thus maximize how many datasets are successfully fitted to the model) is to rerun fitting attempts multiple times, sampling each of the starting values (simultaneously) randomly (that is, randomly vary the set of starting values a bit each time). This sampling of starting values will increase the likelihood of the NLLS optimization algorithm finding a solution (optimal combination of parameters), and not getting stuck in a combination of parameters somewhere far away from that optimal solution. \nIn particular, * You can choose a Gaussian/Normal distribution if you have high confidence in mean value of parameter, or * You can uniform distribution if you have low confidence in the mean, but higher confidence in the range of values that the parameter can take. In both cases, the mean of the sampling distribution will be the starting value you inferred from the model and the data (previous section).\nFurthermore, * Whichever distribution you choose (gaussian vs uniform), you will need to determine what range of values to restrict each parameter’s samples to. In the case of the normal distribution, this is determined by what standard deviation parameter (you choose), and in the case of the uniform distribution, this is determined by what lower and upper bound (you choose). Generally, a good approach is to set the bound to be some percent (say 5-10%) of the parameter’s (mean) starting value. In both cases the chosen range to restrict the sampling to would typically be some subset of the model’s parameter bounds (next section).\n* How many times to re-run the fitting for a single dataset and model?* – this depends on how “difficult” the model is, and how much computational power you have.\nYou may also try and use a more sophisticated approach such as grid searching for varying your starting values randomly.\n\n\n\nBounding parameters revisited\nAt the start, we looked at an exampleof NLLS fitting where we bounded the parameters. It can be a good idea to restrict the range of values that the each of the model’s parameters can take during any one fitting/optimization run. To “bound” a parameter in this way means to give it upper and lower limits. By doing so, during one optimization/fitting (e.g., one call to nlsLM, to fit one model, to one dataset), the fitting algorithm does not allow a parameter to go outside some limits. This reduces the chances of the optimization getting stuck too far from the solution, or failing completely due to some mathematical singularity (e.g., log(0)).\nThe bounds are typically fixed for each parameter of a model at the level of the model (e.g., they do not change based on each dataset). For example, in the Gompertz model for growth rates (eqn. {eq}eq:Gompertz), you can limit the growth rate parameter to never be negative (the bounds would be [0,\\infty]), or restrict it further to be some value between zero and an upper limit (say, 10) that you know organismal growth rates cannot exceed (the bounds would in this case would be [0,10]).\nHowever, as we saw in the Michaelis-Menten model fitting example, bounding the parameters too much (excessively narrow ranges) can result in poor solutions because the algorithm cannot explore sufficient parameter space.\n\nThe values of the parameter bounds you choose, of course, may depend on the *units of measurement* of the data. For example, in [SI](https://en.wikipedia.org/wiki/International_System_of_Units), growth rates in the Logistic or Gompertz models would be in units of 1/X).\n\nIrrespective of which computer language the NLLS fitting algorithm is implemented in (nlsLM  in R or lmfit in Python), the fitting command/method will have options for setting the parameter bounds. In particular,\n\nFor nlsLM in R, look up https://www.rdocumentation.org/packages/minpack.lm/versions/1.2-1/topics/nlsLM (the lower and upper arguments to the function). \nFor lmfit in Python, look up https://lmfit.github.io/lmfit-py/parameters.html (and in particular, https://lmfit.github.io/lmfit-py/parameters.html#lmfit.parameter.Parameter) (the min and max arguments).\n\nBounding the parameter values has nothing to do, per se, with sampling the starting values of each parameter, though if you choose to sample starting values (explained in previous section), you need to make sure that the samples don’t exceed the pre-set bounds (explained in this section).\n\nPython's `lmfit` has an option to also internally vary the parameter. So by using a sampling approach as described in the previous section, *and* allowing the parameter to vary (note that `vary=True` is the default) within `lmfit`, you will be in essence be imposing sampling twice. This may or may not improve fitting performance &ndash; try it out both ways.\n\n\nrm(list=ls())\ngraphics.off()\n\nNow that we have the background requirements going, we can start using the rTPC package (keep in mind in order to load the rTPC package you will need to use:\n\n# remotes::install_github(\"padpadpadpad/rTPC\") ## to install rTPC\n\nLets look through the different models available!\n\n#take a look at the different models available\nget_model_names()\n\n [1] \"beta_2012\"             \"boatman_2017\"          \"briere2_1999\"         \n [4] \"delong_2017\"           \"flinn_1991\"            \"gaussian_1987\"        \n [7] \"hinshelwood_1947\"      \"joehnk_2008\"           \"johnsonlewin_1946\"    \n[10] \"kamykowski_1985\"       \"lactin2_1995\"          \"lrf_1991\"             \n[13] \"modifiedgaussian_2006\" \"oneill_1972\"           \"pawar_2018\"           \n[16] \"quadratic_2008\"        \"ratkowsky_1983\"        \"rezende_2019\"         \n[19] \"sharpeschoolfull_1981\" \"sharpeschoolhigh_1981\" \"sharpeschoollow_1981\" \n[22] \"spain_1982\"            \"thomas_2012\"           \"thomas_2017\"          \n[25] \"weibull_1995\"         \n\n\nThere are 24 models to choose from. For our purposes in this section we will be using the sharpesschoolhigh_1981 model. More information on the model can be found here.\nFrom here lets load in our data from the overall repository. This will be called csm7I.csv.\nThis is from the larger dataset reduced to a single trait. This data comes from the VectorBiTE database and so has unique IDs. We will use this to get our species and trait of interest isolated from the larger dataset. In this example we will be looking at Development Rate across temperatures for Aedes albopictus, which we can find an example of in csm7I.\n\nrequire(tidyverse)\ndf <- read_csv(\"activities/data/csm7I.csv\")\n\nNew names:\nRows: 11 Columns: 8\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(5): originalid, originaltraitname, originaltraitunit, interactor1, cita... dbl\n(3): ...1, originaltraitvalue, ambienttemp\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\ndf1 <- df %>%\n  dplyr::select('originalid', 'originaltraitname', 'originaltraitunit', 'originaltraitvalue', 'interactor1', 'ambienttemp', 'citation')\ndf2 <- as_tibble(df1)\n\nNow lets visualize our data in ggplot.\n\n#visualize\nggplot(df2, aes(ambienttemp, originaltraitvalue))+\n  geom_point()+\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Development Rate',\n       title = 'Development rate across temperatures for Aedes albopictus')\n\n\n\n\nWe will need to write which model we are using (sharpschoolhigh_1981). From here we can actually build our fit. We will use ‘’nls_multstart’’ to automatically find our starting values. This lets us skip the starting value problem. From here we build our predicted line.\n\n# choose model\nmod = 'sharpschoolhigh_1981'\nd<- df2 %>%\n  rename(temp = ambienttemp,\n         rate = originaltraitvalue)\n\n\n# fit Sharpe-Schoolfield model\nd_fit <- nest(d, data = c(temp, rate)) %>%\n  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                                                     data = .x,\n                                                     iter = c(3,3,3,3),\n                                                     start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,\n                                                     start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,\n                                                     lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                                                     upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),\n                                                     supp_errors = 'Y',\n                                                     convergence_count = FALSE)),\n         \n         # create new temperature data\n         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),\n         # predict over that data,\n         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))\n\n\n# unnest predictions\nd_preds <- dplyr::select(d_fit, preds) %>%\n  unnest(preds)\n\nLets visualize the line:\n\n# plot data and predictions\nggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n\n\n\nThis looks like a good fit! We can start exploring using bootstrapping. Lets start with refitting the model using nlsLM.\n\n# refit model using nlsLM\nfit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),\n                               data = d,\n                               start = coef(d_fit$sharpeschoolhigh[[1]]),\n                               lower = get_lower_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                               upper = get_upper_lims(d$temp, d$rate, model_name = 'sharpeschoolhigh_1981'),\n                               weights = rep(1, times = nrow(d)))\n\nNow we can actually bootstrap.\n\n# bootstrap using case resampling\nboot1 <- Boot(fit_nlsLM, method = 'case')\n\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\n\n\nIt is a good idea to explore the data again now.\n\n# look at the data\nhead(boot1$t)\n\n         r_tref        e       eh       th\n[1,] 0.07156528 1.127725 1.569427 31.16678\n[2,] 0.07428071 1.101990 1.460402 31.08500\n[3,] 0.08116097 0.836633 2.145525 36.54408\n[4,] 0.07897508 1.344105 1.338908 25.50450\n[5,] 0.07328032 1.179788 1.422640 29.40599\n[6,] 0.07039726 1.107672 1.653070 31.40087\n\n\n\nhist(boot1, layout = c(2,2))\n\nWarning in norm.inter(t, adj.alpha): extreme order statistics used as endpoints\n\nWarning in norm.inter(t, adj.alpha): extreme order statistics used as endpoints\n\n\n\n\n\nNow we use the bootstrapped model to build predictions which we can explore visually.\n\n# create predictions of each bootstrapped model\nboot1_preds <- boot1$t %>%\n  as.data.frame() %>%\n  drop_na() %>%\n  mutate(iter = 1:n()) %>%\n  group_by_all() %>%\n  do(data.frame(temp = seq(min(d$temp), max(d$temp), length.out = 100))) %>%\n  ungroup() %>%\n  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))\n\n\n# calculate bootstrapped confidence intervals\nboot1_conf_preds <- group_by(boot1_preds, temp) %>%\n  summarise(conf_lower = quantile(pred, 0.025),\n            conf_upper = quantile(pred, 0.975)) %>%\n  ungroup()\n\n\n# plot bootstrapped CIs\np1 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper), boot1_conf_preds, fill = 'blue', alpha = 0.3) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n# plot bootstrapped predictions\np2 <- ggplot() +\n  geom_line(aes(temp, .fitted), d_preds, col = 'blue') +\n  geom_line(aes(temp, pred, group = iter), boot1_preds, col = 'blue', alpha = 0.007) +\n  geom_point(aes(temp, rate), d, size = 2, alpha = 0.5) +\n  theme_bw(base_size = 12) +\n  labs(x = 'Temperature (ºC)',\n       y = 'Growth rate',\n       title = 'Growth rate across temperatures')\n\n\np1 + p2\n\n\n\n\nWe can see here that when we bootstrap this data, the fit is not as good as we would expect from the initial exploration. We do not necessarily get a good thermal optima from this data with confidence intervals from this data.\nLets look at a second example:"
  },
  {
    "objectID": "VB_NLLS_activity.html#readings-and-resources",
    "href": "VB_NLLS_activity.html#readings-and-resources",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Readings and Resources",
    "text": "Readings and Resources\n\nMotulsky, Harvey, and Arthur Christopoulos. Fitting models to biological data using linear and nonlinear regression: a practical guide to curve fitting. OUP USA, 2004: https://www.facm.ucl.ac.be/cooperation/Vietnam/WBI-Vietnam-October-2011/Modelling/RegressionBook.pdf\nThese are a pretty good series of notes on NLLS (even if you are using R instead of Python): https://lmfit.github.io/lmfit-py/intro.html\nAnother technical description of NLLS algorithms: https://www.gnu.org/software/gsl/doc/html/nls.html\nJohnson, J. B. & Omland, K. S. 2004 Model selection in ecology and evolution. Trends Ecol. Evol. 19, 101–108.\nThe nlstools package for NLLS fit diagnostics: https://rdrr.io/rforge/nlstools\n\nThe original paper: http://dx.doi.org/10.18637/jss.v066.i05"
  },
  {
    "objectID": "VB_NLLS_activity.html#practice-problem-for-nlls",
    "href": "VB_NLLS_activity.html#practice-problem-for-nlls",
    "title": "Non-Linear Least Squares (NLLS) and Thermal Performance Curve Fitting with Bootstrapping",
    "section": "Practice Problem for NLLS",
    "text": "Practice Problem for NLLS\n\nThe nllsdataset.csv dataset contains values on trait-temperature relationships in three important vector/pest species. Fit thermal performance curves to trait(s) of interest. Use bootstrapping to generate confidence bounds for each curve."
  },
  {
    "objectID": "VB_Bayes1.html#learning-objectives",
    "href": "VB_Bayes1.html#learning-objectives",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the basic principles underlying Bayesian modeling methodology\nIntroduce how to use Bayesian inference for real-world problems\nIntroduce computation tools to perform inference for simple models in R (how to turn the Bayesian crank)"
  },
  {
    "objectID": "VB_Bayes1.html#what-is-bayesian-inference",
    "href": "VB_Bayes1.html#what-is-bayesian-inference",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "What is Bayesian Inference?",
    "text": "What is Bayesian Inference?\nIn the Bayesian approach our probabilities numerically represent rational beliefs.\n\nBayes rule provides a rational method for updating those beliefs in light of new information and incorporating/quantifying uncertainty in those beliefs.\n\nThus, Bayesian inference is an approach for understanding data inductively."
  },
  {
    "objectID": "VB_Bayes1.html#recall-bayes-theorem",
    "href": "VB_Bayes1.html#recall-bayes-theorem",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Recall: Bayes Theorem",
    "text": "Recall: Bayes Theorem\nBayes Theorem allows us to relate the conditional probabilities of two events \\(A\\) and \\(B\\): \\[\n\\text{Pr}(A|B) = \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#what-is-bayesian-inference-1",
    "href": "VB_Bayes1.html#what-is-bayesian-inference-1",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "What is Bayesian Inference?",
    "text": "What is Bayesian Inference?\nWe can re-write Bayes rule in terms of our parameters, \\(\\theta\\) and our data, \\(Y\\): \\[\\begin{align*}\n\\text{Pr}(\\theta|Y) & = \\frac{\\text{Pr}(Y|\\theta)\\text{Pr}(\\theta)}{\\text{Pr}(Y)}\n\\end{align*}\\]\nThe LHS is the main quantity of interest in a Bayesian analysis, the posterior, denoted \\(f(\\theta|Y)\\): \\[\n\\overbrace{f(\\theta|Y)}^\\text{Posterior} \\propto \\overbrace{\\mathcal{L}(\\theta; Y)}^\\text{Likelihood} \\times \\overbrace{f(\\theta)}^\\text{Prior}\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#bayesian-methods-provide",
    "href": "VB_Bayes1.html#bayesian-methods-provide",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Bayesian methods provide",
    "text": "Bayesian methods provide\n\nmodels for rational, quantitative learning\nparameter estimates with good statistical properties\nestimators that work for small and large sample sizes\nparsimonious descriptions of data, predictions for missing data, and forecasts for future data\na coherent computational framework for model estimation, selection and validation"
  },
  {
    "objectID": "VB_Bayes1.html#classical-vs-bayesian",
    "href": "VB_Bayes1.html#classical-vs-bayesian",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Classical vs Bayesian",
    "text": "Classical vs Bayesian\nThe fundamental differences between classical and Bayesian methods is what is fixed and what is random in an analysis.\n\n\n\n\nParadigm\nFixed\nRandom\n\n\n\n\nClassical\nparam (\\(\\theta\\))\ndata (\\(Y\\))\n\n\nBayesian\ndata (\\(Y\\))\nparam (\\(\\theta\\))"
  },
  {
    "objectID": "VB_Bayes1.html#whywhy-not-bayesian-statistics",
    "href": "VB_Bayes1.html#whywhy-not-bayesian-statistics",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Why/Why Not Bayesian Statistics?",
    "text": "Why/Why Not Bayesian Statistics?\n\nPros\n\nIf \\(f(\\theta)\\) & \\(\\mathcal{L}(\\theta; Y)\\) represent a rational person’s beliefs, then Bayes’ rule is an optimal method of updating these beliefs given new info (Cox 1946, 1961; Savage 1954; 1972).\nProvides more intuitive answers in terms of the probability that parameters have particular values.\nIn many complicated statistical problems there are no obvious non-Bayesian inference methods."
  },
  {
    "objectID": "VB_Bayes1.html#whywhy-not-bayesian-statistics-1",
    "href": "VB_Bayes1.html#whywhy-not-bayesian-statistics-1",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Why/Why Not Bayesian Statistics?",
    "text": "Why/Why Not Bayesian Statistics?\n\nCons\n\nIt can be hard to mathematically formulate prior beliefs (choice of \\(f(\\theta)\\) often ad hoc or for computational reasons).\nPosterior distributions can be sensitive to prior choice.\nAnalyses can be computationally costly."
  },
  {
    "objectID": "VB_Bayes1.html#steps-to-making-inference",
    "href": "VB_Bayes1.html#steps-to-making-inference",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Steps to Making Inference",
    "text": "Steps to Making Inference\n\nResearch question\nData collection\nModel \\(Y_i \\approx f(X_i)\\)\nEstimate the parameter in the model with uncertainty\nMake inference\n\nThe difference between Classical and Bayesian lies in step 4:\n\nClassical uses maximum likelihood estimatation\n\nBayesian derives a posterior distribution."
  },
  {
    "objectID": "VB_Bayes1.html#example-estimating-the-probability-of-a-rare-event",
    "href": "VB_Bayes1.html#example-estimating-the-probability-of-a-rare-event",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: Estimating the probability of a rare event",
    "text": "Example: Estimating the probability of a rare event\nSuppose we are interested in the prevalence of an infectious disease in a small city. A small random sample of 20 individuals will be checked for infection.\n\nInterest is in the fraction of infected individuals \\[\n\\theta \\in \\Theta =[0,1]\n\\]\nThe data records the number of infected individuals \\[\ny \\in \\mathcal{Y} =\\{0,1, \\ldots, 20\\}\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-likelihoodsampling-model",
    "href": "VB_Bayes1.html#example-likelihoodsampling-model",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: Likelihood/sampling model",
    "text": "Example: Likelihood/sampling model\nBefore the sample is obtained, the number of infected individuals is unknown.\n\nLet \\(Y\\) denote this to-be-determined value\nIf \\(\\theta\\) were known, a sensible sampling model is \\[\nY|\\theta \\sim  \\mathrm{Bin} (20, \\theta)\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-prior",
    "href": "VB_Bayes1.html#example-prior",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: Prior",
    "text": "Example: Prior\nOther studies from various parts of the country indicate that the infection rate ranges from about 0.05 to 0.20, with an average prevalence of 0.1.\n\nMoment matching from a beta distribution (a convenient choice) gives the prior \\(\\theta \\sim \\mathrm{Beta} (2,20)\\)"
  },
  {
    "objectID": "VB_Bayes1.html#example-posterior",
    "href": "VB_Bayes1.html#example-posterior",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: Posterior",
    "text": "Example: Posterior\nThe prior and sample model combination: \\[\\begin{align*}\n\\theta & \\sim  \\mathrm{Beta} (a,b) \\\\\nY|\\theta &  \\sim  \\mathrm{Bin} (n, \\theta)\n\\end{align*}\\] and an observed \\(y\\) (the data), leads to the posterior \\[\np(\\theta|y)= \\mathrm{Beta}(a+y, b+n-y)\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-posterior-1",
    "href": "VB_Bayes1.html#example-posterior-1",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: Posterior",
    "text": "Example: Posterior\nFor our case, we have \\(a=2\\), \\(b=20\\), \\(n=20\\).\nIf we don’t find any infections (\\(y=0\\)) our posterior is: \\[\np(\\theta |y=0)= \\mathrm{Beta}(2, 40)\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-sensitivity-analysis",
    "href": "VB_Bayes1.html#example-sensitivity-analysis",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: Sensitivity Analysis",
    "text": "Example: Sensitivity Analysis\nHow influential is our prior? The posterior expectation is \\[\n\\mathrm{E}\\{\\theta|Y=y\\} =   \\frac{n}{w+n} \\bar{y} + \\frac{w}{w+n} \\theta_0\n\\] a weighted average of the sample mean and the prior expectation: \\[\\begin{align*}\n\\theta_0 & =  \\frac{a}{a+b} ~~~~ \\rightarrow \\text{ prior expectation (or guess)} \\\\\nw & = a + b  ~~~~ \\rightarrow \\text{  prior confidence}\n\\end{align*}\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-a-non-bayesian-approach",
    "href": "VB_Bayes1.html#example-a-non-bayesian-approach",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: A non-Bayesian approach",
    "text": "Example: A non-Bayesian approach\nA standard estimate of a population proportion, \\(\\theta\\) is the sample mean \\(\\bar{y} = y/n\\). If \\(y=0 \\rightarrow \\bar{y} = 0\\).\n\nUnderstanding the sampling uncertainty is crucial (e.g., for reporting to health officials).\nThe most popular 95% confidence interval for a population proportion is the Wald Interval: \\[\n\\bar{y} \\pm 1.96 \\sqrt{\\bar{y}(1-\\bar{y})/n}.\n\\] This has the correct asymptotic coverage, but \\(y=0\\) is still problematic!"
  },
  {
    "objectID": "VB_Bayes1.html#conjugate-bayesian-models",
    "href": "VB_Bayes1.html#conjugate-bayesian-models",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Conjugate Bayesian Models",
    "text": "Conjugate Bayesian Models\nSome sets of priors/likelihoods/posteriors exhibit a special relationship called conjugacy: when posterior and prior distributions have the same form.\nE.g., in our Beta-Binomial/Bernoilli example: \\[\\begin{align*}\n\\theta & \\sim  \\mathrm{Beta} (a,b) \\\\\nY|\\theta &  \\sim  \\mathrm{Bin} (n, \\theta) \\\\\n\\theta | Y & \\sim  \\mathrm{Beta}(a^*, b^*)\n\\end{align*}\\]"
  },
  {
    "objectID": "VB_Bayes1.html#are-all-posteriors-in-the-same-family-as-the-priors-no",
    "href": "VB_Bayes1.html#are-all-posteriors-in-the-same-family-as-the-priors-no",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Are all posteriors in the same family as the priors? No",
    "text": "Are all posteriors in the same family as the priors? No\n\nConjugacy is a nice special property, but most of the time this isn’t the case.\n\nUsually getting an analytic form of the posterior distribution can be hard or impossible."
  },
  {
    "objectID": "VB_Bayes1.html#what-do-you-do-with-a-posterior",
    "href": "VB_Bayes1.html#what-do-you-do-with-a-posterior",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "What do you do with a Posterior?",
    "text": "What do you do with a Posterior?\n\nSummarize important aspects of the posterior\n\nmean, median, mode, variance…\n\nCheck sensitivity of posterior to prior choice\nSay what range of parameters is consistent with the observed data given our prior information\nMake predictions"
  },
  {
    "objectID": "VB_Bayes1.html#posterior-summaries-point",
    "href": "VB_Bayes1.html#posterior-summaries-point",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Posterior Summaries (point)",
    "text": "Posterior Summaries (point)\nFor the Beta-Binomial model, we found that \\[\np(\\theta | y)= \\mathrm{Beta}(a+y, b+n-y).\n\\] We can calculate multiple summaries exactly, for example: \\[\\begin{align*}\n\\mathrm{mean}= \\mathrm{E}[\\theta|Y] & = \\frac{a+y}{a+b+n} \\\\\n\\mathrm{mode}(\\theta|Y) & = \\frac{a+y-1}{a+b+n-2} ~~~ \\dagger  \n\\end{align*}\\]\n \\(\\dagger\\) a.k.a. the maximum a posteriori estimator (MAP)"
  },
  {
    "objectID": "VB_Bayes1.html#prior-sensitivity",
    "href": "VB_Bayes1.html#prior-sensitivity",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Prior Sensitivity",
    "text": "Prior Sensitivity\nThe posterior expectation can be written as a weighted average of information from the prior and the data \\[\n\\mathrm{E}\\{\\theta |Y=y\\} =   \\frac{n}{a + b +n} \\bar{y} + \\frac{a+b}{a+b+n} \\theta_0.\n\\] Thus \\(a\\) and \\(b\\) can be interpreted here as prior data where \\(a\\) is the number of prior successes and \\(a+b\\) is the prior sample size. When \\(n\\gg a+b\\) most of our information comes from the data instead of the prior."
  },
  {
    "objectID": "VB_Bayes1.html#visualizing-the-prior-vs.-posterior",
    "href": "VB_Bayes1.html#visualizing-the-prior-vs.-posterior",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Visualizing the prior vs. posterior",
    "text": "Visualizing the prior vs. posterior\nWe can also visually check for sensitivity, since we don’t have general analytic approaches."
  },
  {
    "objectID": "VB_Bayes1.html#confidence-regions",
    "href": "VB_Bayes1.html#confidence-regions",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Confidence Regions",
    "text": "Confidence Regions\nAn interval \\([l(y), u(y)]\\), based on the observed data \\(Y=y\\), has 95 % Bayesian coverage for \\(\\theta\\) if \\[\nP(l(y) <\\theta < u(y)|Y=y)=0.95\n\\] The interpretation: it describes your information about the true value of \\(\\theta\\) after you have observed \\(Y=y\\).\n Such intervals are typically called credible intervals, to distinguish them from frequentist confidence intervals. Both are referred to as CIs."
  },
  {
    "objectID": "VB_Bayes1.html#quantile-based-bayesian-ci",
    "href": "VB_Bayes1.html#quantile-based-bayesian-ci",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Quantile-based (Bayesian) CI",
    "text": "Quantile-based (Bayesian) CI\nPerhaps the easiest way to obtain a credible interval is to use the posterior quantiles.\nTo make a \\(100 \\times (1-\\alpha)\\) % quantile-based CI, find numbers \\(\\theta_{\\alpha/2}<\\theta_{1- \\alpha/2}\\) such that\n\n\\(P(\\theta <\\theta_{\\alpha/2} |Y=y)=\\alpha/2\\)\n\\(P(\\theta >\\theta_{1-\\alpha/2} |Y=y)=\\alpha/2\\)\n\nThe numbers \\(\\theta_{\\alpha/2},\\theta_{1- \\alpha/2}\\) are the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) posterior quantiles of \\(\\theta\\)."
  },
  {
    "objectID": "VB_Bayes1.html#example-binomial-sampling-uniform-prior",
    "href": "VB_Bayes1.html#example-binomial-sampling-uniform-prior",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Example: Binomial sampling + uniform prior",
    "text": "Example: Binomial sampling + uniform prior\nSuppose out of \\(n=10\\) conditionally independent draws of a binary random variable we observe \\(Y=2\\) ones (successes).\n Using a uniform prior distribution (a.k.a., \\(\\mathrm{Beta}(1,1)\\)) for \\(\\theta\\), the posterior distribution is \\(\\theta | y=2 \\sim \\mathrm{Beta}(1+2,1+10-2)\\)."
  },
  {
    "objectID": "VB_Bayes1.html#alternative-hpd-region",
    "href": "VB_Bayes1.html#alternative-hpd-region",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Alternative: HPD region",
    "text": "Alternative: HPD region\nA \\(100 \\times(1-\\alpha)\\) % highest posterior density (HPD) regions is the part of parameter space, \\(s(y)\\), such that:\n\n\\(P(\\theta \\in s(y) |Y=y)= 1-\\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\notin s(y)\\) then \\(P(\\theta_a |Y=y)>P(\\theta_b |Y=y)\\)\n\n\\(\\Rightarrow\\) all points inside the HPD region have higher probability density than those outside."
  },
  {
    "objectID": "VB_Bayes1.html#next-steps",
    "href": "VB_Bayes1.html#next-steps",
    "title": "VectorByte Methods Training\nIntroduction to Bayesian Statistics",
    "section": "Next Steps",
    "text": "Next Steps\nNext you’ll complete a practical where you conduct a conjugate Bayesian analysis for the mean of a normal distribution on the midge data introduced in the likelihood chapter. You’ll also visualize the effect of different prior choices on the posterior distribution."
  }
]