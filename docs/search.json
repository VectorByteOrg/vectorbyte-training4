[
  {
    "objectID": "data_wrangling.html",
    "href": "data_wrangling.html",
    "title": "",
    "section": "",
    "text": "Main materials"
  },
  {
    "objectID": "data_wrangling.html#welcome-to-the-vectorbyte-2023-section-on-data-wrangling-and-visualiszation",
    "href": "data_wrangling.html#welcome-to-the-vectorbyte-2023-section-on-data-wrangling-and-visualiszation",
    "title": "",
    "section": "Welcome to the VectorByte 2023 section on Data Wrangling and Visualiszation",
    "text": "Welcome to the VectorByte 2023 section on Data Wrangling and Visualiszation\nThis section will cover: * Data Management and Visualization * Experimental Design"
  },
  {
    "objectID": "data_wrangling.html#data-management-and-visualization",
    "href": "data_wrangling.html#data-management-and-visualization",
    "title": "",
    "section": "Data Management and Visualization",
    "text": "Data Management and Visualization\n\nClutter and confusion are failures of design, not attributes of information. – Edward Tuftey"
  },
  {
    "objectID": "data_wrangling.html#introduction",
    "href": "data_wrangling.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nThis section aims at introducing you to key principles and methods for data processing, storage, exploration and visualization.\nIn this modern world, massive amounts of data are being generated in biology due to rapid advances in technologies for collecting new, as well as for digitizing old data. Some prominent examples are Genomic, Ecosystem respiration, Climatic, and Animal tracking data. Ultimately, the goal of quantitative biology is to both, discover patterns in these data, and fit mathematical models to them. Reproducible data manipulation, analyses and visualization are particularly necessary when data are so large and complex, and few are more so than biological data, which are extremely heterogeneous in their structure and quality. Furthermore, when these data are “big” (more below on what makes a dataset “big”), computationally-efficient data handling, manipulation and analysis techniques are needed.\n\nR vs. Python\nWe will use R in VectorBiTE 2023 Training because it a great one stop solution for both data manipulation, analysis and visualization. In general, R will do the job for most of your purposes. There is not much between difference these two languages for data science. One advantage that Python has is its greater computational efficiency. On the other hand, R was developed for convenient statistical analyses, with efficiency not being the main concern.\nRead more about R vs Python for data science here and here."
  },
  {
    "objectID": "data_wrangling.html#data-wrangling",
    "href": "data_wrangling.html#data-wrangling",
    "title": "",
    "section": "Data wrangling",
    "text": "Data wrangling\nYou are likely to spend far more time than you think dredging through data files manually – checking them, editing them, and reformatting them to make them useful for data exploration and analysis. It is often the case that you’ll have to deal with messy or incomplete data, either because of sampling challenges (e.g., “field” data), or because you got given data that was poorly recorded and maintained. The data we obtain from different data sources is often unusable at the beginning; for example you may need to: * Identify the variables vs observations within the data—somebody else might have recorded the data, or you youself might have collected the data some time back! * Fill in zeros (true measured or observed absences) * Identify and add a value (e.g., -999999) to denote missing observations * Derive or calculate new variables from the raw observations (e.g., convert measurements to SI units; kilograms, meters, seconds, etc.) * Reshape/reformat your data into a layout that works best for analysis (e.g., for R itself);e.g., from wide to long data format for replicated (across plates, chambers, plots, sites, etc.) data * Merge multiple datasets together into a single data sheet\nThis is not an exhaustive list. Doing so many different things to your raw data is both time-consuming and risky. Why risky? Because to err is very human, and every new, tired mouse-click and/or keyboard-stab has a high probability of inducing an erroneous data point!"
  },
  {
    "objectID": "data_wrangling.html#some-data-wrangling-principles",
    "href": "data_wrangling.html#some-data-wrangling-principles",
    "title": "",
    "section": "Some data wrangling principles",
    "text": "Some data wrangling principles\nSo you would like a record of the data wrangling process (so that it is repeatable and even reversible), and automate it to the extent possible. To this end, here are some guidelines:\n\nStore data in universally (machine)-readable, non-proprietary formats; basically, use plain ASCII text for your file names, variable/field/column names, and data values. And make sure the data file’s “text encoding” is correct and standard (e.g., UTF-8).\nKeep a metadata file for each unique dataset (again, in non-proprietary format).\nMinimize modifying raw data by hand—use scripts instead—keep a copy of the data as they were recorded.\nUse meaningful names for your data and files and field (column) names\nWhen you add data, try not to add columns (widening the format); rather, design your tables/data-sheets so that you add only rows (lengthening the format)—and convert “wide format data” to “long format data” using scripts, not by hand,\nAll cells within a data column should contain only one type of information (i.e., either text (character), numeric, etc.).\nUltimately, consider creating a relational database for your data (More on this below).\n\nThis is not an exhaustive list either— see the Readings & Resources Section.\n\nAn example\nWe will use the Pound Hill dataset collected by students in an Imperial College Field Course for understanding some of these principles. This is not vectorbite data but it is an excellent example of a dataset that needs wrangling. VectorBiTE is too well structured for this sort of wrangling.\nTo start with, we need to import the raw data file, for which, follow these steps:\n★ Copy the file PoundHillData.csv and PoundHillMetaData.csv files from the VectorBiTE data directory into your own R data directory. Then load the data in R:"
  },
  {
    "objectID": "schedule2023.html",
    "href": "schedule2023.html",
    "title": "2023 Training Schedule",
    "section": "",
    "text": "Main materials\n\nPre-workshop\nWe will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting.\nWe will also be using Slack to facilitate the synchronous portions of the training. Please have these installed in advance. We are assuming familiarity with R basics. In addition, we recommend that you do the following:\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter up to the section on Writing R code. Of course, keep going if you want (although we will cover some similar materials here).\nReview background on introductory probability and statistics (solutions to exercises)\n\nOnce you have completed the pre-work and set-up tasks, you should get yourself familiar with the VectorByte Training materials. You will choose a starting point based on your previous experience. This will be in the form of slack channels labeled based on interest. From there you will group yourself (based on whatever you like, but keep in mind time zones and language preferences) and a demonstrator will be matched with your group.\nMuch of the synchronous time is planned to be dedicated to helping you work through exercises and activities. Thus you’ll want to watch the videos and take a look at the slides off line (likely starting earlier in the week for most participants). The small subset of live instruction portions is elaborated below.\n  \n\n\n17th July 2023\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nArrival\n\n\n\n\n  \n\n\n18th July 2023 (08:30 - 17:00)\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n08:30\nCourse overview\n\n\n\n09:00\nIntro to traits\nExample\n\n\n10:30\nBreak\n\n\n\n11:00\nVecTraits database\nVecTraits\n\n\n12:00\nLunch\n\n\n\n13:00\nTutorial: Data wrangling and visualizing data\nData wrangling\n\n\n15:00\nBreak\n\n\n\n15:30\nStatistical Analyses I: Linear models\nLinear models\n\n\n\n  \n\n\n19th July 2023 (08:30 - 17:00)\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n08:30\nQ & A\n\n\n\n09:00\nAllometric analysis using LMs\nLinear models\n\n\n10:30\nBreak\n\n\n\n11:00\nLinear vs. Nonlinear models\nPractical\n\n\n12:00\nLunch\n\n\n\n13:00\nIntro to Bayes\n\n\n\n14:30\nBreak\n\n\n\n15:00\nThermal traits with BayesTPC\n\n\n\n16:30\nChoose projects for your own trait/VBD story\n\n\n\n\n  \n\n\n20th July 2023 (10:00 - 1:00)\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n8:30\nQ & A\n\n\n\n9:00\nWork on analysis\n\n\n\n10:30\nBreak\n\n\n\n11:00\nWork on analysis\n\n\n\n12:00\nLunch\n\n\n\n13:00\nContinue analysis/present project\n\n\n\n15:00\nBreak\n\n\n\n15:30\nPresentations\n\n\n\n16:30\nData uploads\n\n\n\n\n  \n\n\n21st July 2023\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nTravel\n\n\n\n\n  \n\n\nPost-workshop\nEnjoy using these new techniques and databases!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Welcome to the VectorByte 2023 training workshop!\n\nCheck out our about and schedule pages to continue."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the VectorByte Training Materials 2023",
    "section": "",
    "text": "As the VectorByte team has developed these materials, we’ve aimed to provide resources for both guided (during the workshop) and self-led learning. We assume basic familiarity with:\n\nThe R Programming Language\nBasic calculus (especially the mathematical idea of functions)\nBasic probability and statistics (e.g., what is a probability distribution, normal and binomial distributions, means, variances)\n\nWe’ve divided the materials into subject matter modules, and categorized these loosely from beginner to advanced methods. Each module is designed to build on the previous one, and expects at least knowledge of all of the preceding modules in the sequence. Based on your experience, we encourage students to “choose your own adventure” and start from where you feel comfortable.\nEach module consists of four kinds of materials:\n\nslides with presentation of materials\nvideos with recorded lectures based on slides\nlabs/hands-on materials to allow you to practice material in a practical way\nsolutions to exercises, when necessary\n\nWe also include links to additional resources/materials/references.\nFor more information about the goals and approach of the VectorByte RCN are available at vectorbyte.org."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "VectorByte Training Materials 2023",
    "section": "",
    "text": "Overview\n\n\nPre-work and set-up\n\nHardware and Software\nWe will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop. We will have a channel on slack dedicated to software/hardware issues and troubleshooting.\nWe will also be using Zoom and Slack to facilitate the synchronous portions of the training. Please have these installed in advance.\n\n\nPre-requisites\nWe are assuming familiarity with R basics. In addition, we recommend that you do the following:\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter up to the section on Writing R code. Of course, keep going if you want (although we will cover some similar materials here).\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. But there are many more resources online (e.g., this and this ) – pick something that suits your learning style.\nReview background on introductory probability and statistics (solutions to exercises)\nInculcate the coding Jedi inside of you - or the Sith - whatever works.\n\n\n\n\nIntroduction to the VectorByte databases1\n\nThis component will be delivered live & synchronously (with a recording to be uploaded later)\n\n\n\nBeginner materials\n\nData Wrangling in R\n\nSelf-Guided Activity\nDatasets:\n\nPoundhill Data\nPoundhill Meta Data\nHuxley et al Trait Data\nGenome Size\nWrangling Practical Data\n\n\n\n\nIntroduction to Linear Models\n\nLecture Slides, Lecture Video 1, Lecture Video 2, Practical\nDatasets:\n\nGenome Size\nHuxley et al Trait Data\nLinear Models Practical Data\n\n\n \n\n\n\nIntermediate materials\n\nNonlinear Modeling (including Thermal Performance Curves – TPCs)\n\nLecture Slides, Lecture Video, Practical\nDatasets:\n\ncsm7I Data\nAedes Juvenile Mortality Data\nNLLS Practical Data\n\n\n\n\n\n\n\n\nFootnotes\n\n\nWhat is the difference between VectorBiTE and VectorByte? We are glad you asked! VectorBiTE was an RCN or a research coordination network funded by a 5 year grant from the BBSRC. VectorByte is hosting this training which is a newly funded NSF grant to establish a global open access data platform to study disease vectors. All the databases have transitioned to VectorByte but the legacy options will still be available on the VectorBiTE website.↩︎"
  },
  {
    "objectID": "Stats_review.html",
    "href": "Stats_review.html",
    "title": "VectorByte Methods Training 2023",
    "section": "",
    "text": "There are certain core concepts that are necessary to make good progress in this course. In particular, I expect that you’ve been exposed to\n\nthe axioms of probability and their consequences.\nconditional probability and Bayes theorem\nthe definition of a random variable (discrete and continuous)\nbasics of probability distribution\ndefinitions of expectations\nsampling distribution\nCentral Limit Theorem (CLT)\nStudent-t distribution\nconfidence intervals and p-values\n\nBelow I do a very fast overview of the first two (mostly just their definitions). This is followed by a more in depth review of RVs and probability distributions along with an introduction to some related concepts (like expectations). This is important background that we’ll need for later on in the course.\nI provide practice problems within the text. These are noted as . I will provide solutions in another file at a later date."
  },
  {
    "objectID": "Stats_review.html#some-probability-notation",
    "href": "Stats_review.html#some-probability-notation",
    "title": "VectorByte Methods Training 2023",
    "section": "Some probability notation",
    "text": "Some probability notation\nWe have a set, \\(S\\) of all possible events. Let \\(\\text{Pr}(A)\\) (or alternatively \\(\\text{Prob}(A)\\)) be the probability of event \\(A\\). Then:\n\n\\(A^c\\) is the complement to \\(A\\) (all events that are not A).\n\\(A \\cup B\\) is the union of events \\(A\\) and \\(B\\) (``A or B’’).\n\\(A \\cap B\\) is the intersection of events \\(A\\) and \\(B\\) (``A and B’’).\n\\(\\text{Pr}(A|B)\\) is the conditional probability of \\(A\\) given that \\(B\\) occurs."
  },
  {
    "objectID": "Stats_review.html#axioms-of-probability",
    "href": "Stats_review.html#axioms-of-probability",
    "title": "VectorByte Methods Training 2023",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\nThese are the basic definitions that we use when we talk about probabilities. You’ve probably seen these before, but maybe not in mathematical notation. If the notation is new to you, I suggest that you use the notation above to translate these statements into words and confirm that you understand what they mean. I give you an example for the first statement.\n\n\\(\\sum_{i \\in S} \\text{Pr}(A_i)=1\\), where \\(0 \\leq \\text{Pr}(A_i) \\leq 1\\) (the probabilities of all the events that can happen must sum to one, and all of the individual probabilities must be less than one)\n\\(\\text{Pr}(A)=1-\\text{Pr}(A^c)\\)\n\\(\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) -\\text{Pr}(A \\cap B)\\)\n\\(\\text{Pr}(A \\cap B) = \\text{Pr}(A|B)\\text{Pr}(B)\\)\nIf \\(A\\) and \\(B\\) are independent, then \\(\\text{Pr}(A|B) = \\text{Pr}(A)\\)"
  },
  {
    "objectID": "Stats_review.html#bayes-theorem",
    "href": "Stats_review.html#bayes-theorem",
    "title": "VectorByte Methods Training 2023",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nBayes Theorem allows us to related the conditional probabilities of two events \\(A\\) and \\(B\\):\n\\[\\begin{align*}\n\\text{Pr}(A|B) & = \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\\\\\n&\\\\\n& =  \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B|A)\\text{Pr}(A) + \\text{Pr}(B|A^c)\\text{Pr}(A^c)}\n\\end{align*}\\]"
  },
  {
    "objectID": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training 2023",
    "section": "Discrete RVs and their Probability Distributions",
    "text": "Discrete RVs and their Probability Distributions\nMany things that we observe are naturally discrete. For instance, whole numbers of chairs or win/loss outcomes for games. Discrete probability distributions are used to describe these kinds of events.\nFor discrete RVs, the distribution of probabilities is described by the probability mass function (pmf), \\(f_k\\) such that:\n\\[\\begin{align*}\nf_k  \\equiv \\text{Pr}(X & = k) \\\\\n\\text{where } 0\\leq f_k \\leq 1 & \\text{ and } \\sum_k f_k = 1\n\\end{align*}\\]\nFor example, for a fair 6-sided die:\n\\(f_k = 1/6\\) for \\(k= \\{1,2,3,4,5,6\\}\\).\n \n For the six-sided fair die, what is \\(f_k\\) if \\(k=7\\)? \\(k=1.5\\)?\n \nRelated to the pmf is the cumulative distribution function (cdf), \\(F(x)\\). [ F(x) (X x) ]\nFor the 6-sided die [ F(x)= _{k=1}^x f_k ] where \\(x \\in 1\\dots 6\\).\n \n For the fair 6-sided die, what is \\(F(3)\\)? \\(F(7)\\)? \\(F(1.5)\\)?\n \n\nVisualizing distributions of discrete RVs in R\nExample: Imagine a RV can take values 1 through 10, each with probability 0.1:\n \n\nvals<-seq(1,10, by=1)\npmf<-rep(0.1, 10)\ncdf<-pmf[1]\nfor(i in 2:10) cdf<-c(cdf, cdf[i-1]+pmf[i])\npar(mfrow=c(1,2), bty=\"n\")\nbarplot(height=pmf, names.arg=vals, ylim=c(0, 1), main=\"pmf\", col=\"blue\")\nbarplot(height=cdf, names.arg=vals, ylim=c(0, 1), main=\"cdf\", col=\"red\")"
  },
  {
    "objectID": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training 2023",
    "section": "Continuous RVs and their Probability Distributions",
    "text": "Continuous RVs and their Probability Distributions\nThings are just a little different for continuous RVs. Instead we use the probability density function (pdf) of the RV, and denote it by \\(f(x)\\). It still describes how relatively likely are alternative values of an RV – that is, if the pdf his higher around one value than around another, then the first is more likely to happen. However, the pdf does not return a probability, it is a function that describes the probability density.\nAn analogy:\nProbabilities are like weights of objects. The PMF tells you how much weight each possible value or outcome contributes to a whole. The PDF tells you how dense it is around a value. To calculate the weight of a real object, you need to also know the size of the area that you’re interested in and the density there The probability that your RV takes exactly any value is zero, just like the probability that any atom in a very thin wire is lined up at exactly that position is zero (and to the amount of mass at that location is zero). However, you can take a very thin slice around that location to see how much material is there.\nRelated to the pdf is the cumulative distribution function (cdf), \\(F(x)\\). \\[\nF(x) \\equiv \\text{Pr}(X \\leq x)\n\\] For a continuous distribution: \\[\nF(x)= \\int_{-\\infty}^x f(x')dx'\n\\]\n \n For a normal distribution with mean 0, what is \\(F(0)\\)?\n \n\nVisualizing distributions of continuous RVs in R\nExample: exponential RV, where \\(f(x) = re^{-rx}\\):\n\n\nvals<-seq(0,10, length=1000)\nr<-0.5\npar(mfrow=c(1,2), bty=\"n\")\nplot(vals, dexp(vals, rate=r), main=\"pdf\", col=\"blue\", type=\"l\", lwd=3, ylab=\"\", xlab=\"\")\nplot(vals, pexp(vals, rate=r), main=\"cdf\", ylim=c(0,1), col=\"red\",\n     type=\"l\", lwd=3, ylab=\"\", xlab=\"\")"
  },
  {
    "objectID": "Stats_review.html#confidence-intervals",
    "href": "Stats_review.html#confidence-intervals",
    "title": "VectorByte Methods Training 2023",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nSuppose \\(Z_{n-p} \\sim t_{n-p}(0,1)\\). A centered interval is on this \\(t\\) distribution can be written as: [ (-t_{n-p,/2} < Z_{n-p} < t_{n-p,/2}) = 1-. ] That is, between these values of the \\(t\\) distribution \\((1-\\alpha)\\times 100\\) percent of the probability is contained in that symmetric interval. We can visually indicate these location on a plot of the \\(t\\) distribution (here with df\\(=5\\) and \\(\\alpha=0.05\\)):\n\nx<-seq(-4.5, 4.5, length=1000)\nalpha=0.05\n\n## draw a line showing the normal pdf on the histogram\nplot(x, dt(x, df=5), col=\"black\", lwd=2, type=\"l\", xlab=\"x\", ylab=\"\")\nabline(v=qt(alpha/2, df=5), col=3, lty=2, lwd=2)\nabline(v=qt(1-alpha/2, df=5), col=2, lty=2, lwd=2)\n\nlegend(\"topright\", \n       legend=c(\"t, df=5\", \"lower a/2\", \"upper a/2\"),\n       col=c(1,3,2), lwd=2, lty=c(1, 2,2))\n\n\n\n\n\n\n\n\nIn the R code here, \\({\\tt qt}\\) is the Student-\\(t\\) “quantile function”. The function \\({\\tt qt(alpha, df)}\\) returns a value \\(z\\) such that \\(\\alpha = P(Z_{\\mathrm{df}} < z)\\), i.e., \\(t_{\\mathrm{df},\\alpha}\\).\nHow can we use this to determine the confidence interval for \\(\\theta\\)? Since \\(\\theta \\sim t_{n-p}(\\mu, s^2)\\), we can replace the \\(Z_{n-p}\\) in the interval above with the definition in terms of \\(\\theta\\), \\(\\mu\\) and \\(s\\) and rearrange: \\[\\begin{align*}\n1-\\alpha& = \\text{Pr}\\left(-t_{n-p,\\alpha/2} < \\frac{\\mu - \\bar{\\theta}}{s} <\nt_{n-p,\\alpha/2}\\right) \\\\\n&=\n\\text{Pr}(\\bar{\\theta}-t_{n-p,\\alpha/2}s < \\mu <\n\\bar{\\theta} + t_{n-p,\\alpha/2}s)\n\\end{align*}\\]\nThus \\((1-\\alpha)\\)*100% of the time, \\(\\mu\\) is within the confidence interval (written in two equivalent ways):\n[ {} t_{n-p,/2} s ;;;;; ;;;;; [{}- t_{n-p,/2} s, {} + t_{n-p,/2} s] ]\nWhy should we care about confidence intervals?\n\nThe confidence interval captures the amount of information in the data about the parameter.\nThe center of the interval tells you what your estimate is.\nThe length of the interval tells you how sure you are about your estimate."
  },
  {
    "objectID": "Stats_review.html#p-values",
    "href": "Stats_review.html#p-values",
    "title": "VectorByte Methods Training 2023",
    "section": "p-Values",
    "text": "p-Values\nWhat is a p-value? The American Statistical Association issued a statement where they defined it in the following way:\n“Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” (ASA Statement on Statistical Significance and P-Values.)\nMore formally, we formulate a p-value in terms of a null hypothesis/model and test whether or not our observed data are more extreme than we would expect under that specific null model. In your previous courses you’ve probably seen very specific null models, corresponding to, for instance the null hypothesis that the mean of your data is normally distributed with mean \\(m\\) (often \\(m=0\\)). We often denote the null model as \\(H_0\\) and the alternative as \\(H_a\\) or \\(H_1\\). For instance, for our example above with \\(\\theta\\) we might want to test the following:\n[ H_0: {}=0 ;;;;; ;;;;; H_a: {} ]\nTo perform the hypothesis test we would FIRST choose our rejection level, \\(\\alpha\\). Although convention is to use \\(\\alpha =0.05\\) corresponding to a 95% confidence region, one could choose based on how sure one needs to be for a particular application. Next we build our test statistic. There are two cases, first if we know \\(\\sigma\\) and second if we don’t.\nIf we knew the variance \\(\\sigma^2\\), our test statistic would be \\(Z=\\frac{\\bar{\\theta}-0}{\\sigma}\\), and we expect that this should have a standard normal distribution, i.e., \\(Z\\sim\\mathcal{N}(0,1)\\). If we don’t know \\(\\sigma\\) and instead estimate is as \\(s\\) (which is most of the time), our test statistic would be \\(Z_{df}=\\frac{\\bar{\\theta}-0}{s}\\) (i.e., it would have a t-distribution).\nWe calculate the value of the appropriate statistic (either \\(Z\\) or \\(Z_{df}\\)) for our data, and then we compare it to the values of the standard distribution (normal or \\(t\\), respectively) corresponding to the \\(\\alpha\\) level that we chose, i.e., we see if the number that we got for our statistic is inside the horizontal lines that we drew on the standard distribution above. If it is, then the data are consistent with the null hypothesis and we cannot reject the null. If the statistic is outside the region the data are NOT consistent with the null, and instead we reject the null and use the alternative as our new working hypothesis.\nNotice that this process is focused on the null hypothesis. We cannot tell if the alternative hypothesis is true, or, really, if it’s actually better than the null. We can only say that the null is not consistent with our data (i.e., we can falsify the null) at a given level of certainty.\nAlso, the hypothesis testing process is the same as building a confidence interval, as above, and then seeing if the null hypothesis is within your confidence interval. If the null is outside of your confidence interval then you can reject your null at the level of certainty corresponding to the \\(\\alpha\\) that you used to build your CI. If the value for the null is within your CI, you cannot reject at that level."
  },
  {
    "objectID": "Stats_review.html#the-sampling-distribution-1",
    "href": "Stats_review.html#the-sampling-distribution-1",
    "title": "VectorByte Methods Training 2023",
    "section": "The Sampling Distribution",
    "text": "The Sampling Distribution\n\n\n\nSuppose we have a random sample \\(\\{Y_i, i=1,\\dots,N \\}\\), where \\(Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,9)\\) for \\(i=1,\\ldots,N\\).\n\nWhat is the variance of the sample mean?\nWhat is the expectation of the sample mean?\nWhat is the variance for another i.i.d. realization \\(Y_{ N+1}\\)?\nWhat is the standard error of \\(\\bar{Y}\\)?"
  },
  {
    "objectID": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "href": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "title": "VectorByte Methods Training 2023",
    "section": "Hypothesis Testing and Confidence Intervals",
    "text": "Hypothesis Testing and Confidence Intervals\n\n\n\nSuppose we sample some data \\(\\{Y_i, i=1,\\dots,n \\}\\), where \\(Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2)\\) for \\(i=1,\\ldots,n\\), and that you want to test the null hypothesis \\(H_0: ~\\mu=12\\) vs. the alternative \\(H_a: \\mu \\neq 12\\), at the \\(0.05\\) significance level.\n\nWhat test statistic would you use? How do you estimate \\(\\sigma\\)?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and \\(n \\rightarrow \\infty\\)?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the \\(p\\)-value associated with a particular sample?\nWhat is the 95% confidence interval for \\(\\mu\\)? How should one interpret this interval?\nIf \\(\\bar{Y} = 11\\), \\(s_y = 1\\), and \\(n=9\\), what is the test result? What is the 95% CI for \\(\\mu\\)?"
  }
]